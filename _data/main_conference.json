[{"day": "Friday, September 8, 2017", "program": [[{"title": "Welcome Reception", "code": null, "id": "1", "start_time_iso": "2017-09-08T19:00:00", "end_time": "22:00", "end_time_iso": "2017-09-08T22:00:00", "type": "other", "room": "CPH Conference, Room \u00d8sterbro", "start_time": "19:00", "talks": [], "posters": []}]], "weekday": "Friday"}, {"day": "Saturday, September 9, 2017", "program": [[{"title": "Registration Day 1", "code": null, "id": "2", "start_time_iso": "2017-09-09T07:30:00", "end_time": "17:30", "end_time_iso": "2017-09-09T17:30:00", "type": "other", "room": null, "start_time": "07:30", "talks": [], "posters": []}], [{"title": "Morning Coffee", "start_time_iso": "2017-09-09T08:00:00", "end_time": "08:30", "end_time_iso": "2017-09-09T08:30:00", "id": "3", "start_time": "08:00", "type": "break"}], [{"title": "Plenary Session. Opening Remarks", "code": null, "id": "4", "start_time_iso": "2017-09-09T08:30:00", "end_time": "09:00", "end_time_iso": "2017-09-09T09:00:00", "type": "other", "room": "Jutland", "start_time": "08:30", "talks": [], "posters": []}], [{"title": "Plenary Session. Invited Talk by Nando de Freitas", "code": null, "id": "5", "start_time_iso": "2017-09-09T09:00:00", "end_time": "10:00", "end_time_iso": "2017-09-09T10:00:00", "type": "invited_talk", "room": "Jutland", "start_time": "09:00", "talks": [], "posters": []}], [{"title": "Coffee Break", "start_time_iso": "2017-09-09T10:00:00", "end_time": "10:30", "end_time_iso": "2017-09-09T10:30:00", "id": "6", "start_time": "10:00", "type": "break"}], [{"title": "Syntax 1", "start_time_iso": "2017-09-09T10:30:00", "end_time_iso": "2017-09-09T12:10:00", "type": "paper", "posters": [], "chair_affiliation": "Uppsala University", "code": "1A", "room": "Jutland", "end_time": "12:10", "id": "7", "start_time": "10:30", "talks": [{"title": "Monolingual Phrase Alignment on Parse Forests", "venue": "ACL", "end_time": "10:55", "abstract": "We propose an efficient method to conduct phrase alignment on parse forests for\nparaphrase detection. Unlike previous studies, our method identifies syntactic\nparaphrases under linguistically motivated grammar. In addition, it allows\nphrases to non-compositionally align to handle paraphrases with non-homographic\nphrase correspondences. \n\nA dataset that provides gold parse trees and their phrase alignments is\ncreated. The experimental results confirm that the proposed method conducts\nhighly accurate phrase alignment compared to human performance.", "id": "617", "start_time": "10:30", "authors": "Yuki Arase and Jun'ichi Tsujii"}, {"title": "Fast(er) Exact Decoding and Global Training for Transition-Based Dependency Parsing via a Minimal Feature Set", "venue": "ACL", "end_time": "11:20", "abstract": "We first present a minimal feature set for transition-based dependency parsing,\ncontinuing a recent trend started by Kiperwasser and Goldberg (2016a) and Cross\nand Huang (2016a) of using bi-directional LSTM features. We plug our minimal\nfeature set into the dynamic-programming framework of Huang and Sagae (2010)\nand Kuhlmann et al. (2011) to produce the first implementation of worst-case\nO(n^3) exact decoders for arc-hybrid and arc-eager transition systems. With our\nminimal features, we also present O(n^3) global training methods. Finally,\nusing ensembles including our new parsers, we achieve the best unlabeled\nattachment score reported (to our knowledge) on the Chinese Treebank and the\n``second-best-in-class'' result on the English Penn Treebank.", "id": "1381", "start_time": "10:55", "authors": "Tianze Shi, Liang Huang and Lillian Lee"}, {"title": "Parsing with Traces: An O(n^4) Algorithm and a Structural Representation", "venue": "TACL", "end_time": "11:45", "abstract": "General treebank analyses are graph structured, but parsers are typically\nrestricted to tree structures for efficiency and modeling reasons. We propose a\nnew representation and algorithm for a class of graph structures that is\nflexible enough to cover almost all treebank structures, but still admit\nefficient learning and inference. In particular, we consider directed, acyclic,\none-endpoint-crossing graph structures, which cover most long-distance\ndislocation, shared argumentation, and similar tree-violating linguistic\nphenomena. We describe how to convert phrase structure parses, including\ntraces, to our new representation, in a reversible manner. Our dynamic program\nuniquely decomposes structures, is sound and complete, and covers 97.3% of the\nPenn English treebank. We also implement a proof-of-concept parser that\nrecovers a range of null elements and trace types.", "id": "1532", "start_time": "11:20", "authors": "Jonathan K. Kummerfeld and Dan Klein"}, {"title": "Quasi-Second-Order Parsing for 1-Endpoint-Crossing, Pagenumber-2 Graphs", "venue": "ACL", "end_time": "12:10", "abstract": "We propose a new Maximum Subgraph algorithm for first-order parsing to\n1-endpoint-crossing, pagenumber-2 graphs. Our algorithm has two\ncharacteristics: (1) it separates the construction for noncrossing edges and\ncrossing edges; (2) in a single construction step, whether to create a new arc\nis deterministic. These two characteristics make our algorithm relatively easy\nto be extended to incorporiate crossing-sensitive second-order features. We\nthen introduce a new algorithm for quasi-second-order parsing. Experiments\ndemonstrate that second-order features are helpful for Maximum Subgraph\nparsing.", "id": "1501", "start_time": "11:45", "authors": "Junjie Cao, Sheng Huang, Weiwei Sun and Xiaojun Wan"}], "chair": "Joakim Nivre"}, {"title": "Information Extraction 1", "start_time_iso": "2017-09-09T10:30:00", "end_time_iso": "2017-09-09T12:10:00", "type": "paper", "posters": [], "chair_affiliation": "Google", "code": "1B", "room": "Funen", "end_time": "12:10", "id": "8", "start_time": "10:30", "talks": [{"title": "Position-aware Attention and Supervised Data Improve Slot Filling", "venue": "ACL", "end_time": "10:55", "abstract": "Organized relational knowledge in the form of \"knowledge graphs\" is important\nfor many applications. However, the ability to populate knowledge bases with\nfacts automatically extracted from documents has improved frustratingly slowly.\nThis paper simultaneously addresses two issues that have held back prior work.\nWe first propose an effective new model, which combines an LSTM sequence model\nwith a form of entity position-aware attention that is better suited to\nrelation extraction. Then we build TACRED, a large (119,474 examples)\nsupervised relation extraction dataset obtained via crowdsourcing and targeted\ntowards TAC KBP relations. The combination of better supervised data and a more\nappropriate high-capacity model enables much better relation extraction\nperformance. When the model trained on this new dataset replaces the previous\nrelation extraction component of the best TAC KBP 2015 slot filling system, its\nF1 score increases markedly from 22.2% to 26.7%.", "id": "1417", "start_time": "10:30", "authors": "Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli and Christopher D. Manning"}, {"title": "Heterogeneous Supervision for Relation Extraction: A Representation Learning Approach", "venue": "ACL", "end_time": "11:20", "abstract": "Relation extraction is a fundamental task in information extraction.\nMost existing methods have heavy reliance on annotations labeled by human\nexperts, which are costly and time-consuming.\nTo overcome this drawback, we propose a novel framework, REHession, to conduct\nrelation extractor learning using annotations from heterogeneous information\nsource, e.g., knowledge base and domain heuristics.\nThese annotations, referred as heterogeneous supervision, often conflict with\neach other, which brings a new challenge to the original relation extraction\ntask: how to infer the true label from noisy labels for a given instance.\nIdentifying context information as the backbone of both relation extraction and\ntrue label discovery, we adopt embedding techniques to learn the distributed\nrepresentations of context, which bridges all components with mutual\nenhancement in an iterative fashion.\nExtensive experimental results demonstrate the superiority of REHession over\nthe state-of-the-art.", "id": "1321", "start_time": "10:55", "authors": "Liyuan Liu, Xiang Ren, Qi Zhu, Shi Zhi, Huan Gui, Heng Ji and Jiawei Han"}, {"title": "Integrating Order Information and Event Relation for Script Event Prediction", "venue": "ACL", "end_time": "11:45", "abstract": "There has been a recent line of work automatically learning scripts from\nunstructured texts, by modeling narrative event chains. While the dominant\napproach group events using event pair relations, LSTMs have been used to\nencode full chains of narrative events. The latter has the advantage of\nlearning long-range temporal orders, yet the former is more adaptive to partial\norders. We propose a neural model that leverages the advantages of both\nmethods, by using LSTM hidden states as features for event pair modelling. A\ndynamic memory network is utilized to automatically induce weights on existing\nevents for inferring a subsequent event. Standard evaluation shows that our\nmethod significantly outperforms both methods above, giving the best results\nreported so far.", "id": "1082", "start_time": "11:20", "authors": "Zhongqing Wang, Yue Zhang and Ching-Yun Chang"}, {"title": "Entity Linking for Queries by Searching Wikipedia Sentences", "venue": "ACL", "end_time": "12:10", "abstract": "We present a simple yet effective approach for linking entities in queries. The\nkey idea is to search sentences similar to a query from Wikipedia articles and\ndirectly use the human-annotated entities in the similar sentences as candidate\nentities for the query. Then, we employ a rich set of features, such as\nlink-probability, context-matching, word embeddings, and relatedness among\ncandidate entities as well as their related entities, to rank the candidates\nunder a regression based framework. The advantages of our approach lie in two\naspects, which contribute to the ranking process and final linking result.\nFirst, it can greatly reduce the number of candidate entities by filtering out\nirrelevant entities with the words in the query. Second, we can obtain the\nquery sensitive prior probability in addition to the static link-probability\nderived from all Wikipedia articles. We conduct experiments on two benchmark\ndatasets on entity linking for queries, namely the ERD14 dataset and the GERDAQ\ndataset. Experimental results show that our method outperforms state-of-the-art\nsystems and yields 75.0% in F1 on the ERD14 dataset and 56.9% on the GERDAQ\ndataset.", "id": "313", "start_time": "11:45", "authors": "Chuanqi Tan, Furu Wei, Pengjie Ren, Weifeng Lv and Ming Zhou"}], "chair": "Ming-Wei Chang"}, {"title": "Multilingual NLP", "start_time_iso": "2017-09-09T10:30:00", "end_time_iso": "2017-09-09T12:10:00", "type": "paper", "posters": [], "chair_affiliation": "University of Edinburgh", "code": "1C", "room": "Zealand", "end_time": "12:10", "id": "9", "start_time": "10:30", "talks": [{"title": "Train-O-Matic: Large-Scale Supervised Word Sense Disambiguation in Multiple Languages without Manual Training Data", "venue": "ACL", "end_time": "10:55", "abstract": "Annotating large numbers of sentences with senses is the heaviest requirement\nof current Word Sense Disambiguation. We present Train-O-Matic, a\nlanguage-independent method for generating millions of sense-annotated training\ninstances for virtually all meanings of words in a language's vocabulary. The\napproach is fully automatic: no human intervention is required and the only\ntype of human knowledge used is a WordNet-like resource. Train-O-Matic achieves\nconsistently state-of-the-art performance across gold standard datasets and\nlanguages, while at the same time removing the burden of manual annotation. All\nthe training data is available for research purposes at http://trainomatic.org.", "id": "336", "start_time": "10:30", "authors": "Tommaso Pasini and Roberto Navigli"}, {"title": "Universal Semantic Parsing", "venue": "ACL", "end_time": "11:20", "abstract": "Universal Dependencies (UD) offer a uniform cross-lingual syntactic\nrepresentation, with the aim of advancing multilingual applications. Recent\nwork shows that semantic parsing can be accomplished by transforming syntactic\ndependencies to logical forms. However, this work is limited to English, and\ncannot process dependency graphs, which allow handling complex phenomena such\nas control. In this work, we introduce UDepLambda, a semantic interface for UD,\nwhich maps natural language to logical forms in an almost language-independent\nfashion and can process dependency graphs. We perform experiments on question\nanswering against Freebase and provide German and Spanish translations of the\nWebQuestions and GraphQuestions datasets to facilitate multilingual evaluation.\nResults show that UDepLambda outperforms strong baselines across languages and\ndatasets.  For English, it achieves a 4.9 F1 point improvement over the\nstate-of-the-art on GraphQuestions.", "id": "857", "start_time": "10:55", "authors": "Siva Reddy, Oscar T\u00e4ckstr\u00f6m, Slav Petrov, Mark Steedman and Mirella Lapata"}, {"title": "Mimicking Word Embeddings using Subword RNNs", "venue": "ACL", "end_time": "11:45", "abstract": "Word embeddings improve generalization over lexical features by placing each\nword in a lower-dimensional space, using distributional information obtained\nfrom unlabeled data. However, the effectiveness of word embeddings for\ndownstream NLP tasks is limited by out-of-vocabulary (OOV) words, for which\nembeddings do not exist. In this paper, we present MIMICK, an approach to\ngenerating OOV word embeddings compositionally, by learning a function from\nspellings to distributional embeddings. Unlike prior work, MIMICK does not\nrequire re-training on the original word embedding corpus; instead, learning is\nperformed at the type level. Intrinsic and extrinsic evaluations demonstrate\nthe power of this simple approach. On 23 languages, MIMICK improves\nperformance over a word-based baseline for tagging part-of-speech and\nmorphosyntactic attributes. It is competitive with (and complementary to) a\nsupervised character-based model in low resource settings.", "id": "1195", "start_time": "11:20", "authors": "Yuval Pinter, Robert Guthrie and Jacob Eisenstein"}, {"title": "Past, Present, Future: A Computational Investigation of the Typology of Tense in 1000 Languages", "venue": "ACL", "end_time": "12:10", "abstract": "We present SuperPivot, an analysis method for low-resource languages that occur\nin a superparallel corpus, i.e., in a corpus that contains an order of\nmagnitude more languages than parallel corpora currently in use. We show that\nSuperPivot performs well for the crosslingual analysis of the linguistic\nphenomenon of tense. We produce analysis results for more than 1000 languages,\nconducting -- to the best of our knowledge -- the largest crosslingual\ncomputational study performed to date. We extend existing methodology for\nleveraging parallel corpora for typological analysis by overcoming a limiting\nassumption of earlier work: We only require that a linguistic feature is\novertly marked in a few of thousands of languages as opposed to requiring that\nit be marked in all languages under investigation.", "id": "963", "start_time": "11:45", "authors": "Ehsaneddin Asgari and Hinrich Sch\u00fctze"}], "chair": "Ivan Titov"}, {"title": "Poster Session: Demo", "start_time_iso": "2017-09-09T10:30:00", "end_time_iso": "2017-09-09T12:10:00", "posters": [], "type": "other", "code": "1D", "room": "Aarhus", "end_time": "12:10", "id": "10", "start_time": "10:30", "talks": [], "chair": "Michael Paul"}], [{"title": "Lunch", "start_time_iso": "2017-09-09T12:10:00", "end_time": "13:40", "end_time_iso": "2017-09-09T13:40:00", "id": "11", "start_time": "12:10", "type": "break"}], [{"title": "Machine Translation 1", "start_time_iso": "2017-09-09T13:40:00", "end_time_iso": "2017-09-09T15:20:00", "type": "paper", "posters": [], "chair_affiliation": "Carnegie Mellon University", "code": "2A", "room": "Jutland", "end_time": "15:20", "id": "12", "start_time": "13:40", "talks": [{"title": "Neural Machine Translation with Source-Side Latent Graph Parsing", "venue": "ACL", "end_time": "14:05", "abstract": "This paper presents a novel neural machine translation model which jointly\nlearns translation and source-side latent graph representations of sentences.\nUnlike existing pipelined approaches using syntactic parsers, our end-to-end\nmodel learns a latent graph parser as part of the encoder of an attention-based\nneural machine translation model, and thus the parser is optimized according to\nthe translation objective.\nIn experiments, we first show that our model compares favorably with\nstate-of-the-art sequential and pipelined syntax-based NMT models.\nWe also show that the performance of our model can be further improved by\npre-training it with a small amount of treebank annotations.\nOur final ensemble model significantly outperforms the previous best models on\nthe standard English-to-Japanese translation dataset.", "id": "994", "start_time": "13:40", "authors": "Kazuma Hashimoto and Yoshimasa Tsuruoka"}, {"title": "Neural Machine Translation with Word Predictions", "venue": "ACL", "end_time": "14:30", "abstract": "In the encoder-decoder architecture for neural machine translation (NMT), the\nhidden states of the recurrent structures in the encoder and decoder carry the\ncrucial information about the sentence. These vectors are generated by\nparameters which are updated by back-propagation of translation errors through\ntime.We argue that propagating errors through the end-to-end recurrent\nstructures are not a direct way of control the hidden vectors. \nIn this paper, we propose to use word predictions as a mechanism for direct\nsupervision. More specifically, we require these vectors to be able to predict\nthe vocabulary in target sentence. Our simple mechanism ensures better\nrepresentations in the encoder and decoder without using any extra data or\nannotation. It is also helpful in reducing the target side vocabulary and\nimproving the decoding efficiency. Experiments on Chinese-English machine\ntranslation task show an average BLEU improvement by 4.53, respectively.", "id": "1213", "start_time": "14:05", "authors": "Rongxiang Weng, Shujian Huang, Zaixiang Zheng, XIN-YU DAI and Jiajun CHEN"}, {"title": "Towards Decoding as Continuous Optimisation in Neural Machine Translation", "venue": "ACL", "end_time": "14:55", "abstract": "We propose a novel decoding approach for neural machine translation (NMT) based\non continuous optimisation. We reformulate decoding, a discrete optimization\nproblem, into a continuous problem, such that optimization can make use of\nefficient gradient-based techniques. Our powerful decoding framework allows for\nmore accurate decoding for standard neural machine translation models, as well\nas enabling decoding in intractable models such as intersection of several\ndifferent NMT models. Our empirical results show that our decoding framework is\neffective, and can leads to substantial improvements in translations,\nespecially in situations where greedy search and beam search are not feasible.\nFinally, we show how the technique is highly competitive with, and\ncomplementary to, reranking.", "id": "1250", "start_time": "14:30", "authors": "Cong Duy Vu Hoang, Gholamreza Haffari and Trevor Cohn"}, {"title": "Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation", "venue": "TACL", "end_time": "15:20", "abstract": "We propose a simple solution to use a single Neural Machine Translation (NMT)\nmodel to translate between multiple languages. Our solution requires no changes\nto the model architecture from a standard NMT system but instead introduces an\nartificial token at the beginning of the input sentence to specify the required\ntarget language. Using a shared wordpiece vocabulary, our approach enables\nMultilingual NMT using a single model. On the WMT\u201914 benchmarks, a single\nmultilingual model achieves comparable performance for English\u2192French and\nsurpasses state-of-the-art results for English\u2192German. Similarly, a single\nmultilingual model surpasses state-of-the-art results for French\u2192English and\nGerman\u2192English on WMT\u201914 and WMT\u201915 benchmarks, respectively. On\nproduction corpora, multilingual models of up to twelve language pairs allow\nfor better translation of many individual pairs. Our models can also learn to\nperform implicit bridging between language pairs never seen explicitly during\ntraining, showing that transfer learning and zero-shot translation is possible\nfor neural translation. Finally, we show analyses that hints at a universal\ninterlingua representation in our models and show some interesting examples\nwhen mixing languages.", "id": "1529", "start_time": "14:55", "authors": "Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vi\u00e9gas, Martin Wattenberg, Greg Corrado, Macduff Hughes and Jeffrey Dean"}], "chair": "Graham Neubig"}, {"title": "Language Grounding", "start_time_iso": "2017-09-09T13:40:00", "end_time_iso": "2017-09-09T15:20:00", "type": "paper", "posters": [], "chair_affiliation": "University of Washington", "code": "2B", "room": "Funen", "end_time": "15:20", "id": "13", "start_time": "13:40", "talks": [{"title": "Where is Misty? Interpreting Spatial Descriptors by Modeling Regions in Space", "venue": "ACL", "end_time": "14:05", "abstract": "We present a model for locating regions in space based on natural language\ndescriptions. Starting with a 3D scene and a sentence, our model is able to\nassociate words in the sentence with regions in the scene, interpret relations\nsuch as `on top of' or `next to,' and finally locate the region described in\nthe sentence. All components form a single neural network that is trained\nend-to-end without prior knowledge of object segmentation. To evaluate our\nmodel, we construct and release a new dataset consisting of Minecraft scenes\nwith crowdsourced natural language descriptions. We achieve a 32% relative\nerror reduction compared to a strong neural baseline.", "id": "1085", "start_time": "13:40", "authors": "Nikita Kitaev and Dan Klein"}, {"title": "Continuous Representation of Location for Geolocation and Lexical Dialectology using Mixture Density Networks", "venue": "ACL", "end_time": "14:30", "abstract": "We propose a method for embedding two-dimensional locations in a continuous\nvector space using a neural network-based model incorporating mixtures of\nGaussian distributions, presenting two model variants for text-based\ngeolocation and lexical dialectology. Evaluated over Twitter data, the proposed\nmodel outperforms conventional regression-based geolocation and provides a\nbetter estimate of uncertainty. We also show the effectiveness of the\nrepresentation for predicting words from location in lexical dialectology, and\nevaluate it using the DARE dataset.", "id": "480", "start_time": "14:05", "authors": "Afshin Rahimi, Timothy Baldwin and Trevor Cohn"}, {"title": "Colors in Context: A Pragmatic Neural Model for Grounded Language Understanding", "venue": "TACL", "end_time": "14:55", "abstract": "We present a model of pragmatic referring expression interpretation in a\ngrounded communication task (identifying colors from descriptions) that draws\nupon predictions from two recurrent neural network classifiers, a speaker and a\nlistener, unified by a recursive pragmatic reasoning framework. Experiments\nshow that this combined pragmatic model interprets color descriptions more\naccurately than the classifiers from which it is built, and that much of this\nimprovement results from combining the speaker and listener perspectives. We\nobserve that pragmatic reasoning helps primarily in the hardest cases: when the\nmodel must distinguish very similar colors, or when few utterances adequately\nexpress the target color. Our findings make use of a newly-collected corpus of\nhuman utterances in color reference games, which exhibit a variety of pragmatic\nbehaviors. We also show that the embedded speaker model reproduces many of\nthese pragmatic behaviors.", "id": "1531", "start_time": "14:30", "authors": "Will Monroe, Robert X. D. Hawkins, Noah D. Goodman and Christopher Potts"}, {"title": "Obj2Text: Generating Visually Descriptive Language from Object Layouts", "venue": "ACL", "end_time": "15:20", "abstract": "Generating captions for images is a task that has recently received\nconsiderable attention. Another type of visual inputs are abstract scenes or\nobject layouts where the only information provided is a set of objects and\ntheir locations. This type of imagery is commonly found in many applications in\ncomputer graphics, virtual reality, and storyboarding. We explore in this paper\nOBJ2TEXT, a sequence-to-sequence model that encodes a set of objects and their\nlocations as an input sequence using an LSTM network, and decodes this\nrepresentation using an LSTM language model. We show in our paper that this\nmodel despite using a sequence encoder can effectively represent complex\nspatial object-object relationships and produce descriptions that are globally\ncoherent and semantically relevant. We test our approach for the task of\ndescribing object layouts in the MS-COCO dataset by producing sentences given\nonly object annotations. We additionally show that our model combined with a\nstate-of-the-art object detector can improve the accuracy of an image\ncaptioning model.", "id": "1367", "start_time": "14:55", "authors": "Xuwang Yin and Vicente Ordonez"}], "chair": "Yejin Choi"}, {"title": "Discourse and Summarization", "start_time_iso": "2017-09-09T13:40:00", "end_time_iso": "2017-09-09T15:20:00", "type": "paper", "posters": [], "chair_affiliation": "Northeastern University", "code": "2C", "room": "Zealand", "end_time": "15:20", "id": "14", "start_time": "13:40", "talks": [{"title": "End-to-end Neural Coreference Resolution", "venue": "ACL", "end_time": "14:05", "abstract": "We introduce the first end-to-end coreference resolution model and show that it\nsignificantly outperforms all previous work without using a syntactic parser or\nhand-engineered mention detector. The key idea is to directly consider all\nspans in a document as potential mentions and learn distributions over possible\nantecedents for each. The model computes span embeddings that combine\ncontext-dependent boundary representations with a head-finding attention\nmechanism. It is trained to maximize the marginal likelihood of gold antecedent\nspans from coreference clusters and is factored to enable aggressive pruning of\npotential mentions. Experiments demonstrate state-of-the-art performance, with\na gain of 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5-model\nensemble, despite the fact that this is the first approach to be successfully\ntrained with no external resources.", "id": "912", "start_time": "13:40", "authors": "Kenton Lee, Luheng He, Mike Lewis and Luke Zettlemoyer"}, {"title": "Neural Net Models of Open-domain Discourse Coherence", "venue": "ACL", "end_time": "14:30", "abstract": "Discourse coherence is strongly associated with text quality,\nmaking it important to natural language generation and understanding.\nYet existing models of coherence focus on measuring individual aspects of\ncoherence\n(lexical overlap, rhetorical structure, entity centering) in narrow domains.\n\nIn this paper, we describe domain-independent neural models\nof discourse coherence that are capable of measuring multiple aspects of\ncoherence \nin existing sentences and can maintain coherence while generating new\nsentences.\nWe study both\ndiscriminative models that learn to distinguish coherent from incoherent\ndiscourse,\nand generative models that produce coherent text,\nincluding a novel neural latent-variable Markovian generative model that \ncaptures the latent discourse dependencies between sentences in a text.\n\nOur work achieves state-of-the-art performance on multiple coherence\nevaluations,\nand marks an initial step in generating coherent texts given discourse\ncontexts.", "id": "20", "start_time": "14:05", "authors": "Jiwei Li and Dan Jurafsky"}, {"title": "Affinity-Preserving Random Walk for Multi-Document Summarization", "venue": "ACL", "end_time": "14:55", "abstract": "Multi-document summarization provides users with a short text that summarizes\nthe information in a set of related documents. This paper introduces\naffinity-preserving random walk to the summarization task, which preserves the\naffinity relations of sentences by an absorbing random walk model. Meanwhile,\nwe put forward adjustable affinity-preserving random walk to enforce the\ndiversity constraint of summarization in the random walk process. The ROUGE\nevaluations on DUC 2003 topic-focused summarization task and DUC 2004 generic\nsummarization task show the good performance of our method, which has the best\nROUGE-2 recall among the graph-based ranking methods.", "id": "524", "start_time": "14:30", "authors": "Kexiang Wang, Tianyu Liu, Zhifang Sui and Baobao Chang"}, {"title": "A Mention-Ranking Model for Abstract Anaphora Resolution", "venue": "ACL", "end_time": "15:20", "abstract": "Resolving abstract anaphora is an important, but difficult task for text\nunderstanding. Yet, with recent advances in representation learning this task\nbecomes a more tangible aim. A central property of abstract anaphora is that it\nestablishes a relation between the anaphor embedded in the anaphoric sentence\nand its (typically non-nominal) antecedent. We propose a mention-ranking model\nthat learns how abstract anaphors relate to their antecedents with an\nLSTM-Siamese Net. We overcome the lack of training data by generating\nartificial anaphoric sentence--antecedent pairs. Our model outperforms\nstate-of-the-art results on shell noun resolution. We  also report first\nbenchmark results on an abstract anaphora subset of the ARRAU corpus. This\ncorpus presents a greater challenge due to a mixture of nominal and pronominal\nanaphors and a greater range of confounders. We found model variants that\noutperform the baselines for nominal anaphors, without training on individual\nanaphor data, but still lag behind for pronominal anaphors. Our model selects\nsyntactically plausible candidates and -- if disregarding syntax --\ndiscriminates candidates using deeper features.", "id": "779", "start_time": "14:55", "authors": "Ana Marasovic, Leo Born, Juri Opitz and Anette Frank"}], "chair": "Lu Wang"}, {"title": "Poster Session. Embeddings", "start_time_iso": "2017-09-09T13:40:00", "end_time_iso": "2017-09-09T15:20:00", "type": "poster", "posters": [{"title": "Hierarchical Embeddings for Hypernymy Detection and Directionality", "id": "188", "venue": "ACL", "authors": "Kim Anh Nguyen, Maximilian K\u00f6per, Sabine Schulte im Walde and Ngoc Thang Vu", "abstract": "We present a novel neural model HyperVec to learn hierarchical embeddings for\nhypernymy detection and directionality. While previous embeddings have shown\nlimitations on prototypical hypernyms, HyperVec represents an unsupervised\nmeasure where embeddings are learned in a specific order and capture the\nhypernym\u2013hyponym distributional hierarchy. Moreover, our model is able to\ngeneralize over unseen hypernymy pairs, when using only small sets of training\ndata, and by mapping to other languages. Results on benchmark datasets show\nthat HyperVec outperforms both state-of-the- art unsupervised measures and\nembedding models on hypernymy detection and directionality, and on predicting\ngraded lexical entailment."}, {"title": "Ngram2vec: Learning Improved Word Representations from Ngram Co-occurrence Statistics", "id": "193", "venue": "ACL", "authors": "Zhe Zhao, Tao Liu, Shen Li, Bofang Li and Xiaoyong Du", "abstract": "The existing word representation methods mostly limit their information source\nto word co-occurrence statistics. In this paper, we introduce ngrams into four\nrepresentation methods: SGNS, GloVe, PPMI matrix, and its SVD factorization.\nComprehensive experiments are conducted on word analogy and similarity tasks.\nThe results show that improved word representations are learned from ngram\nco-occurrence statistics. We also demonstrate that the trained ngram\nrepresentations are useful in many aspects such as finding antonyms and\ncollocations. Besides, a novel approach of building co-occurrence matrix is\nproposed to alleviate the hardware burdens brought by ngrams."}, {"title": "Dict2vec : Learning Word Embeddings using Lexical Dictionaries", "id": "663", "venue": "ACL", "authors": "Julien Tissier, Christopher Gravier and Amaury Habrard", "abstract": "Learning word embeddings on large unlabeled corpus has been shown to be\nsuccessful in improving many natural language tasks. The most efficient and\npopular approaches learn or retrofit such representations using additional\nexternal data. Resulting embeddings are generally better than their corpus-only\ncounterparts, although such resources cover a fraction of words in the\nvocabulary. In this paper, we propose a new approach, Dict2vec, based on one of\nthe largest yet refined datasource for describing words \u2013 natural language\ndictionaries. Dict2vec builds new word pairs from dictionary entries so that\nsemantically-related words are moved closer, and negative sampling filters out\npairs whose words are unrelated in dictionaries. We evaluate the word\nrepresentations obtained using Dict2vec on eleven datasets for the word\nsimilarity task and on four datasets for a text classification task."}, {"title": "Learning Chinese Word Representations From Glyphs Of Characters", "id": "1197", "venue": "ACL", "authors": "Tzu-ray Su and Hung-yi Lee", "abstract": "In this paper, we propose new methods to learn Chinese word representations.\nChinese characters are composed of graphical components, which carry rich\nsemantics. It is common for a Chinese learner to comprehend the meaning of a\nword from these graphical components. As a result, we propose models that\nenhance word representations by character glyphs. The character glyph features\nare directly learned from the bitmaps of characters by convolutional\nauto-encoder(convAE), and the glyph features improve Chinese word\nrepresentations which are already enhanced by character embeddings. Another\ncontribution in this paper is that we created several evaluation datasets in\ntraditional Chinese and made them public."}, {"title": "Learning Paraphrastic Sentence Embeddings from Back-Translated Bitext", "id": "1396", "venue": "ACL", "authors": "John Wieting, Jonathan Mallinson and Kevin Gimpel", "abstract": "We consider the problem of learning general-purpose, paraphrastic sentence\nembeddings in the setting of Wieting et al. (2016b). We use neural machine\ntranslation to generate sentential paraphrases via back-translation of\nbilingual sentence pairs. We evaluate the paraphrase pairs by their ability to\nserve as training data for learning paraphrastic sentence embeddings. We find\nthat the data quality is stronger than prior work based on bitext and on par\nwith manually-written English paraphrase pairs, with the advantage that our\napproach can scale up to generate large training sets for many languages and\ndomains. We experiment with several language pairs and data sources, and\ndevelop a variety of data filtering techniques. In the process, we explore how\nneural machine translation output differs from human-written sentences, finding\nclear differences in length, the amount of repetition, and the use of rare\nwords."}, {"title": "Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components", "id": "102", "venue": "ACL", "authors": "Jinxing Yu, Xun Jian, Hao Xin and Yangqiu Song", "abstract": "Word embeddings have attracted much attention recently. Different from\nalphabetic writing systems, Chinese characters are often composed of\nsubcharacter components which are also semantically informative. In this work,\nwe propose an approach to jointly embed Chinese words as well as their\ncharacters and fine-grained subcharacter components. We use three likelihoods\nto evaluate whether the context words, characters, and components can predict\nthe current target word, and collected 13,253 subcharacter components to\ndemonstrate the existing approaches of decomposing Chinese characters are not\nenough. Evaluation on both word similarity and word analogy tasks demonstrates\nthe superior performance of our model."}, {"title": "Exploiting Morphological Regularities in Distributional Word Representations", "id": "427", "venue": "ACL", "authors": "Arihant Gupta, Syed Sarfaraz Akhtar, Avijit Vajpayee, Arjit Srivastava, Madan Gopal Jhanwar and Manish Shrivastava", "abstract": "We present an unsupervised, language agnostic approach for exploiting\nmorphological regularities present in high dimensional vector spaces. We\npropose a novel method for generating embeddings of words from their\nmorphological variants using morphological transformation operators. We\nevaluate this approach on MSR word analogy test set with an accuracy of 85%\nwhich is 12% higher than the previous best known system."}, {"title": "Exploiting Word Internal Structures for Generic Chinese Sentence Representation", "id": "600", "venue": "ACL", "authors": "Shaonan Wang, Jiajun Zhang and Chengqing Zong", "abstract": "We introduce a novel mixed characterword architecture to improve Chinese\nsentence representations, by utilizing rich semantic information of word\ninternal structures. Our architecture uses two key strategies. The first is a\nmask gate on characters, learning the relation among characters in a word. The\nsecond is a maxpooling operation on words, adaptively finding the optimal\nmixture of the atomic and compositional word representations. Finally, the\nproposed architecture is applied to various sentence composition models, which\nachieves substantial performance gains over baseline models on sentence\nsimilarity task."}, {"title": "High-risk learning: acquiring new word vectors from tiny data", "id": "853", "venue": "ACL", "authors": "Aur\u00e9lie Herbelot and Marco Baroni", "abstract": "Distributional semantics models are known to struggle with small data. It is\ngenerally accepted that in order to learn 'a good vector' for a word, a model\nmust have sufficient examples of its usage. This contradicts the fact that\nhumans can guess the meaning of a word from a few occurrences only.  In this\npaper, we show that a neural language model such as Word2Vec only necessitates\nminor modifications to its standard architecture to learn new terms from tiny\ndata, using background knowledge from a previously learnt semantic space. We\ntest our model on word definitions and on a nonce task involving 2-6 sentences'\nworth of context, showing a large increase in performance over state-of-the-art\nmodels on the definitional task."}, {"title": "Word Embeddings based on Fixed-Size Ordinally Forgetting Encoding", "id": "884", "venue": "ACL", "authors": "Joseph Sanu, Mingbin Xu, Hui Jiang and Quan Liu", "abstract": "In this paper, we propose to learn word embeddings based on the recent\nfixed-size ordinally forgetting encoding (FOFE) method, which can almost\nuniquely encode any variable-length sequence into a fixed-size representation.\nWe use FOFE to fully encode the left and right context of each word in a corpus\nto construct a novel word-context matrix, which is further weighted and\nfactorized using truncated SVD to generate low-dimension word embedding\nvectors. We evaluate this alternate method in encoding word-context statistics\nand show the new FOFE method has a notable effect on the resulting word\nembeddings. Experimental results on several popular word similarity tasks have\ndemonstrated that the proposed method  outperforms other SVD models that use\ncanonical count based techniques to generate word context matrices."}, {"title": "VecShare: A Framework for Sharing Word Representation Vectors", "id": "990", "venue": "ACL", "authors": "Jared Fernandez, Zhaocheng Yu and Doug Downey", "abstract": "Many Natural Language Processing (NLP) models rely on distributed vector\nrepresentations of words.  Because the process of training word vectors can\nrequire large amounts of data and computation, NLP researchers and\npractitioners often utilize pre-trained embeddings downloaded from the Web. \nHowever, finding the best embeddings for a given task is difficult, and can be\ncomputationally prohibitive.  We present a framework, called VecShare, that\nmakes it easy to share and retrieve word embeddings on the Web.  The framework\nleverages a public data-sharing infrastructure to host embedding sets, and\nprovides automated mechanisms for retrieving the embeddings most similar to a\ngiven corpus.  We perform an experimental evaluation of VecShare's similarity\nstrategies, and show that they are effective at efficiently retrieving\nembeddings that boost accuracy in a document classification task.  Finally, we\nprovide an open-source Python library for using the VecShare framework."}, {"title": "Word Re-Embedding via Manifold Dimensionality Retention", "id": "898", "venue": "ACL", "authors": "Souleiman Hasan and Edward Curry", "abstract": "Word embeddings seek to recover a Euclidean metric space by mapping words into\nvectors, starting from words co-occurrences in a corpus. Word embeddings may\nunderestimate the similarity between nearby words, and overestimate it between\ndistant words in the Euclidean metric space. In this paper, we re-embed\npre-trained word embeddings with a stage of manifold learning which retains\ndimensionality. We show that this approach is\ntheoretically founded in the metric recovery paradigm, and empirically show\nthat it can improve on state-of-the-art embeddings in word similarity tasks 0.5\n- 5.0% points depending on the original space."}, {"title": "MUSE: Modularizing Unsupervised Sense Embeddings", "id": "1302", "venue": "ACL", "authors": "Guang-He Lee and Yun-Nung Chen", "abstract": "This paper proposes to address the word sense ambiguity issue in an\nunsupervised manner, where word sense representations are learned along a word\nsense selection mechanism given contexts. Prior work focused on designing a\nsingle model to deliver both mechanisms, and thus suffered from either\ncoarse-grained representation learning or inefficient sense selection. The\nproposed modular approach, MUSE, implements flexible modules to optimize\ndistinct mechanisms, achieving the first purely sense-level representation\nlearning system with linear-time sense selection. We leverage reinforcement\nlearning to enable joint training on the proposed modules, and introduce\nvarious exploration techniques on sense selection for better robustness. The\nexperiments on benchmark data show that the proposed approach achieves the\nstate-of-the-art performance on synonym selection as well as on contextual word\nsimilarities in terms of MaxSimC."}], "chair_affiliation": "LMU Munich", "code": "2D", "room": "Aarhus", "end_time": "15:20", "id": "15", "start_time": "13:40", "talks": [], "chair": "Heike Adel"}, {"title": "Poster Session. Machine Learning 1", "start_time_iso": "2017-09-09T13:40:00", "end_time_iso": "2017-09-09T15:20:00", "type": "poster", "posters": [{"title": "Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging", "id": "65", "venue": "ACL", "authors": "Nils Reimers and Iryna Gurevych", "abstract": "In this paper we show that reporting a single performance score is insufficient\nto compare non-deterministic approaches. We demonstrate for common sequence\ntagging tasks that the seed value for the random number generator can result in\nstatistically significant p < 10^-4 differences for state-of-the-art systems.\nFor two recent systems for NER, we observe an absolute difference of one\npercentage point F_1-score depending on the selected seed value, making these\nsystems perceived either as state-of-the-art or mediocre. Instead of publishing\nand reporting single performance scores, we propose to compare score\ndistributions based on multiple executions. \nBased on the evaluation of 50.000 LSTM-networks for five sequence tagging\ntasks, we present network architectures that produce both superior performance\nas well as are more stable with respect to the remaining hyperparameters."}, {"title": "Learning What's Easy: Fully Differentiable Neural Easy-First Taggers", "id": "111", "venue": "ACL", "authors": "Andr\u00e9 F. T. Martins and Julia Kreutzer", "abstract": "We introduce a novel neural easy-first decoder that learns to solve sequence\ntagging tasks in a flexible order. In contrast to previous easy-first decoders,\nour models are end-to-end differentiable. The decoder iteratively updates a\n\u201csketch\u201d of the predictions over the sequence. At its core is an attention\nmechanism that controls which parts of the input are strategically the best to\nprocess next. We present a new constrained softmax transformation that ensures\nthe same cumulative attention to every word, and show how to efficiently\nevaluate and backpropagate over it. Our models compare favourably to BILSTM\ntaggers on three sequence tagging tasks."}, {"title": "Incremental Skip-gram Model with Negative Sampling", "id": "516", "venue": "ACL", "authors": "Nobuhiro Kaji and Hayato Kobayashi", "abstract": "This paper explores an incremental training strategy for the skip-gram model\nwith negative sampling (SGNS) from both empirical and theoretical perspectives.\nExisting methods of neural word embeddings, including SGNS, are multi-pass\nalgorithms and thus cannot perform incremental model update. To address this\nproblem, we present a simple incremental extension of SGNS and provide a\nthorough theoretical analysis to demonstrate its validity. Empirical\nexperiments demonstrated the correctness of the theoretical analysis as well as\nthe practical usefulness of the incremental algorithm."}, {"title": "Learning to select data for transfer learning with Bayesian Optimization", "id": "737", "venue": "ACL", "authors": "Sebastian Ruder and Barbara Plank", "abstract": "Domain similarity measures can be used to gauge adaptability and select\nsuitable data for transfer learning, but existing approaches define ad hoc\nmeasures that are deemed suitable for respective tasks. Inspired by work on\ncurriculum learning, we propose to learn data selection measures using Bayesian\nOptimization and evaluate them across models, domains and tasks.\nOur learned measures outperform existing domain similarity measures\nsignificantly on three tasks: sentiment analysis, part-of-speech tagging, and\nparsing.  We show the importance of complementing similarity with diversity,\nand that learned measures are--to some degree--transferable across models,\ndomains, and even tasks."}, {"title": "Unsupervised Pretraining for Sequence to Sequence Learning", "id": "754", "venue": "ACL", "authors": "Prajit Ramachandran, Peter Liu and Quoc Le", "abstract": "This work presents a general unsupervised learning method to improve the\naccuracy of sequence to sequence (seq2seq) models. In our method, the weights\nof the encoder and decoder of a seq2seq model are initialized with the\npretrained weights of two language models and then fine-tuned with labeled\ndata. We apply this method to challenging benchmarks in machine translation and\nabstractive summarization and find that it significantly improves the\nsubsequent supervised models.  Our main result is that pretraining improves the\ngeneralization of seq2seq models. We achieve state-of-the-art results on the\nWMT English$\\rightarrow$German task, surpassing a range of methods using both\nphrase-based machine translation and neural machine translation. Our method\nachieves a significant improvement of 1.3 BLEU from th previous best models on\nboth WMT'14 and WMT'15 English$\\rightarrow$German. We also conduct human\nevaluations on abstractive summarization and find that our method outperforms a\npurely supervised learning baseline in a statistically significant manner."}, {"title": "Efficient Attention using a Fixed-Size Memory Representation", "id": "1046", "venue": "ACL", "authors": "Denny Britz, Melody Guan and Minh-Thang Luong", "abstract": "The standard content-based attention mechanism typically used in\nsequence-to-sequence models is computationally expensive as it requires the\ncomparison of large encoder and decoder states at each time step. In this work,\nwe propose an alternative attention mechanism based on a fixed size memory\nrepresentation that is more efficient. Our technique predicts a compact set of\nK attention contexts during encoding and lets the decoder compute an efficient\nlookup that does not need to consult the memory. We show that our approach\nperforms on-par with the standard attention mechanism while yielding inference\nspeedups of 20% for real-world translation tasks and more for tasks with longer\nsequences. By visualizing attention scores we demonstrate that our models learn\ndistinct, meaningful alignments."}, {"title": "Rotated Word Vector Representations and their Interpretability", "id": "1211", "venue": "ACL", "authors": "Sungjoon Park, JinYeong Bak and Alice Oh", "abstract": "Vector representation of words improves performance in various NLP tasks, but\nthe high dimensional word vectors are very difficult to interpret. We apply\nseveral rotation algorithms to the vector representation of words to improve\nthe interpretability. Unlike previous approaches that induce sparsity, the\nrotated vectors are interpretable while preserving the expressive performance\nof the original vectors. Furthermore, any prebuilt word vector representation\ncan be rotated for improved interpretability. We apply rotation to skipgrams\nand glove and compare the expressive power and interpretability with the\noriginal vectors and the sparse overcomplete vectors. The results show that the\nrotated vectors outperform the original and the sparse overcomplete vectors for\ninterpretability and expressiveness tasks."}, {"title": "A causal framework for explaining the predictions of black-box sequence-to-sequence models", "id": "1260", "venue": "ACL", "authors": "David Alvarez-Melis and Tommi Jaakkola", "abstract": "We interpret the predictions of any black-box structured input-structured\noutput model around a specific input-output pair. Our method returns an\n\"explanation\" consisting of groups of input-output tokens that are causally\nrelated.  These dependencies are inferred by querying the model with perturbed\ninputs, generating a graph over tokens from the responses, and solving a\npartitioning problem to select the most relevant components. We focus the\ngeneral approach on sequence-to-sequence problems, adopting a variational\nautoencoder to yield meaningful input perturbations. We test our method across\nseveral NLP sequence generation tasks."}, {"title": "Piecewise Latent Variables for Neural Variational Text Processing", "id": "1462", "venue": "ACL", "authors": "Iulian Vlad Serban, Alexander G. Ororbia, Joelle Pineau and Aaron Courville", "abstract": "Advances in neural variational inference have facilitated the learning of\npowerful directed graphical models with continuous latent variables, such as\nvariational autoencoders. The hope is that such models will learn to represent\nrich, multi-modal latent factors in real-world data, such as natural language\ntext. However, current models often assume simplistic priors on the latent\nvariables - such as the uni-modal Gaussian distribution - which are incapable\nof representing complex latent factors efficiently. To overcome this\nrestriction, we propose the simple, but highly flexible, piecewise constant\ndistribution. This distribution has the capacity to represent an exponential\nnumber of modes of a latent target distribution, while remaining mathematically\ntractable. Our results demonstrate that incorporating this new latent\ndistribution into different models yields substantial improvements in natural\nlanguage processing tasks such as document modeling and natural language\ngeneration for dialogue."}, {"title": "Learning the Structure of Variable-Order CRFs: a finite-state perspective", "id": "690", "venue": "ACL", "authors": "Thomas Lavergne and Fran\u00e7ois Yvon", "abstract": "The computational complexity of linear-chain Conditional Random Fields (CRFs)\nmakes it difficult to deal with very large label sets and long range\ndependencies. Such situations are not rare and arise when dealing with\nmorphologically rich languages or joint labelling tasks. We extend here recent\nproposals to consider variable order CRFs. Using an effective finite-state\nrepresentation of variable-length dependencies, we propose new ways to perform\nfeature selection at large scale and report experimental results where we\noutperform strong baselines on a tagging task."}, {"title": "Sparse Communication for Distributed Gradient Descent", "id": "864", "venue": "ACL", "authors": "Alham Fikri Aji and Kenneth Heafield", "abstract": "We make distributed stochastic gradient descent faster by exchanging sparse\nupdates instead of dense updates. Gradient updates are positively skewed as\nmost updates are near zero, so we map the 99% smallest updates (by absolute\nvalue) to zero then exchange sparse matrices. This method can be combined with\nquantization to further improve the compression. We explore different\nconfigurations and apply them to neural machine translation and MNIST image\nclassification tasks. Most configurations work on MNIST, whereas different\nconfigurations reduce convergence rate on the more complex translation task.\nOur experiments show that we can achieve up to 49% speed up on MNIST and 22% on\nNMT without damaging the final accuracy or BLEU."}, {"title": "Why ADAGRAD Fails for Online Topic Modeling", "id": "1229", "venue": "ACL", "authors": "You Lu, Jeffrey Lund and Jordan Boyd-Graber", "abstract": "Online topic modeling, i.e., topic modeling\nwith stochastic variational inference, is a\npowerful and efficient technique for analyzing\nlarge datasets, and ADAGRAD is a\nwidely-used technique for tuning learning\nrates during online gradient optimization.\nHowever, these two techniques do not work\nwell together. We show that this is because\nADAGRAD uses accumulation of previous\ngradients as the learning rates\u2019 denominators.\nFor online topic modeling, the magnitude\nof gradients is very large. It causes\nlearning rates to shrink very quickly, so the\nparameters cannot fully converge until the\ntraining ends"}], "chair_affiliation": "University College London", "code": "2E", "room": "Odense", "end_time": "15:20", "id": "16", "start_time": "13:40", "talks": [], "chair": "Pontus Stenetorp"}, {"title": "Poster Session. Sentiment Analysis 1", "start_time_iso": "2017-09-09T13:40:00", "end_time_iso": "2017-09-09T15:20:00", "type": "poster", "posters": [{"title": "Recurrent Attention Network on Memory for Aspect Sentiment Analysis", "id": "215", "venue": "ACL", "authors": "Peng Chen, Zhongqian Sun, Lidong Bing and Wei Yang", "abstract": "We propose a novel framework based on neural networks to identify the sentiment\nof opinion targets in a comment/review. Our framework adopts multiple-attention\nmechanism to capture sentiment features separated by a long distance, so that\nit is more robust against irrelevant information. The results of multiple\nattentions are non-linearly combined with a recurrent neural network, which\nstrengthens the expressive power of our model for handling more complications.\nThe weighted-memory mechanism not only helps us avoid the labor-intensive\nfeature engineering work, but also provides a tailor-made memory for different\nopinion targets of a sentence. We examine the merit of our model on four\ndatasets: two are from SemEval2014, i.e. reviews of restaurants and laptops; a\ntwitter dataset, for testing its performance on social media data; and a\nChinese news comment dataset, for testing its language sensitivity. The\nexperimental results show that our model consistently outperforms the\nstate-of-the-art methods on different types of data."}, {"title": "A Cognition Based Attention Model for Sentiment Analysis", "id": "378", "venue": "ACL", "authors": "Yunfei Long, Lu Qin, Rong Xiang, Minglei Li and Chu-Ren Huang", "abstract": "Attention models are proposed in sentiment analysis because some words are more\nimportant than others. However,most existing methods either use local context\nbased text information or user preference information. In this work, we propose\na novel attention model trained by cognition grounded eye-tracking data. A\nreading prediction model is first built using eye-tracking data as dependent\ndata and other features in the context as independent data. The predicted\nreading time is then used to build a cognition based attention (CBA) layer for\nneural sentiment analysis. As a comprehensive model, We can capture attentions\nof words in sentences as well as sentences in documents. Different attention\nmechanisms can also be incorporated to capture other aspects of attentions.\nEvaluations show the CBA based method outperforms the state-of-the-art local\ncontext based attention methods significantly. This brings insight to how\ncognition grounded data can be brought into NLP tasks."}, {"title": "Author-aware Aspect Topic Sentiment Model to Retrieve Supporting Opinions from Reviews", "id": "623", "venue": "ACL", "authors": "Lahari Poddar, Wynne Hsu and Mong Li Lee", "abstract": "User generated content about products and services in the form of reviews are\noften diverse and even contradictory. This makes it difficult for users to know\nif an opinion in a review is prevalent or biased.\nWe study the problem of searching for supporting opinions in the context of\nreviews. We propose a framework called SURF, that first identifies opinions\nexpressed in a review, and then finds similar opinions from other reviews. We\ndesign a novel probabilistic graphical model that captures opinions as a\ncombination of aspect, topic and sentiment dimensions, takes into account the\npreferences of individual authors, as well as the quality of the  entity under\nreview, and encodes the flow of thoughts in a review by constraining the aspect\ndistribution dynamically among successive review segments. We derive a\nsimilarity measure that  considers both lexical and semantic similarity to find\nsupporting opinions. Experiments on TripAdvisor hotel reviews and Yelp\nrestaurant reviews  show that  our model outperforms  existing methods for\nmodeling opinions, and the proposed framework is effective in finding\nsupporting opinions."}, {"title": "Magnets for Sarcasm: Making Sarcasm Detection Timely, Contextual and Very Personal", "id": "988", "venue": "ACL", "authors": "Aniruddha Ghosh and Tony Veale", "abstract": "Sarcasm is a pervasive phenomenon in social media, permitting the concise\ncommunication of meaning, affect and attitude. Concision requires wit to\nproduce and wit to understand, which demands from each party knowledge of\nnorms, context and a speaker's mindset. Insight into a speaker's psychological\nprofile at the time of production is a valuable source of context for sarcasm\ndetection. Using a neural architecture, we show significant gains in detection\naccuracy when knowledge of the speaker's mood at the time of production can be\ninferred. Our focus is on sarcasm detection on Twitter, and show that the mood\nexhibited by a speaker over tweets leading up to a new post is as useful a cue\nfor sarcasm as the topical context of the post itself. The work opens the door\nto an empirical exploration not just of sarcasm in text but of the sarcastic\nstate of mind."}, {"title": "Identifying Humor in Reviews using Background Text Sources", "id": "1118", "venue": "ACL", "authors": "Alex Morales and Chengxiang Zhai", "abstract": "We study the problem of automatically identifying humorous text from a new kind\nof text data, i.e., online reviews. We propose a generative language model,\nbased on the theory of incongruity, to model humorous text, which allows us to\nleverage background text sources, such as Wikipedia entry descriptions, and\nenables construction of multiple features for identifying humorous reviews.\nEvaluation of these features using supervised learning for classifying reviews\ninto humorous and non-humorous reviews shows that the features constructed\nbased on the proposed generative model are much more effective than the major\nfeatures proposed in the existing literature, allowing us to achieve almost\n86\\% accuracy. These humorous review predictions can also supply good\nindicators for identifying helpful reviews."}, {"title": "Sentiment Lexicon Construction with Representation Learning Based on Hierarchical Sentiment Supervision", "id": "1169", "venue": "ACL", "authors": "Leyi Wang and Rui Xia", "abstract": "Sentiment lexicon is an important tool for identifying the sentiment polarity\nof words and texts. How to automatically construct sentiment lexicons has\nbecome a research topic in the field of sentiment analysis and opinion mining.\nRecently there were some attempts to employ representation learning algorithms\nto construct a sentiment lexicon with sentiment-aware word embedding. However,\nthese methods were normally trained under document-level sentiment supervision.\nIn this paper, we develop a neural architecture to train a sentiment-aware word\nembedding by integrating the sentiment supervision at both  document and word\nlevels, to enhance the quality of word embedding as well as the sentiment\nlexicon. Experiments on the SemEval 2013-2016 datasets indicate that the\nsentiment lexicon generated by our approach achieves the state-of-the-art\nperformance in both supervised and unsupervised sentiment classification, in\ncomparison with several strong sentiment lexicon construction methods."}, {"title": "Towards a Universal Sentiment Classifier in Multiple languages", "id": "1399", "venue": "ACL", "authors": "Kui Xu and Xiaojun Wan", "abstract": "Existing sentiment classifiers usually work for only one specific language, and\ndifferent classification models are used in different languages. In this paper\nwe aim to build a universal sentiment classifier with a single classification\nmodel in multiple different languages. In order to achieve this goal, we\npropose to learn multilingual sentiment-aware word embeddings simultaneously\nbased only on the labeled reviews in English and unlabeled parallel data\navailable in a few language pairs. It is not required that the parallel data\nexist between English and any other language, because the sentiment information\ncan be transferred into any language via pivot languages. We present the\nevaluation results of our universal sentiment classifier in five languages, and\nthe results are very promising even when the parallel data between English and\nthe target languages are not used. Furthermore, the universal single classifier\nis compared with a few cross-language sentiment classifiers relying on direct\nparallel data between the source and target languages, and the results show\nthat the performance of our universal sentiment classifier is very promising\ncompared to that of different cross-language classifiers in multiple target\nlanguages."}, {"title": "Capturing User and Product Information for Document Level Sentiment Analysis with Deep Memory Network", "id": "696", "venue": "ACL", "authors": "Zi-Yi Dou", "abstract": "Document-level sentiment classification is a fundamental problem which aims to\npredict a user's overall sentiment about a \nproduct in a document. Several methods have been proposed to tackle the problem\nwhereas most of them fail to consider the influence of users who express the\nsentiment and products which are evaluated. To address the issue,\nwe propose a deep memory network for document-level sentiment classification\nwhich could capture the user and product information at the same time. To prove\nthe effectiveness of our algorithm, we conduct experiments on IMDB and Yelp\ndatasets and the results indicate that our model can achieve better performance\nthan several existing methods."}, {"title": "Identifying and Tracking Sentiments and Topics from Social Media Texts during Natural Disasters", "id": "948", "venue": "ACL", "authors": "Min Yang, Jincheng Mei, Heng Ji, zhao wei, Zhou Zhao and Xiaojun Chen", "abstract": "We study the problem of identifying the topics and sentiments and tracking\ntheir shifts from social media texts in different geographical regions during\nemergencies and disasters. We propose a location-based dynamic sentiment-topic\nmodel (LDST) which can jointly model topic, sentiment, time and Geolocation\ninformation. The experimental results demonstrate that LDST performs very well\nat discovering topics and sentiments from social media and tracking their\nshifts in different geographical regions during emergencies and disasters. We\nwill release the data and source code after this work is published."}, {"title": "Refining Word Embeddings for Sentiment Analysis", "id": "1126", "venue": "ACL", "authors": "Liang-Chih Yu, Jin Wang, K. Robert Lai and Xuejie Zhang", "abstract": "Word embeddings that can capture semantic and syntactic information from\ncontexts have been extensively used for various natural language processing\ntasks. However, existing methods for learning context-based word embeddings\ntypically fail to capture sufficient sentiment information. This may result in\nwords with similar vector representations having an opposite sentiment polarity\n(e.g., good and bad), thus degrading sentiment analysis performance. Therefore,\nthis study proposes a word vector refinement model that can be applied to any\npre-trained word vectors (e.g., Word2vec and GloVe). The refinement model is\nbased on adjusting the vector representations of words such that they can be\ncloser to both semantically and sentimentally similar words and further away\nfrom sentimentally dissimilar words. Experimental results show that the\nproposed method can improve conventional word embeddings and outperform\npreviously proposed sentiment embeddings for both binary and fine-grained\nclassification on Stanford Sentiment Treebank (SST)."}, {"title": "A Multilayer Perceptron based Ensemble Technique for Fine-grained Financial Sentiment Analysis", "id": "1416", "venue": "ACL", "authors": "Md Shad Akhtar, Abhishek Kumar, Deepanway Ghosal, Asif Ekbal and Pushpak Bhattacharyya", "abstract": "In this paper, we propose a novel method for combining deep learning and\nclassical feature based models using a Multi-Layer Perceptron (MLP) network for\nfinancial sentiment analysis. We develop various deep learning models based on\nConvolutional Neural Network (CNN), Long Short Term Memory (LSTM) and Gated\nRecurrent Unit (GRU). These are trained on top of pre-trained,\nautoencoder-based, financial word embeddings and lexicon features. An ensemble\nis constructed by combining these deep learning models and a classical\nsupervised model based on Support Vector Regression (SVR). We evaluate our\nproposed technique on a benchmark dataset of SemEval-2017 shared task on\nfinancial sentiment analysis. The propose model shows impressive results on two\ndatasets, i.e. microblogs and news headlines datasets. Comparisons show that\nour proposed model performs better than the existing state-of-the-art systems\nfor the above two datasets by 2.0 and 4.1 cosine points, respectively."}, {"title": "Sentiment Intensity Ranking among Adjectives Using Sentiment Bearing Word Embeddings", "id": "1436", "venue": "ACL", "authors": "Raksha Sharma, Arpan Somani, Lakshya Kumar and Pushpak Bhattacharyya", "abstract": "Identification of intensity ordering among polar (positive or negative) words\nwhich have the same semantics can lead to a fine-grained sentiment analysis.\nFor\nexample, 'master', 'seasoned' and 'familiar' point to different intensity\nlevels, though they all convey the same meaning (semantics), i.e., expertise:\nhaving a good\nknowledge of. In this paper, we propose a semi-supervised technique that uses\nsentiment\nbearing word embeddings to produce a continuous ranking among adjectives that\nshare common semantics. Our system demonstrates a strong Spearman\u2019s rank\ncorrelation of 0.83 with the gold standard ranking. We show that sentiment\nbearing word embeddings facilitate a more accurate intensity ranking system\nthan other standard word embeddings (word2vec and GloVe). Word2vec is the\nstate-of-the-art for intensity ordering task."}, {"title": "Sentiment Lexicon Expansion Based on Neural PU Learning, Double Dictionary Lookup, and Polarity Association", "id": "723", "venue": "ACL", "authors": "Yasheng Wang, Yang Zhang and Bing Liu", "abstract": "Although many sentiment lexicons in different languages exist, most are not\ncomprehensive. In a recent sentiment analysis application, we used a large\nChinese sentiment lexicon and found that it missed a large number of sentiment\nwords in social media. This prompted us to make a new attempt to study\nsentiment lexicon expansion. This paper first poses the problem as a PU\nlearning problem, which is a new formulation. It then proposes a new PU\nlearning method suitable for our problem using a neural network. The results\nare enhanced further with a new dictionary-based technique and a novel polarity\nclassification technique. Experimental results show that the proposed approach\noutperforms baseline methods greatly."}], "chair_affiliation": "Carnegie Mellon University", "code": "2F", "room": "Copenhagen", "end_time": "15:20", "id": "17", "start_time": "13:40", "talks": [], "chair": "Diyi Yang"}], [{"title": "Machine Learning 2", "start_time_iso": "2017-09-09T15:50:00", "end_time_iso": "2017-09-09T17:30:00", "type": "paper", "posters": [], "chair_affiliation": "DeepMind", "code": "3A", "room": "Jutland", "end_time": "17:30", "id": "18", "start_time": "15:50", "talks": [{"title": "DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning", "venue": "ACL", "end_time": "16:15", "abstract": "We study the problem of learning to reason in large scale knowledge graphs\n(KGs). More specifically, we describe a novel reinforcement learning framework\nfor learning multi-hop relational paths: we use a policy-based agent with\ncontinuous states based on knowledge graph embeddings, which reasons in a KG\nvector-space by sampling the most promising relation to extend its path. In\ncontrast to prior work, our approach includes a reward function that takes the\naccuracy, diversity, and efficiency into consideration. Experimentally, we show\nthat our proposed method outperforms a path-ranking based algorithm and\nknowledge graph embedding methods on Freebase and Never-Ending Language\nLearning datasets.", "id": "1047", "start_time": "15:50", "authors": "Wenhan Xiong, Thien Hoang and William Yang Wang"}, {"title": "Task-Oriented Query Reformulation with Reinforcement Learning", "venue": "ACL", "end_time": "16:40", "abstract": "Search engines play an important role in our everyday lives by assisting us in\nfinding the information we need. When we input a complex query, however,\nresults are often far from satisfactory. In this work, we introduce a query\nreformulation system based on a neural network that rewrites a query to\nmaximize the number of relevant documents returned.\nWe train this neural network with reinforcement learning. The actions\ncorrespond to selecting terms to build a reformulated query, and the reward is\nthe document recall. We evaluate our approach on three datasets against strong\nbaselines and show a relative improvement of 5-20% in terms of recall.\nFurthermore, we present a simple method to estimate a conservative upper-bound\nperformance of a model in a particular environment and verify that there is\nstill large room for improvements.", "id": "434", "start_time": "16:15", "authors": "Rodrigo Nogueira and Kyunghyun Cho"}, {"title": "Sentence Simplification with Deep Reinforcement Learning", "venue": "ACL", "end_time": "17:05", "abstract": "Sentence simplification aims to make sentences easier to read and\n  understand. Most recent approaches draw on insights from machine\n  translation to learn simplification rewrites from monolingual\n  corpora of complex and simple sentences. We address the\n  simplification problem with an encoder-decoder model coupled with a\n  deep reinforcement learning framework. Our model, which we call {\\sc\n    Dress} (as shorthand for {\\bf D}eep {\\bf RE}inforcement {\\bf\n    S}entence {\\bf S}implification), explores the space of possible\n  simplifications while learning to optimize a reward function that\n  encourages outputs which are simple, fluent, and preserve the\n  meaning of the input. Experiments on three datasets demonstrate that\n  our model outperforms competitive simplification\n  systems.", "id": "437", "start_time": "16:40", "authors": "Xingxing Zhang and Mirella Lapata"}, {"title": "Learning how to Active Learn: A Deep Reinforcement Learning Approach", "venue": "ACL", "end_time": "17:30", "abstract": "Active learning aims to select a small subset of data for annotation such that\na classifier learned on the data is highly accurate. This is usually done using\nheuristic selection methods, however the effectiveness of such methods is\nlimited and moreover, the performance of heuristics varies between datasets. To\naddress these shortcomings, we introduce a novel formulation by reframing the\nactive learning as a reinforcement learning problem and explicitly learning a\ndata selection policy, where the policy takes the role of the active learning\nheuristic. Importantly, our method allows the selection policy learned using\nsimulation to one language to be transferred to other languages. We demonstrate\nour method using cross-lingual named entity recognition, observing uniform\nimprovements over traditional active learning algorithms.", "id": "679", "start_time": "17:05", "authors": "Meng Fang, Yuan Li and Trevor Cohn"}], "chair": "Karl Moritz Hermann"}, {"title": "Generation", "start_time_iso": "2017-09-09T15:50:00", "end_time_iso": "2017-09-09T17:30:00", "type": "paper", "posters": [], "chair_affiliation": "Ohio State University", "code": "3B", "room": "Funen", "end_time": "17:30", "id": "19", "start_time": "15:50", "talks": [{"title": "Split and Rephrase", "venue": "ACL", "end_time": "16:15", "abstract": "We propose a new sentence simplification task (Split-and-Rephrase) where the\naim is to split a complex sentence into a meaning preserving sequence of\nshorter sentences. Like sentence simplification, splitting-and-rephrasing has\nthe potential of benefiting both natural language processing and societal\napplications. Because shorter sentences are generally better processed by NLP\nsystems, it could be used as a preprocessing step which facilitates and\nimproves the performance of parsers, semantic role labellers and machine\ntranslation systems. It should also be of use for people with reading\ndisabilities because it allows the conversion of longer sentences into shorter\nones. This paper makes two contributions towards this new task. First, we\ncreate and make available a benchmark consisting of 1,066,115 tuples mapping a\nsingle complex sentence to a sequence of sentences expressing the same meaning.\nSecond, we propose five models (vanilla sequence-to-sequence to\nsemantically-motivated models) to understand the difficulty of the proposed\ntask.", "id": "842", "start_time": "15:50", "authors": "Shashi Narayan, Claire Gardent, Shay B. Cohen and Anastasia Shimorina"}, {"title": "Neural Response Generation via GAN with an Approximate Embedding Layer", "venue": "ACL", "end_time": "16:40", "abstract": "This paper presents a Generative Adversarial Network (GAN) to model single-turn\nshort-text conversations, which trains a sequence-to-sequence (Seq2Seq) network\nfor response generation simultaneously with a discriminative classifier that\nmeasures the differences between human-produced responses and machine-generated\nones. In addition, the proposed method introduces an approximate embedding\nlayer to solve the non-differentiable problem caused by the sampling-based\noutput decoding procedure in the Seq2Seq generative model. The GAN setup\nprovides an effective way to avoid noninformative responses (a.k.a \u201csafe\nresponses\u201d), which are frequently observed in traditional neural response\ngenerators.\nThe experimental results show that the proposed approach significantly\noutperforms existing neural response generation models in diversity metrics,\nwith slight increases in relevance scores as well, when evaluated on both a\nMandarin corpus and an English corpus.", "id": "1344", "start_time": "16:15", "authors": "Zhen Xu, Bingquan Liu, Baoxun Wang, Chengjie SUN, Xiaolong Wang, Zhuoran Wang and Chao Qi"}, {"title": "A Hybrid Convolutional Variational Autoencoder for Text Generation", "venue": "ACL", "end_time": "17:05", "abstract": "In this paper we explore the effect of architectural choices on learning a\nvariational autoencoder (VAE) for text generation. In contrast to the\npreviously introduced VAE model for text where both the encoder and decoder are\nRNNs, we propose a novel hybrid architecture that blends fully feed-forward\nconvolutional and deconvolutional components with a recurrent language model.\nOur architecture exhibits several attractive properties such as faster run time\nand convergence, ability to better handle long sequences and, more importantly,\nit helps to avoid the issue of the VAE collapsing to a deterministic model.", "id": "591", "start_time": "16:40", "authors": "Stanislau Semeniuta, Aliaksei Severyn and Erhardt Barth"}, {"title": "Filling the Blanks (hint: plural noun) for Mad Libs Humor", "venue": "ACL", "end_time": "17:30", "abstract": "Computerized generation of humor is a notoriously difficult AI problem. We\ndevelop an algorithm called Libitum that helps humans generate humor in a Mad\nLib, which is a popular fill-in-the-blank game. The algorithm is based on a\nmachine learned classifier that determines whether a potential fill-in word is\nfunny in the context of the Mad Lib story. We use Amazon Mechanical Turk to\ncreate ground truth data and to judge humor for our classifier to mimic, and we\nmake this data freely available. Our testing shows that Libitum successfully\naids humans in filling in Mad Libs that are usually judged funnier than those\nfilled in by humans with no computerized help. We go on to analyze why some\nwords are better than others at making a Mad Lib funny.", "id": "1035", "start_time": "17:05", "authors": "Nabil Hossain, John Krumm, Lucy Vanderwende, Eric Horvitz and Henry Kautz"}], "chair": "Wei Xu"}, {"title": "Semantics 1", "start_time_iso": "2017-09-09T15:50:00", "end_time_iso": "2017-09-09T17:30:00", "type": "paper", "posters": [], "chair_affiliation": "DeepMind", "code": "3C", "room": "Zealand", "end_time": "17:30", "id": "20", "start_time": "15:50", "talks": [{"title": "Measuring Thematic Fit with Distributional Feature Overlap", "venue": "ACL", "end_time": "16:15", "abstract": "In this paper, we introduce a new distributional method for modeling\npredicate-argument thematic fit judgments. \nWe use a syntax-based DSM to build a prototypical representation of\nverb-specific roles: for every verb, we extract the most salient second order\ncontexts for each of its roles (i.e. the most salient dimensions of typical\nrole fillers), and then we compute thematic fit as a weighted overlap between\nthe top features of candidate fillers and role prototypes.\nOur experiments show that our method consistently outperforms a baseline\nre-implementing a state-of-the-art system, and achieves better or comparable\nresults to those reported in the literature for the other unsupervised systems.\nMoreover, it provides an explicit representation of the features characterizing\nverb-specific semantic roles.", "id": "84", "start_time": "15:50", "authors": "Enrico Santus, Emmanuele Chersoni, Alessandro Lenci and Philippe Blache"}, {"title": "SCDV : Sparse Composite Document Vectors using soft clustering over distributional representations", "venue": "ACL", "end_time": "16:40", "abstract": "We present a feature vector formation technique for documents - Sparse\nComposite Document Vector (SCDV) - which overcomes several shortcomings of the\ncurrent distributional paragraph vector representations that are widely used\nfor text representation. In SCDV, word embeddings are clustered to capture\nmultiple semantic contexts in which words occur. They are then chained together\nto form document topic-vectors that can express complex, multi-topic documents.\nThrough extensive experiments on multi-class and multi-label classification\ntasks, we outperform the previous state-of-the-art method, NTSG. We also show\nthat SCDV embeddings perform well on heterogeneous tasks like Topic Coherence,\ncontext-sensitive Learning and Information Retrieval. Moreover, we achieve a\nsignificant reduction in training and prediction times compared to other\nrepresentation methods. SCDV achieves best of both worlds - better performance\nwith lower time and space complexity.", "id": "822", "start_time": "16:15", "authors": "Dheeraj Mekala, Vivek Gupta, Bhargavi Paranjape and Harish Karnick"}, {"title": "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data", "venue": "ACL", "end_time": "17:05", "abstract": "Many modern NLP systems rely on word embeddings, previously trained in an\nunsupervised manner on large corpora, as base features. Efforts to obtain\nembeddings for larger chunks of text, such as sentences, have however not been\nso successful. Several attempts at learning unsupervised representations of\nsentences have not reached satisfactory enough performance to be widely\nadopted.\nIn this paper, we show how universal sentence representations trained using the\nsupervised data of the Stanford Natural Language Inference datasets can\nconsistently outperform unsupervised methods like SkipThought vectors on a wide\nrange of transfer tasks. Much like how computer vision uses ImageNet to obtain\nfeatures, which can then be transferred to other tasks, our work tends to\nindicate the suitability of natural language inference for transfer learning to\nother NLP tasks. Our encoder is publicly available.", "id": "871", "start_time": "16:40", "authors": "Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00efc Barrault and Antoine Bordes"}, {"title": "Determining Semantic Textual Similarity using Natural Deduction Proofs", "venue": "ACL", "end_time": "17:30", "abstract": "Determining semantic textual similarity is a core research subject in natural\nlanguage processing.\nSince vector-based models for sentence representation often use shallow\ninformation, capturing accurate semantics is difficult. By contrast, logical\nsemantic representations capture deeper levels of sentence semantics, but their\nsymbolic nature does not offer graded notions of textual similarity.\nWe propose a method for determining semantic textual similarity by combining\nshallow features with features extracted from natural deduction proofs of\nbidirectional entailment relations between sentence pairs. For the natural\ndeduction proofs, we use ccg2lambda, a higher-order automatic inference system,\nwhich converts Combinatory Categorial Grammar (CCG) derivation trees into\nsemantic representations and conducts natural deduction proofs. Experiments\nshow that our system was able to outperform other logic-based systems and that\nfeatures derived from the proofs are effective for learning textual similarity.", "id": "624", "start_time": "17:05", "authors": "Hitomi Yanaka, Koji Mineshima, Pascual Mart\u00ednez-G\u00f3mez and Daisuke Bekki"}], "chair": "Felix Hill"}, {"title": "Poster Session. Syntax 2", "start_time_iso": "2017-09-09T15:50:00", "end_time_iso": "2017-09-09T17:30:00", "type": "poster", "posters": [{"title": "Multi-Grained Chinese Word Segmentation", "id": "43", "venue": "ACL", "authors": "Chen Gong, Zhenghua Li, Min Zhang and Xinzhou Jiang", "abstract": "Traditionally, word segmentation (WS) adopts the single-grained formalism,\nwhere a sentence corresponds to a single word sequence. However, Sproat et al.\n(1997) show that the inter-native-speaker consistency ratio over Chinese word\nboundaries is only 76\\%, indicating single-grained WS (SWS) imposes unnecessary\nchallenges on both manual annotation and statistical modeling.\nMoreover, WS results of different granularities can be complementary and\nbeneficial for high-level applications. \n\nThis work proposes and addresses multi-grained WS (MWS). We build a large-scale\npseudo MWS dataset for model training and tuning by leveraging the annotation\nheterogeneity of \nthree SWS datasets.\nThen we manually annotate 1,500 test sentences with true MWS annotations. \nFinally, we propose three benchmark approaches by casting MWS as constituent\nparsing and sequence\nlabeling. \nExperiments and analysis lead to many interesting findings."}, {"title": "Don't Throw Those Morphological Analyzers Away Just Yet: Neural Morphological Disambiguation for Arabic", "id": "996", "venue": "ACL", "authors": "Nasser Zalmout and Nizar Habash", "abstract": "This paper presents a model for Arabic morphological disambiguation based on\nRecurrent Neural Networks (RNN). We train Long Short-Term Memory (LSTM) cells\nin several configurations and embedding levels to model the various\nmorphological features. Our experiments show that these models outperform\nstate-of-the-art systems without explicit use of feature engineering. However,\nadding learning features from a morphological analyzer to model the space of\npossible analyses provides additional improvement.\nWe make use of the resulting morphological models for scoring and ranking the\nanalyses of the morphological analyzer for morphological disambiguation. The\nresults show significant gains in accuracy across several evaluation metrics.\nOur system results in 4.4% absolute increase over the state-of-the-art in full\nmorphological analysis accuracy (30.6% relative error reduction), and 10.6% \n(31.5% relative error reduction) for out-of-vocabulary words."}, {"title": "Paradigm Completion for Derivational Morphology", "id": "127", "venue": "ACL", "authors": "Ryan Cotterell, Ekaterina Vylomova, Huda Khayrallah, Christo Kirov and David Yarowsky", "abstract": "The generation of complex derived word forms has been an overlooked problem in\nNLP; we fill this gap by applying neural sequence-to-sequence models to the\ntask. We overview the theoretical motivation for a paradigmatic treatment of\nderivational morphology, and introduce the task of derivational paradigm\ncompletion as a parallel to inflectional paradigm completion. State-of-the-art\nneural models adapted from the inflection task are able to learn the range of\nderivation patterns, and outperform a non-neural baseline by 16.4%. However,\ndue to semantic, historical, and lexical considerations involved in\nderivational morphology, future work will be needed to achieve performance\nparity with inflection-generating systems."}, {"title": "A Sub-Character Architecture for Korean Language Processing", "id": "650", "venue": "ACL", "authors": "Karl Stratos", "abstract": "We introduce a novel sub-character architecture that exploits a unique\ncompositional structure of the Korean language. Our method decomposes each\ncharacter into a small set of primitive phonetic units called jamo letters from\nwhich character- and word-level representations are induced. The jamo letters\ndivulge syntactic and semantic information that is difficult to access with\nconventional character-level units. They greatly alleviate the data sparsity\nproblem, reducing the observation space to 1.6% of the original while\nincreasing accuracy in our experiments. We apply our architecture to dependency\nparsing and achieve dramatic improvement over strong lexical baselines."}, {"title": "Do LSTMs really work so well for PoS tagging? -- A replication study", "id": "39", "venue": "ACL", "authors": "Tobias Horsmann and Torsten Zesch", "abstract": "A recent study by Plank et al. (2016) found that LSTM-based PoS taggers\nconsiderably improve over the current state-of-the-art when evaluated on the\ncorpora of the Universal Dependencies project that use a coarse-grained tagset.\nWe replicate this study using a fresh collection of 27 corpora of 21 languages\nthat are annotated with fine-grained tagsets of varying size.\nOur replication confirms the result in general, and we additionally find that\nthe advantage of LSTMs is even bigger for larger tagsets.\nHowever, we also find that for the very large tagsets of morphologically rich\nlanguages, hand-crafted morphological lexicons are still necessary to reach\nstate-of-the-art performance."}, {"title": "The Labeled Segmentation of Printed Books", "id": "446", "venue": "ACL", "authors": "Lara McConnaughey, Jennifer Dai and David Bamman", "abstract": "We introduce the task of book structure labeling: segmenting and assigning a\nfixed category (such as Table of Contents, Preface, Index) to the document\nstructure of printed books.  We manually annotate the page-level structural\ncategories for a large dataset totaling 294,816 pages in 1,055 books evenly\nsampled from 1750-1922, and present empirical results comparing the performance\nof several classes of models.  The best-performing model, a bidirectional LSTM\nwith rich features, achieves an overall accuracy of 95.8 and a class-balanced\nmacro F-score of 71.4."}, {"title": "Cross-lingual Character-Level Neural Morphological Tagging", "id": "128", "venue": "ACL", "authors": "Ryan Cotterell and Georg Heigold", "abstract": "Even for common NLP tasks, sufficient supervision is not available in many\nlanguages -- morphological tagging is no exception. In the work presented here,\nwe explore a transfer learning scheme, whereby we train character-level\nrecurrent neural taggers to predict morphological taggings for high-resource\nlanguages and low-resource languages together. Learning joint character\nrepresentations among multiple related languages successfully enables knowledge\ntransfer from the high-resource languages to the low-resource ones."}, {"title": "Word-Context Character Embeddings for Chinese Word Segmentation", "id": "1299", "venue": "ACL", "authors": "Hao Zhou, Zhenting Yu, Yue Zhang, Shujian Huang, XIN-YU DAI and Jiajun Chen", "abstract": "Neural parsers have benefited from automatically labeled data via\ndependency-context word embeddings. \nWe investigate training character embeddings on a word-based context in a\nsimilar way, showing that the simple method improves state-of-the-art neural\nword segmentation models significantly, beating tri-training baselines for\nleveraging auto-segmented data."}, {"title": "Segmentation-Free Word Embedding for Unsegmented Languages", "id": "44", "venue": "ACL", "authors": "Takamasa Oshikiri", "abstract": "In this paper, we propose a new pipeline of word embedding for unsegmented\nlanguages, called segmentation-free word embedding, which does not require word\nsegmentation as a preprocessing step. Unlike space-delimited languages,\nunsegmented languages, such as Chinese and Japanese, require word segmentation\nas a preprocessing step. However, word segmentation, that often requires\nmanually annotated resources, is difficult and expensive, and unavoidable\nerrors in word segmentation affect downstream tasks. To avoid these problems in\nlearning word vectors of unsegmented languages, we consider word co-occurrence\nstatistics over all possible candidates of segmentations based on frequent\ncharacter n-grams instead of segmented sentences provided by conventional word\nsegmenters. Our experiments of noun category prediction tasks on raw Twitter,\nWeibo, and Wikipedia corpora show that the proposed method outperforms the\nconventional approaches that require word segmenters."}], "chair_affiliation": "Georgia Tech", "code": "3D", "room": "Aarhus", "end_time": "17:30", "id": "21", "start_time": "15:50", "talks": [], "chair": "Yuval Pinter"}, {"title": "Poster Session. Question Answering and Machine Comprehension", "start_time_iso": "2017-09-09T15:50:00", "end_time_iso": "2017-09-09T17:30:00", "type": "poster", "posters": [{"title": "From Textbooks to Knowledge: A Case Study in Harvesting Axiomatic Knowledge from Textbooks to Solve Geometry Problems", "id": "22", "venue": "ACL", "authors": "Mrinmaya Sachan, Kumar Dubey and Eric Xing", "abstract": "Textbooks are rich sources of information. Harvesting structured knowledge from\ntextbooks is a key challenge in many educational applications. As a case study,\nwe present an approach for harvesting structured axiomatic knowledge from math\ntextbooks. Our approach uses rich contextual and typographical features\nextracted from raw textbooks. It leverages the redundancy and shared ordering\nacross multiple textbooks to further refine the harvested axioms. These axioms\nare then parsed into rules that are used to improve the state-of-the-art in\nsolving geometry problems."}, {"title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations", "id": "37", "venue": "ACL", "authors": "Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang and Eduard Hovy", "abstract": "We present RACE, a new dataset for benchmark evaluation of methods in the\nreading comprehension task. Collected from the English exams for middle and\nhigh school Chinese students in the age range between 12 to 18, RACE consists\nof near 28,000 passages and near 100,000 questions generated by human experts\n(English instructors), and covers a variety of topics which are carefully\ndesigned for evaluating the students' ability in  understanding and reasoning. \nIn particular, the proportion of questions that requires reasoning is much\nlarger in RACE than that in other benchmark datasets for reading comprehension,\nand there is a significant gap between the performance of the state-of-the-art\nmodels (43%) and the ceiling human performance (95%). We hope this new dataset\ncan serve as a valuable resource for research and evaluation in machine\ncomprehension. The dataset is freely available at\nhttp://www.cs.cmu.edu/~glai1/data/race/ and the code is available at\nhttps://github.com/qizhex/RACE_AR_baselines."}, {"title": "Beyond Sentential Semantic Parsing: Tackling the Math SAT with a Cascade of Tree Transducers", "id": "432", "venue": "ACL", "authors": "Mark Hopkins, Cristian Petrescu-Prahova, Roie Levin, Ronan Le Bras, Alvaro Herrasti and Vidur Joshi", "abstract": "We present an approach for answering questions that span multiple sentences and\nexhibit sophisticated cross-sentence anaphoric phenomena, evaluating on a rich\nsource of such questions -- the math portion of the Scholastic Aptitude Test\n(SAT). By using a tree transducer cascade as its basic architecture, our system\npropagates uncertainty from multiple sources (e.g. coreference resolution or\nverb interpretation) until it can be confidently resolved. Experiments show the\nfirst-ever results 43% recall and 91% precision) on SAT algebra word problems.\nWe also apply our system to the public Dolphin algebra question set, and\nimprove the state-of-the-art F1-score from 73.9% to 77.0%."}, {"title": "Learning Fine-Grained Expressions to Solve Math Word Problems", "id": "627", "venue": "ACL", "authors": "Danqing Huang, Shuming Shi, Chin-Yew Lin and Jian Yin", "abstract": "This paper presents a novel template-based method to solve math word problems.\nThis method learns the mappings between math concept phrases in math word\nproblems and their math expressions from training data. For each equation\ntemplate, we automatically construct a rich template sketch by aggregating\ninformation from various problems with the same template. Our approach is\nimplemented in a two-stage system. It first retrieves a few relevant equation\nsystem templates and aligns numbers in math word problems to those templates\nfor candidate equation generation. It then does a fine-grained inference to\nobtain the final answer. Experiment results show that our method achieves an\naccuracy of 28.4% on the linear Dolphin18K benchmark, which is 10% (54%\nrelative) higher than previous state-of-the-art systems while achieving an\naccuracy\nincrease of 12% (59% relative) on the TS6 benchmark subset."}, {"title": "Structural Embedding of Syntactic Trees for Machine Comprehension", "id": "1219", "venue": "ACL", "authors": "Rui Liu, Junjie Hu, Wei Wei, Zi Yang and Eric Nyberg", "abstract": "Deep neural networks for machine comprehension typically utilizes only word or\ncharacter embeddings without explicitly taking advantage of structured\nlinguistic information such as constituency trees and dependency trees. In this\npaper, we propose structural embedding of syntactic trees (SEST), an algorithm\nframework to utilize structured information and encode them into vector\nrepresentations that can boost the performance of algorithms for the machine\ncomprehension. We evaluate our approach using a state-of-the-art neural\nattention model on the SQuAD dataset. Experimental results demonstrate that our\nmodel can accurately identify the syntactic boundaries of the sentences and\nextract answers that are syntactically coherent over the baseline methods."}, {"title": "World Knowledge for Reading Comprehension: Rare Entity Prediction with Hierarchical LSTMs Using External Descriptions", "id": "856", "venue": "ACL", "authors": "Teng Long, Emmanuel Bengio, Ryan Lowe, Jackie Chi Kit Cheung and Doina Precup", "abstract": "Humans interpret texts with respect to some background information, or world\nknowledge, and we would like to develop automatic reading comprehension systems\nthat can do the same. In this paper, we introduce a task and several models to\ndrive progress towards this goal. In particular, we propose the task of rare\nentity prediction: given a web document with several entities removed, models\nare tasked with predicting the correct missing entities conditioned on the\ndocument context and the lexical resources. This task is challenging due to the\ndiversity of language styles and the extremely large number of rare entities.\nWe propose two recurrent neural network architectures which make use of\nexternal knowledge in the form of entity descriptions. Our experiments show\nthat our hierarchical LSTM model performs significantly better at the rare\nentity prediction task than those that do not make use of external resources."}, {"title": "Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension", "id": "1145", "venue": "ACL", "authors": "David Golub, Po-Sen Huang, Xiaodong He and Li Deng", "abstract": "We develop a technique for transfer learning in machine comprehension (MC)\nusing a novel two-stage synthesis network.  Given a high performing MC model in\none domain, our technique aims to answer questions about documents in another\ndomain, where we use no labeled data of question-answer pairs. Using the\nproposed synthesis network with a pretrained model on the SQuAD dataset, we\nachieve an F1 measure of 46.6% on the challenging NewsQA dataset, approaching\nperformance of in-domain models (F1 measure of 50.0%) and outperforming the\nout-of-domain baseline by 7.6%, without use of provided annotations."}, {"title": "Deep Neural Solver for Math Word Problems", "id": "359", "venue": "ACL", "authors": "Yan Wang, Xiaojiang Liu and Shuming Shi", "abstract": "This paper presents a deep neural solver to automatically solve math word\nproblems. In contrast to previous statistical learning approaches, we directly\ntranslate math word problems to equation templates using a recurrent neural\nnetwork (RNN) model, without sophisticated feature engineering. We further\ndesign a hybrid model that combines the RNN model and a similarity-based\nretrieval model to achieve additional performance improvement. Experiments\nconducted on a large dataset show that the RNN model and the hybrid model\nsignificantly outperform state-of-the-art statistical learning methods for math\nword problem solving."}, {"title": "Latent Space Embedding for Retrieval in Question-Answer Archives", "id": "72", "venue": "ACL", "authors": "Deepak P, Dinesh Garg and Shirish Shevade", "abstract": "Community-driven Question Answering (CQA) systems such as Yahoo! Answers have\nbecome valuable sources of reusable information. CQA retrieval enables usage of\nhistorical CQA archives to solve new questions posed by users. This task has\nreceived much recent attention, with methods building upon literature from\ntranslation models, topic models, and deep learning. In this paper, we devise a\nCQA retrieval technique, LASER-QA, that embeds question-answer pairs within a\nunified latent space preserving the local neighborhood structure of question\nand answer spaces. The idea is that such a space mirrors semantic similarity\namong questions as well as answers, thereby enabling high quality retrieval.\nThrough an empirical analysis on various real-world QA datasets, we illustrate\nthe improved effectiveness of LASER-QA over state-of-the-art methods."}, {"title": "Question Generation for Question Answering", "id": "294", "venue": "ACL", "authors": "Nan Duan, Duyu Tang, Peng Chen and Ming Zhou", "abstract": "This paper presents how to generate questions from given passages using neural\nnetworks, where large scale QA pairs are automatically crawled and processed\nfrom Community-QA website, and used as training data. The contribution of the\npaper is 2-fold: First, two types of question generation approaches are\nproposed, one is a retrieval-based method using convolution neural network\n(CNN), the other is a generation-based method using recurrent\nneural network (RNN); Second, we show how to leverage the generated questions\nto improve existing question answering systems. We evaluate our question\ngeneration method for the answer sentence selection task on three benchmark\ndatasets, including SQuAD, MS MARCO, and WikiQA. Experimental results show\nthat, by using generated questions as an extra signal, significant QA\nimprovement can be achieved."}, {"title": "Learning to Paraphrase for Question Answering", "id": "770", "venue": "ACL", "authors": "Li Dong, Jonathan Mallinson, Siva Reddy and Mirella Lapata", "abstract": "Question answering (QA) systems are sensitive to the many different ways\nnatural language expresses the same information need. In this paper we turn to\nparaphrases as a means of capturing this knowledge and present a general\nframework which learns felicitous paraphrases for various QA tasks. Our method\nis trained end-to-end using question-answer pairs as a supervision signal. A\nquestion and its paraphrases serve as input to a neural scoring model which\nassigns higher weights to linguistic expressions most likely to yield correct\nanswers. We evaluate our approach on QA over Freebase and answer sentence\nselection. Experimental results on three datasets show that our framework\nconsistently improves performance, achieving competitive results despite the\nuse of simple QA models."}, {"title": "Temporal Information Extraction for Question Answering Using Syntactic Dependencies in an LSTM-based Architecture", "id": "1419", "venue": "ACL", "authors": "Yuanliang Meng, Anna Rumshisky and Alexey Romanov", "abstract": "In this paper, we propose to use a set of simple, uniform in architecture\nLSTM-based models to recover different kinds of temporal relations from text.\nUsing the shortest dependency path between entities as input, the same\narchitecture is used to extract intra-sentence, cross-sentence, and document\ncreation time relations. A ``double-checking'' technique reverses entity pairs\nin classification, boosting the recall of positive cases and reducing\nmisclassifications between opposite classes. An efficient pruning algorithm\nresolves conflicts globally. Evaluated on QA-TempEval (SemEval2015 Task 5), our\nproposed technique outperforms state-of-the-art methods by a large margin. We\nalso conduct intrinsic evaluation and post state-of-the-art results on\nTimebank-Dense."}, {"title": "Ranking Kernels for Structures and Embeddings: A Hybrid Preference and Classification Model", "id": "797", "venue": "ACL", "authors": "Kateryna Tymoshenko, Daniele Bonadiman and Alessandro Moschitti", "abstract": "Recent work has shown that Tree Kernels (TKs) and Convolutional Neural Networks\n(CNNs) obtain the state of the art in answer sentence reranking. Additionally,\ntheir combination used in Support Vector Machines (SVMs) is promising as it can\nexploit both the syntactic patterns captured by TKs and the embeddings learned\nby CNNs. However, the embeddings are constructed according to a classification\nfunction, which is not directly exploitable in the preference ranking algorithm\nof SVMs. In this work, we propose a new hybrid approach combining preference\nranking applied to TKs and pointwise ranking applied to CNNs. We show that our\napproach produces better results on two well-known and rather different\ndatasets: WikiQA for answer sentence selection and SemEval cQA for comment\nselection in Community Question Answering."}, {"title": "Recovering Question Answering Errors via Query Revision", "id": "1405", "venue": "ACL", "authors": "Semih Yavuz, Izzeddin Gur, Yu Su and Xifeng Yan", "abstract": "The existing factoid QA systems often\nlack a post-inspection component that can\nhelp models recover from their own mistakes.\nIn this work, we propose to crosscheck\nthe corresponding KB relations behind\nthe predicted answers and identify\npotential inconsistencies. Instead of developing\na new model that accepts evidences\ncollected from these relations, we choose\nto plug them back to the original questions\ndirectly and check if the revised question\nmakes sense or not. A bidirectional LSTM\nis applied to encode revised questions. We\ndevelop a scoring mechanism over the revised\nquestion encodings to refine the predictions\nof a base QA system. This approach\ncan improve the F1 score of STAGG\n(Yih et al., 2015), one of the leading QA\nsystems, from 52.5% to 53.9% on WEBQUESTIONS\ndata."}], "chair_affiliation": "University of Maryland", "code": "3E", "room": "Odense", "end_time": "17:30", "id": "22", "start_time": "15:50", "talks": [], "chair": "Jay Pujara"}, {"title": "Poster Session. Multimodal NLP 1", "start_time_iso": "2017-09-09T15:50:00", "end_time_iso": "2017-09-09T17:30:00", "type": "poster", "posters": [{"title": "An empirical study on the effectiveness of images in Multimodal Neural Machine Translation", "id": "1026", "venue": "ACL", "authors": "Jean-Benoit Delbrouck and St\u00e9phane Dupont", "abstract": "In state-of-the-art Neural Machine Trans-\nlation (NMT), an attention mechanism is\nused during decoding to enhance the trans-\nlation. At every step, the decoder uses this\nmechanism to focus on different parts of\nthe source sentence to gather the most use-\nful information before outputting its tar-\nget word. Recently, the effectiveness of\nthe attention mechanism has also been ex-\nplored for multi-modal tasks, where it be-\ncomes possible to focus both on sentence\nparts and image regions that they describe.\nIn this paper, we compare several atten-\ntion mechanism on the multi-modal trans-\nlation task (English, image \u2192 German)\nand evaluate the ability of the model to\nmake use of images to improve translation.\nWe surpass state-of-the-art scores on the\nMulti30k data set, we nevertheless iden-\ntify and report different misbehavior of the\nmachine while translating."}, {"title": "Sound-Word2Vec: Learning Word Representations Grounded in Sounds", "id": "1037", "venue": "ACL", "authors": "Ashwin Vijayakumar, Ramakrishna Vedantam and Devi Parikh", "abstract": "To be able to interact better with humans, it is crucial for machines to\nunderstand sound \u2013 a primary modality of human perception. Previous works\nhave used sound to learn embeddings for improved generic semantic similarity\nassessment. In this work, we treat sound as a first-class citizen, studying\ndownstream 6textual tasks which require aural grounding. To this end, we\npropose sound-word2vec \u2013 a new embedding scheme that learns specialized word\nembeddings grounded in sounds. For example, we learn that two seemingly (se-\nmantically) unrelated concepts, like leaves and paper are similar due to the\nsimilar rustling sounds they make. Our embed- dings prove useful in textual\ntasks requiring aural reasoning like text-based sound retrieval and discovering\nFoley sound effects (used in movies). Moreover, our em- bedding space captures\ninteresting dependencies between words and onomatopoeia and outperforms prior\nwork on aurally- relevant word relatedness datasets such as AMEN and ASLex."}, {"title": "The Promise of Premise: Harnessing Question Premises in Visual Question Answering", "id": "7", "venue": "ACL", "authors": "Aroma Mahendru, Viraj Prabhu, Akrit Mohapatra, Dhruv Batra and Stefan Lee", "abstract": "In this paper, we make a simple observation that questions about images often\ncontain premises \u2013 objects and relationships implied by the question \u2013 and\nthat reasoning about premises can help Visual Question Answering (VQA) models\nrespond more intelligently to irrelevant or previously unseen questions.\n\nWhen presented with a question that is irrelevant to an image, state-of-the-art\nVQA models will still answer purely based on learned language biases, resulting\nin non-sensical or even misleading answers. We note that a visual question is\nirrelevant to an image if at least one of its premises is false (i.e. not\ndepicted in the image). We leverage this observation to construct a dataset for\nQuestion Relevance Prediction and Explanation (QRPE) by searching for false\npremises. We train novel question relevance detection models and show that\nmodels that reason about premises consistently outperform models that do not.\n\nWe also find that forcing standard VQA models to reason about premises during\ntraining can lead to improvements on tasks requiring compositional reasoning."}, {"title": "Guided Open Vocabulary Image Captioning with Constrained Beam Search", "id": "169", "venue": "ACL", "authors": "Peter Anderson, Basura Fernando, Mark Johnson and Stephen Gould", "abstract": "Existing image captioning models do not generalize well to out-of-domain images\ncontaining novel scenes or objects. This limitation severely hinders the use of\nthese models in real world applications dealing with images in the wild. We\naddress this problem using a flexible approach that enables existing deep\ncaptioning architectures to take advantage of image taggers at test time,\nwithout re-training. Our method uses constrained beam search to force the\ninclusion of selected tag words in the output, and fixed, pretrained word\nembeddings to facilitate vocabulary expansion to previously unseen tag words.\nUsing this approach we achieve state of the art results for out-of-domain\ncaptioning on MSCOCO (and improved results for in-domain captioning). Perhaps\nsurprisingly, our results significantly outperform approaches that incorporate\nthe same tag predictions into the learning algorithm. We also show that we can\nsignificantly improve the quality of generated ImageNet captions by leveraging\nground-truth labels."}, {"title": "Zero-Shot Activity Recognition with Verb Attribute Induction", "id": "1459", "venue": "ACL", "authors": "Rowan Zellers and Yejin Choi", "abstract": "In this paper, we investigate large-scale zero-shot activity recognition by\nmodeling the visual and linguistic attributes of action verbs. For example, the\nverb ``salute'' has several properties, such as being a light movement, a\nsocial act, and short in duration. We use these attributes as the internal\nmapping between visual and textual representations to reason about a previously\nunseen action. In contrast to much prior work that assumes access to gold\nstandard attributes for zero-shot classes and focuses primarily on object\nattributes, our model uniquely learns to infer action attributes from\ndictionary definitions and distributed word representations. Experimental\nresults confirm that action attributes inferred from language can provide a\npredictive signal for zero-shot prediction of previously unseen activities."}, {"title": "Deriving continous grounded meaning representations from referentially structured multimodal contexts", "id": "553", "venue": "ACL", "authors": "Sina Zarrie\u00df and David Schlangen", "abstract": "Corpora of referring expressions paired with their visual referents are a good\nsource for learning word meanings directly grounded in visual representations.\nHere, we explore additional ways of extracting from them word representations\nlinked to  multi-modal context: through expressions that refer to the same\nobject, and through expressions that refer to different objects in the same\nscene. We show that continuous meaning representations derived from these\ncontexts capture complementary aspects of similarity, , even if not\noutperforming textual embeddings trained on very large amounts of raw text when\ntested on standard similarity benchmarks. We propose a new task for evaluating\ngrounded meaning representations---detection of potentially co-referential\nphrases---and show that it requires precise denotational representations of\nattribute meanings, which our method provides."}, {"title": "Hierarchically-Attentive RNN for Album Summarization and Storytelling", "id": "1151", "venue": "ACL", "authors": "Licheng Yu, Mohit Bansal and Tamara Berg", "abstract": "We address the problem of end-to-end visual storytelling. \nGiven a photo album, our model first selects the most representative (summary)\nphotos, and then composes a natural language story for the album.\nFor this task, we make use of the Visual Storytelling dataset and a model\ncomposed of three hierarchically-attentive Recurrent Neural Nets (RNNs) to:\nencode the album photos, select representative (summary) photos, and  compose\nthe story. Automatic and human evaluations show our model achieves better\nperformance on\nselection, generation, and retrieval than baselines."}, {"title": "Video Highlight Prediction Using Audience Chat Reactions", "id": "1223", "venue": "ACL", "authors": "Cheng-Yang Fu, Joon Lee, Mohit Bansal and Alexander Berg", "abstract": "Sports channel video portals offer an exciting domain for research on\nmultimodal, multilingual analysis. We present methods addressing the problem of\nautomatic video highlight prediction based on joint visual features and textual\nanalysis of the real-world audience discourse with complex slang, in both\nEnglish and traditional Chinese. We present a novel dataset based on League of\nLegends championships recorded from North American and Taiwanese Twitch.tv\nchannels (will be released for further research), and demonstrate strong\nresults on these using multimodal, character-level CNN-RNN model architectures."}, {"title": "Reinforced Video Captioning with Entailment Rewards", "id": "1296", "venue": "ACL", "authors": "Ramakanth Pasunuru and Mohit Bansal", "abstract": "Sequence-to-sequence models have shown promising  improvements on the temporal\ntask of video captioning, but they optimize word-level cross-entropy loss\nduring training. First, using policy gradient and mixed-loss methods for\nreinforcement learning,  we directly optimize sentence-level task-based metrics\n(as rewards), achieving significant improvements over the baseline, based on\nboth automatic metrics and human evaluation on multiple datasets. Next, we\npropose a novel entailment-enhanced reward (CIDEnt) that corrects\nphrase-matching based metrics (such as CIDEr) to only allow for\nlogically-implied partial matches and avoid contradictions, achieving further\nsignificant improvements over the CIDEr-reward model. Overall, our\nCIDEnt-reward model achieves the new state-of-the-art on the MSR-VTT dataset."}, {"title": "Evaluating Hierarchies of Verb Argument Structure with Hierarchical Clustering", "id": "1446", "venue": "ACL", "authors": "Jesse Mu, Joshua K. Hartshorne and Timothy O'Donnell", "abstract": "Verbs can only be used with a few specific arrangements of their arguments\n(syntactic frames). Most theorists note that verbs can be organized into a\nhierarchy of verb classes based on the frames they admit. Here we show that\nsuch a hierarchy is objectively well-supported by the patterns of verbs and\nframes in English, since a systematic hierarchical clustering algorithm\nconverges on the same structure as the handcrafted taxonomy of VerbNet, a\nbroad-coverage verb lexicon. We also show that the hierarchies capture\nmeaningful psychological dimensions of generalization by predicting novel verb\ncoercions by human participants. We discuss limitations of a simple\nhierarchical representation and suggest similar approaches for identifying the\nrepresentations underpinning verb argument structure."}, {"title": "Incorporating Global Visual Features into Attention-based Neural Machine Translation.", "id": "163", "venue": "ACL", "authors": "Iacer Calixto, Qun Liu and Nick Campbell", "abstract": "We introduce multi-modal, attention-based neural machine translation (NMT)\nmodels which incorporate visual features into different parts of both the\nencoder and the decoder. Global image features are extracted using a\npre-trained convolutional neural network and are incorporated (i) as words in\nthe source sentence, (ii) to initialise the encoder hidden state, and (iii) as\nadditional data to initialise the decoder hidden state. In our experiments, we\nevaluate translations into English and German, how different strategies to\nincorporate global image features compare and which ones perform best. We also\nstudy the impact that adding synthetic multi-modal, multilingual data brings\nand find that the additional data have a positive impact on multi-modal NMT\nmodels. We report new state-of-the-art results and our best models also\nsignificantly improve on a comparable phrase-based Statistical MT (PBSMT) model\ntrained on the Multi30k data set according to all metrics evaluated. To the\nbest of our knowledge, it is the first time a purely neural model significantly\nimproves over a PBSMT model on all metrics evaluated on this data set."}, {"title": "Mapping Instructions and Visual Observations to Actions with Reinforcement Learning", "id": "138", "venue": "ACL", "authors": "Dipendra Misra, John Langford and Yoav Artzi", "abstract": "We propose to directly map raw visual observations and text input to actions\nfor instruction execution. While existing approaches assume access to\nstructured environment representations or use a pipeline of separately trained\nmodels, we learn a single model to jointly reason about linguistic and visual\ninput. We use reinforcement learning in a contextual bandit setting to train a\nneural network agent. To guide the agent's exploration, we use reward shaping\nwith different forms of supervision. Our approach does not require intermediate\nrepresentations, planning procedures, or training different models. We evaluate\nin a simulated environment, and show significant improvements over supervised\nlearning and common reinforcement learning variants."}, {"title": "An analysis of eye-movements during reading for the detection of mild cognitive impairment", "id": "301", "venue": "ACL", "authors": "Kathleen C. Fraser, Kristina Lundholm Fors, Dimitrios Kokkinakis and Arto Nordlund", "abstract": "We present a machine learning analysis of eye-tracking data for the detection\nof mild cognitive impairment, a decline in cognitive abilities that is\nassociated with an increased risk of developing dementia. We compare two\nexperimental configurations (reading aloud versus reading silently), as well as\ntwo methods of combining information from the two trials (concatenation and\nmerging). Additionally, we annotate the words being read with information about\ntheir frequency and syntactic category, and use these annotations to generate\nnew features. Ultimately, we are able to distinguish between participants with\nand without cognitive impairment with up to 86% accuracy."}, {"title": "Evaluating Low-Level Speech Features Against Human Perceptual Data", "id": "1534", "venue": "ACL", "authors": "Caitlin Richter, Naomi H Feldman, Harini Salgado and Aren Jansen", "abstract": "We introduce a method for measuring the correspondence between low-level speech\nfeatures and human perception, using a cognitive model of speech perception\nimplemented directly on speech recordings.  We evaluate two speaker\nnormalization techniques using this method and find that in both cases, speech\nfeatures that are normalized across speakers predict human data better than\nunnormalized speech features, consistent with previous research.  Results\nfurther reveal differences across normalization methods in how well each\npredicts human data.  This work provides a new framework for evaluating\nlow-level representations of speech on their match to human perception, and\nlays the groundwork for creating more ecologically valid models of speech\nperception."}], "chair_affiliation": "Cornell University", "code": "3F", "room": "Copenhagen", "end_time": "17:30", "id": "23", "start_time": "15:50", "talks": [], "chair": "Tianze Shi"}]], "weekday": "Saturday"}, {"day": "Sunday, September 10, 2017", "program": [[{"title": "Registration Day 2", "code": null, "id": "24", "start_time_iso": "2017-09-10T07:30:00", "end_time": "17:30", "end_time_iso": "2017-09-10T17:30:00", "type": "other", "room": null, "start_time": "07:30", "talks": [], "posters": []}], [{"title": "Morning Coffee", "start_time_iso": "2017-09-10T08:00:00", "end_time": "09:00", "end_time_iso": "2017-09-10T09:00:00", "id": "25", "start_time": "08:00", "type": "break"}], [{"title": "Plenary Session. Invited Talk by Sharon Goldwater", "code": null, "id": "26", "start_time_iso": "2017-09-10T09:00:00", "end_time": "10:00", "end_time_iso": "2017-09-10T10:00:00", "type": "invited_talk", "room": "Jutland", "start_time": "09:00", "talks": [], "posters": []}], [{"title": "Coffee Break", "start_time_iso": "2017-09-10T10:00:00", "end_time": "10:30", "end_time_iso": "2017-09-10T10:30:00", "id": "27", "start_time": "10:00", "type": "break"}], [{"title": "Reading and Retrieving", "start_time_iso": "2017-09-10T10:30:00", "end_time_iso": "2017-09-10T12:10:00", "type": "paper", "posters": [], "chair_affiliation": "Rensselaer Polytechnic Institute", "code": "4A", "room": "Jutland", "end_time": "12:10", "id": "28", "start_time": "10:30", "talks": [{"title": "A Structured Learning Approach to Temporal Relation Extraction", "venue": "ACL", "end_time": "10:55", "abstract": "Identifying temporal relations between events is an essential step towards\nnatural language understanding. However, the temporal relation between two\nevents in a story depends on, and is often dictated by, relations among other\nevents. Consequently, effectively identifying temporal relations between events\nis a challenging problem even for human annotators. This paper suggests that it\nis important to take these dependencies into account while learning to identify\nthese relations and proposes a structured learning approach to address this\nchallenge. As a byproduct, this provides a new perspective on handling missing\nrelations, a known issue that hurts existing methods. As we show, the proposed\napproach results in significant improvements on the two commonly used data sets\nfor this problem.", "id": "504", "start_time": "10:30", "authors": "Qiang Ning, Zhili Feng and Dan Roth"}, {"title": "Importance sampling for unbiased on-demand evaluation of knowledge base population", "venue": "ACL", "end_time": "11:20", "abstract": "Knowledge base population (KBP) systems take in a large document corpus and\nextract entities and their relations. Thus far, KBP evaluation has relied on\njudgements on the pooled predictions of existing systems.\nWe show that this evaluation is problematic: when a new system predicts a\npreviously unseen relation, it is penalized even if it is correct. This leads\nto significant bias against new systems, which counterproductively discourages\ninnovation in the field. Our first contribution is a new importance-sampling\nbased evaluation which corrects for this bias by annotating a new system's\npredictions on-demand via crowdsourcing. We show this eliminates bias and\nreduces variance using data from the 2015 TAC KBP task. Our second contribution\nis an implementation of our method made publicly available as an online KBP\nevaluation service. We pilot the service by testing diverse state-of-the-art\nsystems on the TAC KBP 2016 corpus and obtain accurate scores in a cost\neffective manner.", "id": "1376", "start_time": "10:55", "authors": "Arun Chaganty, Ashwin Paranjape, Percy Liang and Christopher D. Manning"}, {"title": "PACRR: A Position-Aware Neural IR Model for Relevance Matching", "venue": "ACL", "end_time": "11:45", "abstract": "In order to adopt deep learning for information retrieval, models are needed\nthat can capture all relevant information required to assess the relevance of a\ndocument to a given\nuser query. While previous works have successfully captured unigram term\nmatches, how to fully employ position-dependent information such as proximity\nand term dependencies has been insufficiently explored. In this work, we\npropose a novel neural IR model named PACRR\naiming at better modeling position-dependent interactions between a query and a\ndocument.\nExtensive experiments on six years' TREC Web Track data confirm that the\nproposed model yields better results under multiple benchmarks.", "id": "961", "start_time": "11:20", "authors": "Kai Hui, Andrew Yates, Klaus Berberich and Gerard de Melo"}, {"title": "Globally Normalized Reader", "venue": "ACL", "end_time": "12:10", "abstract": "Rapid progress has been made towards question answering (QA) systems that can\nextract answers from text. Existing neural approaches make use of expensive\nbi-directional attention mechanisms or score all possible answer spans,\nlimiting scalability. We propose instead to cast extractive QA as an iterative\nsearch problem: select the answer's sentence, start word, and end word. This\nrepresentation reduces the space of each search step and allows computation to\nbe conditionally allocated to promising search paths. We show that globally\nnormalizing the decision process and back-propagating through beam search makes\nthis representation viable and learning efficient. We empirically demonstrate\nthe benefits of this approach using our model, Globally Normalized Reader\n(GNR), which achieves the second highest single model performance on the\nStanford Question Answering Dataset (68.4 EM, 76.21 F1 dev) and is 24.7x faster\nthan bi-attention-flow. We also introduce a data-augmentation method to produce\nsemantically valid examples by aligning named entities to a knowledge base and\nswapping them with new entities of the same type. This method  improves the\nperformance of all models considered in this work and is of independent\ninterest for a variety of NLP tasks.", "id": "506", "start_time": "11:45", "authors": "Jonathan Raiman and John Miller"}], "chair": "Heng Ji"}, {"title": "Multimodal NLP 2", "start_time_iso": "2017-09-10T10:30:00", "end_time_iso": "2017-09-10T12:10:00", "type": "paper", "posters": [], "chair_affiliation": "Google", "code": "4B", "room": "Funen", "end_time": "12:10", "id": "29", "start_time": "10:30", "talks": [{"title": "Speech segmentation with a neural encoder model of working memory", "venue": "ACL", "end_time": "10:55", "abstract": "We present the first unsupervised LSTM speech segmenter as a cognitive model of\nthe acquisition of words from unsegmented input. Cognitive biases toward\nphonological and syntactic predictability in speech are rooted in the\nlimitations of human memory (Baddeley et al., 1998); compressed representations\nare easier to acquire and retain in memory. To model the biases introduced by\nthese memory limitations, our system uses an LSTM-based encoder-decoder with a\nsmall number of hidden units, then searches for a segmentation that minimizes\nautoencoding loss. Linguistically meaningful segments (e.g. words) should share\nregular patterns of features that facilitate decoder performance in comparison\nto random segmentations, and we show that our learner discovers these patterns\nwhen trained on either phoneme sequences or raw acoustics. To our knowledge,\nours is the first fully unsupervised system to be able to segment both symbolic\nand acoustic representations of speech.", "id": "445", "start_time": "10:30", "authors": "Micha Elsner and Cory Shain"}, {"title": "Speaking, Seeing, Understanding: Correlating semantic models with conceptual representation in the brain", "venue": "ACL", "end_time": "11:20", "abstract": "Research in computational semantics is increasingly guided by our understanding\nof human semantic processing. However, semantic models are typically studied in\nthe context of natural language processing system performance. In this paper,\nwe present a systematic evaluation and comparison of a range of widely-used,\nstate-of-the-art semantic models in their ability to predict patterns of\nconceptual representation in the human brain. Our results provide new insights\nboth for the design of computational semantic models and for further research\nin cognitive neuroscience.", "id": "959", "start_time": "10:55", "authors": "Luana Bulat, Stephen Clark and Ekaterina Shutova"}, {"title": "Multi-modal Summarization for Asynchronous Collection of Text, Image, Audio and Video", "venue": "ACL", "end_time": "11:45", "abstract": "The rapid increase of the multimedia data over the Internet necessitates\nmulti-modal summarization from collections of text, image, audio and video.  \nIn this work, we propose an extractive Multi-modal Summarization (MMS) method\nwhich can automatically generate a textual summary given a set of documents,\nimages, audios and videos related to a specific topic. The key idea is to\nbridge the semantic gaps between multi-modal contents. For audio information,\nwe design an approach to selectively use its transcription. For vision\ninformation, we learn joint representations of texts and images using a neural\nnetwork. Finally, all the multi-modal aspects are considered to generate the\ntextural summary by maximizing the salience, non-redundancy, readability and\ncoverage through budgeted optimization of submodular functions.  We further\nintroduce an MMS corpus in English and Chinese. The experimental results on\nthis dataset demonstrate that our\nmethod outperforms other competitive baseline methods.", "id": "75", "start_time": "11:20", "authors": "Haoran Li, Junnan Zhu, Cong Ma, Jiajun Zhang and Chengqing Zong"}, {"title": "Tensor Fusion Network for Multimodal Sentiment Analysis", "venue": "ACL", "end_time": "12:10", "abstract": "Multimodal sentiment analysis is an increasingly popular research area, which\nextends the conventional language-based definition of sentiment analysis to a\nmultimodal setup where other relevant modalities accompany language. In this\npaper, we pose the problem of multimodal sentiment analysis as modeling\nintra-modality and inter-modality dynamics. We introduce a novel model, termed\nTensor Fusion Networks, which learns both such dynamics end-to-end. The\nproposed approach is tailored for the volatile nature of spoken language in\nonline videos as well as accompanying gestures and voice. In the experiments,\nour model outperforms state-of-the-art approaches for both multimodal and\nunimodal sentiment analysis.", "id": "1400", "start_time": "11:45", "authors": "Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria and Louis-Philippe Morency"}], "chair": "Brian Roark"}, {"title": "Human Centered NLP and Linguistic Theory", "start_time_iso": "2017-09-10T10:30:00", "end_time_iso": "2017-09-10T12:10:00", "type": "paper", "posters": [], "chair_affiliation": "Ohio State University", "code": "4C", "room": "Zealand", "end_time": "12:10", "id": "30", "start_time": "10:30", "talks": [{"title": "ConStance: Modeling Annotation Contexts to Improve Stance Classification", "venue": "ACL", "end_time": "10:55", "abstract": "Manual annotations are a prerequisite for many applications of machine\nlearning.\nHowever, weaknesses in the annotation process itself are easy to overlook. In\nparticular, scholars often choose what information to give to annotators\nwithout examining these decisions empirically.\nFor subjective tasks such as sentiment analysis, sarcasm, and stance detection,\nsuch choices can impact results. \nHere, for the task of political stance detection on Twitter, we show that\nproviding\ntoo little context can result in noisy and uncertain annotations, \nwhereas providing too strong a context may cause it to outweigh other signals.\nTo characterize and reduce these biases, we develop ConStance, a general model\nfor reasoning about annotations across information \nconditions. \nGiven conflicting labels produced by multiple annotators seeing the same\ninstances with different contexts, ConStance simultaneously \nestimates gold standard labels and also learns a classifier for new instances.\nWe show that the classifier learned by ConStance outperforms \na variety of baselines at predicting political stance, while the model's\ninterpretable parameters shed light on the effects of each context.", "id": "412", "start_time": "10:30", "authors": "Kenneth Joseph, Lisa Friedland, William Hobbs, David Lazer and Oren Tsur"}, {"title": "Deeper Attention to Abusive User Content Moderation", "venue": "ACL", "end_time": "11:20", "abstract": "Experimenting with a new dataset of 1.6M\nuser comments from a news portal and an\nexisting dataset of 115K Wikipedia talk\npage comments, we show that an RNN operating\non word embeddings outpeforms\nthe previous state of the art in moderation,\nwhich used logistic regression or an MLP\nclassifier with character or word n-grams.\nWe also compare against a CNN operating\non word embeddings, and a word-list\nbaseline. A novel, deep, classificationspecific\nattention mechanism improves the\nperformance of the RNN further, and can\nalso highlight suspicious words for free,\nwithout including highlighted words in the\ntraining data. We consider both fully automatic\nand semi-automatic moderation.", "id": "909", "start_time": "10:55", "authors": "John Pavlopoulos, Prodromos Malakasiotis and Ion Androutsopoulos"}, {"title": "Outta Control: Laws of Semantic Change and Inherent Biases in Word Representation Models", "venue": "ACL", "end_time": "11:45", "abstract": "This article evaluates three proposed laws of semantic change. Our claim is\nthat in order to validate a putative law of semantic change, the effect should\nbe observed in the genuine condition but absent or reduced in a suitably\nmatched control condition, in which no change can possibly have taken place.\nOur analysis shows that the effects reported in recent literature must be\nsubstantially revised: (i) the proposed negative correlation between meaning\nchange and word frequency is shown to be largely an artefact of the models of\nword representation used; (ii) the proposed negative correlation between\nmeaning change and prototypicality is shown to be much weaker than what has\nbeen claimed in prior art; and (iii) the proposed positive correlation between\nmeaning change and polysemy is largely an artefact of word frequency. These\nempirical observations are corroborated by analytical proofs that show that\ncount representations introduce an inherent dependence on word frequency, and\nthus word frequency cannot be evaluated as an independent factor with these\nrepresentations.", "id": "659", "start_time": "11:20", "authors": "Haim Dubossarsky, Daphna Weinshall and Eitan Grossman"}, {"title": "Human Centered NLP with User-Factor Adaptation", "venue": "ACL", "end_time": "12:10", "abstract": "We pose the general task of user-factor adaptation -- adapting supervised\nlearning models to real-valued user factors inferred from a background of their\nlanguage, reflecting the idea that a piece of text should be understood within\nthe context of the user that wrote it. We introduce a continuous adaptation\ntechnique, suited for real-valued user factors that are common in social\nscience and bringing us closer to personalized NLP, adapting to each user\nuniquely. We apply this technique with known user factors including age,\ngender, and personality traits, as well as latent factors, evaluating over five\ntasks: POS tagging, PP-attachment, sentiment analysis, sarcasm detection, and\nstance detection. Adaptation provides statistically significant benefits for 3\nof the 5 tasks: up to +1.2 points for PP-attachment, +3.4 points for sarcasm,\nand +3.0 points for stance.", "id": "391", "start_time": "11:45", "authors": "Veronica Lynn, Youngseo Son, Vivek Kulkarni, Niranjan Balasubramanian and H. Andrew Schwartz"}], "chair": "Alan Ritter"}, {"title": "Poster Session. Semantics 2", "start_time_iso": "2017-09-10T10:30:00", "end_time_iso": "2017-09-10T12:10:00", "type": "poster", "posters": [{"title": "Neural Sequence Learning Models for Word Sense Disambiguation", "id": "566", "venue": "ACL", "authors": "Alessandro Raganato, Claudio Delli Bovi and Roberto Navigli", "abstract": "Word Sense Disambiguation models exist in many flavors. Even though supervised\nones tend to perform best in terms of accuracy, they often lose ground to more\nflexible knowledge-based solutions, which do not require training by a word\nexpert for every disambiguation target. To bridge this gap we adopt a different\nperspective and rely on sequence learning to frame the disambiguation problem:\nwe propose and study in depth a series of end-to-end neural architectures\ndirectly tailored to the task, from bidirectional Long Short-Term Memory to\nencoder-decoder models. Our extensive evaluation over standard benchmarks and\nin multiple languages shows that sequence learning enables more versatile\nall-words models that consistently lead to state-of-the-art results, even\nagainst word experts with engineered features."}, {"title": "Learning Word Relatedness over Time", "id": "962", "venue": "ACL", "authors": "Guy D. Rosin, Eytan Adar and Kira Radinsky", "abstract": "Search systems are often focused on providing relevant results for the \"now\",\nassuming both corpora and user needs that focus on the present. However, many\ncorpora today reflect significant longitudinal collections ranging from 20\nyears of the Web to hundreds of years of digitized newspapers and books.\nUnderstanding the temporal intent of the user and retrieving the most relevant\nhistorical content has become a significant challenge. Common search features,\nsuch as query expansion, leverage the relationship between terms but cannot\nfunction well across all times when relationships vary temporally. In this\nwork, we introduce a temporal relationship model that is extracted from\nlongitudinal data collections. The model supports the task of identifying,\ngiven two words, when they relate to each other. We present an algorithmic\nframework for this task and show its application for the task of query\nexpansion, achieving high gain."}, {"title": "Inter-Weighted Alignment Network for Sentence Pair Modeling", "id": "117", "venue": "ACL", "authors": "Gehui Shen, Yunlun Yang and Zhi-Hong Deng", "abstract": "Sentence pair modeling is a crucial problem in the field of natural language\nprocessing. \nIn this paper, we propose a model to measure the similarity of a sentence pair\nfocusing on the interaction information. We utilize the word level similarity\nmatrix to discover fine-grained alignment of two sentences. It should be\nemphasized that each word in a sentence has a different importance from the\nperspective of semantic composition, so we exploit two novel and efficient\nstrategies to explicitly calculate a weight for each word. Although the\nproposed model only use a sequential LSTM for sentence modeling without any\nexternal resource such as syntactic parser tree and additional lexicon\nfeatures, experimental results show that our model achieves state-of-the-art\nperformance on three datasets of two tasks."}, {"title": "A Short Survey on Taxonomy Learning from Text Corpora: Issues, Resources and Recent Advances", "id": "120", "venue": "ACL", "authors": "Chengyu Wang, Xiaofeng He and Aoying Zhou", "abstract": "A taxonomy is a semantic hierarchy, consisting of concepts linked by is-a\nrelations. While a large number of taxonomies have been constructed from\nhuman-compiled resources (e.g., Wikipedia), learning taxonomies from text\ncorpora has received a growing interest and is essential for long-tailed and\ndomain-specific knowledge acquisition. In this paper, we overview recent\nadvances on taxonomy construction from free texts, reorganizing relevant\nsubtasks into a complete framework. We also overview resources for evaluation\nand discuss challenges for future research."}, {"title": "Idiom-Aware Compositional Distributed Semantics", "id": "811", "venue": "ACL", "authors": "Pengfei Liu, Kaiyu Qian, Xipeng Qiu and Xuanjing Huang", "abstract": "Idioms are peculiar linguistic constructions that impose great challenges for\nrepresenting the semantics of language, especially in current prevailing\nend-to-end neural models, which assume that the semantics of a phrase or\nsentence can be literally composed from its constitutive words.\nIn this paper, we propose an idiom-aware distributed semantic model to build\nrepresentation of sentences on the basis of understanding their contained\nidioms. Our models are grounded in the literal-first psycholinguistic\nhypothesis, which can adaptively learn semantic compositionality of a phrase\nliterally or idiomatically. To better evaluate our models, we also construct an\nidiom-enriched sentiment classification dataset with considerable scale and\nabundant peculiarities of idioms. The qualitative and quantitative experimental\nanalyses demonstrate the efficacy of our models."}, {"title": "Macro Grammars and Holistic Triggering for Efficient Semantic Parsing", "id": "982", "venue": "ACL", "authors": "Yuchen Zhang, Panupong Pasupat and Percy Liang", "abstract": "To learn a semantic parser from denotations, a learning algorithm must search\nover a combinatorially large space of logical forms for ones consistent with\nthe annotated denotations. We propose a new online learning algorithm that\nsearches faster as training progresses. The two key ideas are using macro\ngrammars to cache the abstract patterns of useful logical forms found thus far,\nand holistic triggering to efficiently retrieve the most relevant patterns\nbased on sentence similarity. On the WikiTableQuestions dataset, we first\nexpand the search space of an existing model to improve the state-of-the-art\naccuracy from 38.7% to 42.7%, and then use macro grammars and holistic\ntriggering to achieve an 11x speedup and an accuracy of 43.7%."}, {"title": "A Continuously Growing Dataset of Sentential Paraphrases", "id": "1042", "venue": "ACL", "authors": "Wuwei Lan, Siyu Qiu, Hua He and Wei Xu", "abstract": "A major challenge in paraphrase research is the lack of parallel corpora. In\nthis paper, we present a new method to collect large-scale sentential\nparaphrases from Twitter by linking tweets through shared URLs. The main\nadvantage of our method is its simplicity, as it gets rid of the classifier or\nhuman in the loop needed to select data before annotation and subsequent\napplication of paraphrase identification algorithms in the previous work.\nWe present the largest human-labeled paraphrase corpus to date of 51,524\nsentence pairs and the first cross-domain benchmarking for automatic paraphrase\nidentification. In addition, we show that more than 30,000 new sentential\nparaphrases can be easily and continuously captured every month at ~70\\%\nprecision, and demonstrate their utility for downstream NLP tasks through\nphrasal paraphrase extraction. We make our code and data freely available."}, {"title": "Cross-domain Semantic Parsing via Paraphrasing", "id": "1048", "venue": "ACL", "authors": "Yu Su and Xifeng Yan", "abstract": "Existing studies on semantic parsing mainly focus on the in-domain setting. We\nformulate cross-domain semantic parsing as a domain adaptation problem: train a\nsemantic parser on some source domains and then adapt it to the target domain.\nDue to the diversity of logical forms in different domains, this problem\npresents unique and intriguing challenges. By converting logical forms into\ncanonical utterances in natural language, we reduce semantic parsing to\nparaphrasing, and develop an attentive sequence-to-sequence paraphrase model\nthat is general and flexible to adapt to different domains. We discover two\nproblems, small micro variance and large macro variance, of pre-trained word\nembeddings that hinder their direct use in neural networks, and propose\nstandardization techniques as a remedy. On the popular Overnight dataset, which\ncontains eight domains, we show that both cross-domain training and\nstandardized pre-trained word embeddings can bring significant improvement."}, {"title": "A Joint Sequential and Relational Model for Frame-Semantic Parsing", "id": "1087", "venue": "ACL", "authors": "Bishan Yang and Tom Mitchell", "abstract": "We introduce a new method for frame-semantic parsing that significantly\nimproves the prior state of the art. Our model leverages the advantages of a\ndeep bidirectional LSTM network which predicts semantic role labels word by\nword and a relational network which predicts semantic roles for individual text\nexpressions in relation to a predicate. The two networks are integrated into a\nsingle model via knowledge distillation, and a unified graphical model is\nemployed to jointly decode frames and semantic roles during inference.\nExperiments on the standard FrameNet data show that our model significantly\noutperforms existing neural and non-neural approaches, achieving a 5.7 F1 gain\nover the current state of the art, for full frame structure extraction."}, {"title": "Getting the Most out of AMR Parsing", "id": "1407", "venue": "ACL", "authors": "Chuan Wang and Nianwen Xue", "abstract": "This paper proposes to tackle the AMR parsing bottleneck by improving two\ncomponents of an AMR parser: concept identification and alignment. We first\nbuild a Bidirectional LSTM based concept identifier that is able to incorporate\nricher contextual information to learn sparse AMR concept labels. We then\nextend an HMM-based word-to-concept alignment model with graph distance\ndistortion and a rescoring method during decoding to incorporate the structural\ninformation in the AMR graph. We show integrating the two components into an\nexisting AMR parser results in consistently better performance over the state\nof the art on various datasets."}, {"title": "AMR Parsing using Stack-LSTMs", "id": "820", "venue": "ACL", "authors": "Miguel Ballesteros and Yaser Al-Onaizan", "abstract": "We present a transition-based AMR parser that directly generates AMR parses\nfrom plain text. \nWe use Stack-LSTMs to represent our parser state and make decisions greedily.\nIn our experiments, we show that our parser achieves very competitive scores on\nEnglish using only AMR training data. Adding additional information, such as\nPOS tags and dependency trees, improves the results further."}, {"title": "An End-to-End Deep Framework for Answer Triggering with a Novel Group-Level Objective", "id": "1134", "venue": "ACL", "authors": "Jie Zhao, Yu Su, Ziyu Guan and Huan Sun", "abstract": "Given a question and a set of answer candidates, answer triggering determines\nwhether the candidate set contains any correct answers. If yes, it then outputs\na correct one. In contrast to existing pipeline methods which first consider\nindividual candidate answers separately and then make a prediction based on a\nthreshold, we propose an end-to-end deep neural network framework, which is\ntrained by a novel group-level objective function that directly optimizes the\nanswer triggering performance. Our objective function penalizes three potential\ntypes of error and allows training the framework in an end-to-end manner.\nExperimental results on the WikiQA benchmark show that our framework\noutperforms the state of the arts by a 6.6% absolute gain under F1 measure."}, {"title": "Predicting Word Association Strengths", "id": "1443", "venue": "ACL", "authors": "Andrew Cattle and Xiaojuan Ma", "abstract": "This paper looks at the task of predicting word association strengths across\nthree datasets; WordNet Evocation (Boyd-Graber et al., 2006), University of\nSouthern Florida Free Association norms (Nelson et al., 2004), and Edinburgh\nAssociative Thesaurus (Kiss et al., 1973). We achieve results of r=0.357 and\np=0.379, r=0.344 and p=0.300, and r=0.292 and p=0.363, respectively. We find\nWord2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) cosine\nsimilarities, as well as vector offsets, to be the highest performing features.\nFurthermore, we examine the usefulness of Gaussian embeddings (Vilnis and\nMcCallum, 2014) for predicting word association strength, the first work to do\nso."}], "chair_affiliation": "University of Cambridge", "code": "4D", "room": "Aarhus", "end_time": "12:10", "id": "31", "start_time": "10:30", "talks": [], "chair": "Ivan Vuli\u0107"}, {"title": "Poster Session. Discourse", "start_time_iso": "2017-09-10T10:30:00", "end_time_iso": "2017-09-10T12:10:00", "type": "poster", "posters": [{"title": "Learning Contextually Informed Representations for Linear-Time Discourse Parsing", "id": "204", "venue": "ACL", "authors": "Yang Liu and Mirella Lapata", "abstract": "Recent advances in RST discourse parsing have focused on two modeling\nparadigms: (a) high order parsers which jointly predict the tree structure of\nthe discourse and the relations it encodes; or                                (b)\nlinear-time\nparsers\nwhich\nare efficient but mostly based on local features.  In this work, we propose a\nlinear-time parser with a novel way of representing discourse constituents\nbased on neural networks which takes into account global contextual information\nand is able to capture long-distance dependencies. Experimental results show\nthat our parser obtains state-of-the art performance on benchmark datasets,\nwhile being efficient (with time complexity linear in the number of sentences\nin the document) and requiring minimal feature engineering."}, {"title": "Multi-task Attention-based Neural Networks for Implicit Discourse Relationship Representation and Identification", "id": "732", "venue": "ACL", "authors": "Man Lan, Jianxiang Wang, Yuanbin Wu, Zheng-Yu Niu and Haifeng Wang", "abstract": "We present a novel multi-task attention based neural network model to address\nimplicit discourse relationship representation and identification through two\ntypes of representation learning, an attention based neural network for\nlearning discourse relationship representation with two arguments and a\nmulti-task framework for learning knowledge from annotated and unannotated\ncorpora. The extensive experiments have been performed on two benchmark corpora\n(i.e., PDTB and CoNLL-2016 datasets). Experimental results show that our\nproposed model outperforms the state-of-the-art systems on benchmark corpora."}, {"title": "Chinese Zero Pronoun Resolution with Deep Memory Network", "id": "751", "venue": "ACL", "authors": "Qingyu Yin, Yu Zhang, Weinan Zhang and Ting Liu", "abstract": "Existing approaches for Chinese zero pronoun resolution typically utilize only\nsyntactical and lexical features while ignoring semantic information. The\nfundamental reason is that zero pronouns have no descriptive information, which\nbrings difficulty in explicitly capturing their semantic similarities with\nantecedents. Meanwhile, representing zero pronouns is challenging since they\nare merely gaps that convey no actual content. In this paper, we address this\nissue by building a deep memory network that is capable of encoding zero\npronouns into vector representations with information obtained from their\ncontexts and potential antecedents. Consequently, our resolver takes advantage\nof semantic information by using these continuous distributed representations.\nExperiments on the OntoNotes 5.0 dataset show that the proposed memory network\ncould substantially outperform the state-of-the-art systems in various\nexperimental settings."}, {"title": "How much progress have we made on RST discourse parsing? A replication study of recent results on the RST-DT", "id": "705", "venue": "ACL", "authors": "Mathieu Morey, Philippe Muller and Nicholas Asher", "abstract": "This article evaluates purported progress over the past years in RST discourse\nparsing.\nSeveral studies report a relative error reduction of 24 to 51\\% on all metrics\nthat authors attribute to the introduction of distributed representations of\ndiscourse units.\nWe replicate the standard evaluation of 9 parsers, 5 of which use distributed\nrepresentations, from 8 studies published between 2013 and 2017, using their\npredictions on the test set of the RST-DT.\nOur main finding is that most recently reported increases in RST discourse\nparser performance are an artefact of differences in implementations of the\nevaluation procedure.\nWe evaluate all these parsers with the standard Parseval procedure to provide a\nmore accurate picture of the actual RST discourse parsers performance in\nstandard evaluation settings.\nUnder this more stringent procedure, the gains attributable to distributed\nrepresentations represent at most a 16\\% relative error reduction on\nfully-labelled structures."}, {"title": "What is it? Disambiguating the different readings of the pronoun \u2018it\u2019", "id": "880", "venue": "ACL", "authors": "Sharid Lo\u00e1iciga, Liane Guillou and Christian Hardmeier", "abstract": "In this paper, we address the problem of predicting one of three functions for\nthe English pronoun `it': anaphoric, event reference or pleonastic. This\ndisambiguation is valuable in the context of machine translation and\ncoreference resolution. We present experiments using a MAXENT classifier\ntrained on gold-standard data and self-training experiments of an RNN trained\non silver-standard data, annotated using the MAXENT classifier. Lastly, we\nreport on an analysis of the strengths of these two models."}, {"title": "Revisiting Selectional Preferences for Coreference Resolution", "id": "1019", "venue": "ACL", "authors": "Benjamin Heinzerling, Nafise Sadat Moosavi and Michael Strube", "abstract": "Selectional preferences have long been claimed to be essential for coreference\nresolution. However, they are modeled only implicitly by current coreference\nresolvers. We propose a dependency-based embedding model of selectional\npreferences which allows fine-grained compatibility judgments with high\ncoverage. Incorporating our model improves performance, matching\nstate-of-the-art results of a more complex system. However, it comes with a\ncost that makes it debatable how worthwhile are such improvements."}, {"title": "Learning to Rank Semantic Coherence for Topic Segmentation", "id": "497", "venue": "ACL", "authors": "Liang Wang, Sujian Li, Yajuan Lv and Houfeng WANG", "abstract": "Topic segmentation plays an important role for discourse parsing and\ninformation retrieval. Due to the absence of training data, previous work\nmainly adopts unsupervised methods to rank semantic coherence between\nparagraphs for topic segmentation. In this paper, we present an intuitive and\nsimple idea to automatically create a \"quasi\" training dataset, which includes\na large amount of text pairs from the same or different documents with\ndifferent semantic coherence. With the training corpus, we design a symmetric\nCNN neural network to model text pairs and rank the semantic coherence within\nthe learning to rank framework. Experiments show that our algorithm is able to\nachieve competitive performance over strong baselines on several real-world\ndatasets."}, {"title": "GRASP: Rich Patterns for Argumentation Mining", "id": "76", "venue": "ACL", "authors": "Eyal Shnarch, Ran Levy, Vikas Raykar and Noam Slonim", "abstract": "GRASP (GReedy Augmented Sequential Patterns) is an algorithm for automatically\nextracting patterns that characterize subtle linguistic phenomena. To that end,\nGRASP augments each term of input text with multiple layers of linguistic\ninformation. These different facets of the text terms are systematically\ncombined to reveal rich patterns. We report highly promising experimental\nresults in several challenging text analysis tasks within the field of\nArgumentation Mining. We believe that GRASP is general enough to be useful for\nother domains too.\n\nFor example, each of the following sentences includes a claim for a [topic]:\n1. Opponents often argue that the open primary is unconstitutional. [Open\nPrimaries]\n2. Prof. Smith suggested that affirmative action devalues the accomplishments\nof the chosen. [Affirmative Action]\n3. The majority stated that the First Amendment does not guarantee the right to\noffend others. [Freedom of Speech]\n\nThese sentences share almost no words in common, however, they are similar at a\nmore abstract level. A human observer may notice the following underlying\ncommon structure, or pattern: [someone][argue/suggest/state][that][topic\nterm][sentiment term].\n\nGRASP aims to automatically capture such underlying structures of the given\ndata. For the above examples it finds the pattern\n[noun][express][that][noun,topic][sentiment], where [express] stands for all\nits (in)direct hyponyms, and [noun,topic] means a noun which is also related to\nthe topic."}, {"title": "Patterns of Argumentation Strategies across Topics", "id": "425", "venue": "ACL", "authors": "Khalid Al Khatib, Henning Wachsmuth, Matthias Hagen and Benno Stein", "abstract": "This paper presents an analysis of argumentation strategies in news editorials\nwithin and across topics. Given nearly 29,000 argumentative editorials from the\nNew York Times, we develop two machine learning models, one for determining an\neditorial's topic, and one for identifying evidence types in the editorial.\nBased on the distribution and structure of the identified types, we analyze the\nusage patterns of argumentation strategies among 12 different topics. We detect\nseveral common patterns that provide insights into the manifestation of\nargumentation strategies. Also, our experiments reveal clear correlations\nbetween the topics and the detected patterns."}, {"title": "Using Argument-based Features to Predict and Analyse Review Helpfulness", "id": "286", "venue": "ACL", "authors": "Haijing Liu, Yang Gao, Pin Lv, Mengxue Li, Shiqiang Geng, Minglan Li and Hao Wang", "abstract": "We study the helpful product reviews identification problem in this paper. We\nobserve that the evidence-conclusion discourse relations, also known as\narguments, often appear in product reviews, and we hypothesise that some\nargument-based features, e.g. the percentage of argumentative sentences, the\nevidences-conclusions ratios, are good indicators of helpful reviews. To\nvalidate this hypothesis, we manually annotate arguments in 110 hotel reviews,\nand investigate the effectiveness of several combinations of argument-based\nfeatures. Experiments suggest that, when being used together with the\nargument-based features, the state-of-the-art baseline features can enjoy a\nperformance boost (in terms of F1) of 11.01\\% in average."}, {"title": "Here's My Point: Joint Pointer Architecture for Argument Mining", "id": "1043", "venue": "ACL", "authors": "Peter Potash, Alexey Romanov and Anna Rumshisky", "abstract": "In order to determine argument structure in text, one must understand how\nindividual components of the overall argument are linked. This work presents\nthe first neural network-based approach to link extraction in argument mining.\nSpecifically, we propose a novel architecture that applies Pointer Network\nsequence-to-sequence attention modeling to structural prediction in discourse\nparsing tasks. We then develop a joint model that extends this architecture to\nsimultaneously address the link extraction task and the classification of\nargument components. The proposed joint model achieves state-of-the-art results\non two separate evaluation corpora, showing far superior performance than the\npreviously proposed corpus-specific and heavily feature-engineered models.\nFurthermore, our results demonstrate that jointly optimizing for both tasks is\ncrucial for high performance."}, {"title": "Identifying attack and support argumentative relations using deep learning", "id": "201", "venue": "ACL", "authors": "Oana Cocarascu and Francesca Toni", "abstract": "We propose a deep learning architecture to capture argumentative relations of\nattack and support from one piece of text to another, of the kind that\nnaturally occur in a debate. The architecture uses two (unidirectional or\nbidirectional) Long Short-Term Memory networks and (trained or non-trained)\nword embeddings, and allows to considerably improve upon existing techniques\nthat use syntactic features and supervised classifiers for the same form of\n(relation-based) argument mining."}], "chair_affiliation": "Harvard University", "code": "4E", "room": "Odense", "end_time": "12:10", "id": "32", "start_time": "10:30", "talks": [], "chair": "Sam Wiseman"}, {"title": "Poster Session. Machine Translation and Multilingual NLP 1", "start_time_iso": "2017-09-10T10:30:00", "end_time_iso": "2017-09-10T12:10:00", "type": "poster", "posters": [{"title": "Neural Lattice-to-Sequence Models for Uncertain Inputs", "id": "223", "venue": "ACL", "authors": "Matthias Sperber, Graham Neubig, Jan Niehues and Alex Waibel", "abstract": "The input to a neural sequence-to-sequence model is often determined by an\nup-stream system, e.g. a word segmenter, part of speech tagger, or speech\nrecognizer. These up-stream models are potentially error-prone. Representing\ninputs through word lattices allows making this uncertainty explicit by\ncapturing alternative sequences and their posterior probabilities in a compact\nform.\nIn this work, we extend the TreeLSTM (Tai et al., 2015) into a LatticeLSTM that\nis able to consume word lattices, and can be used as encoder in an attentional\nencoder-decoder model. We integrate lattice posterior scores into this\narchitecture by extending the TreeLSTM's child-sum and forget gates and\nintroducing a bias term into the attention mechanism. We experiment with speech\ntranslation lattices and report consistent improvements over baselines that\ntranslate either the 1-best hypothesis or the lattice without posterior scores."}, {"title": "Memory-augmented Neural Machine Translation", "id": "277", "venue": "ACL", "authors": "Yang Feng, Shiyue Zhang, Andi Zhang, Dong Wang and Andrew Abel", "abstract": "Neural machine translation (NMT) has achieved notable success in recent times,\nhowever it is also widely recognized that this approach has limitations with\nhandling infrequent words and word pairs. This paper presents a novel\nmemory-augmented NMT (M-NMT) architecture, which stores knowledge about how\nwords (usually infrequently encountered ones) should be translated in a memory\nand then utilizes them to assist the neural model. We use this memory mechanism\nto combine the knowledge learned from a conventional statistical machine\ntranslation system and the rules learned by an NMT system, and also propose a\nsolution for out-of-vocabulary (OOV) words based on this framework. Our\nexperiments on two Chinese-English translation tasks demonstrated that the\nM-NMT architecture outperformed the NMT baseline by $9.0$ and $2.7$ BLEU points\non the two tasks, respectively. Additionally, we found this architecture\nresulted in a much more effective OOV treatment compared to competitive\nmethods."}, {"title": "Dynamic Data Selection for Neural Machine Translation", "id": "372", "venue": "ACL", "authors": "Marlies van der Wees, Arianna Bisazza and Christof Monz", "abstract": "Intelligent selection of training data has proven a successful technique to\nsimultaneously increase training efficiency and translation performance for\nphrase-based machine translation (PBMT). With the recent increase in popularity\nof neural machine translation (NMT), we explore in this paper to what extent\nand how NMT can also benefit from data selection. While state-of-the-art data\nselection (Axelrod et al., 2011) consistently performs well for PBMT, we show\nthat gains are substantially lower for NMT. Next, we introduce 'dynamic data\nselection' for NMT, a method in which we vary the selected subset of training\ndata between different training epochs. Our experiments show that the best\nresults are achieved when applying a technique we call 'gradual fine-tuning',\nwith improvements up to +2.6 BLEU over the original data selection approach and\nup to +3.1 BLEU over a general baseline."}, {"title": "Neural Machine Translation Leveraging Phrase-based Models in a Hybrid Search", "id": "398", "venue": "ACL", "authors": "Leonard Dahlmann, Evgeny Matusov, Pavel Petrushkov and Shahram Khadivi", "abstract": "In this paper, we introduce a hybrid search for attention-based neural machine\ntranslation (NMT). A target phrase learned with statistical MT models extends a\nhypothesis in the NMT beam search when the attention of the NMT model focuses\non the source words translated by this phrase. Phrases added in this way are\nscored with the NMT model, but also with SMT features including phrase-level\ntranslation probabilities and a target language model. Experimental results on\nGerman-to-English news domain and English-to-Russian e-commerce domain \ntranslation tasks show that using phrase-based models in NMT search improves MT\nquality by up to 2.3\\% BLEU absolute as compared to a strong NMT baseline."}, {"title": "Translating Phrases in Neural Machine Translation", "id": "699", "venue": "ACL", "authors": "Xing Wang, Zhaopeng Tu, Deyi Xiong and Min Zhang", "abstract": "Phrases play an important role in natural language understanding and machine\ntranslation (Sag et al., 2002; Villavicencio et al., 2005). However, it is\ndifficult to integrate them into current neural machine translation (NMT) which\nreads and generates sentences word by word. In this work, we propose a method\nto translate phrases in NMT by integrating a phrase memory storing target\nphrases from a phrase-based statistical machine translation (SMT) system into\nthe encoder-decoder architecture of NMT. At each decoding step, the phrase\nmemory is first re-written by the SMT model, which dynamically generates\nrelevant target phrases with contextual information provided by the NMT model.\nThen the proposed model reads the phrase memory to make probability estimations\nfor all phrases in the phrase memory. If phrase generation is carried on, the\nNMT decoder selects an appropriate phrase from the memory to perform phrase\ntranslation and updates its decoding state by consuming the words in the\nselected phrase. Otherwise, the NMT decoder generates a word from the\nvocabulary as the general NMT decoder does. Experiment results on the Chinese\nto\nEnglish translation show that the proposed model achieves significant\nimprovements over the baseline on various test sets."}, {"title": "Towards Bidirectional Hierarchical Representations for Attention-based Neural Machine Translation", "id": "992", "venue": "ACL", "authors": "Baosong Yang, Derek F. Wong, Tong Xiao, Lidia S. Chao and Jingbo Zhu", "abstract": "This paper proposes a hierarchical attentional neural translation model which\nfocuses on enhancing source-side hierarchical representations by covering both\nlocal and global semantic information using a bidirectional tree-based encoder.\nTo maximize the predictive likelihood of target words, a weighted variant of an\nattention mechanism is used to balance the attentive information between\nlexical and phrase vectors. Using a tree-based rare word encoding, the proposed\nmodel is extended to sub-word level to alleviate the out-of-vocabulary (OOV)\nproblem. Empirical results reveal that the proposed model significantly\noutperforms sequence-to-sequence attention-based and tree-based neural\ntranslation models in English-Chinese translation tasks."}, {"title": "Exploring Hyperparameter Sensitivity in Neural Machine Translation Architectures", "id": "1157", "venue": "ACL", "authors": "Denny Britz, Anna Goldie, Minh-Thang Luong and Quoc Le", "abstract": "Neural Machine Translation (NMT) has shown remarkable progress over the past\nfew years, with production systems now being deployed to end-users.\n    As the field is moving rapidly, it has become unclear which elements of NMT\narchitectures have a significant impact on translation quality.\n    In this work, we present a large-scale analysis of the sensitivity of NMT\narchitectures to common hyperparameters. We report empirical results and\nvariance numbers for several hundred experimental runs, corresponding to over\n250,000 GPU hours on a WMT English to German translation task. Our experiments\nprovide practical insights into the relative importance of factors such as\nembedding size, network depth, RNN cell type, residual connections, attention\nmechanism, and decoding heuristics. As part of this contribution, we also\nrelease an open-source NMT framework in TensorFlow to make it easy for others\nto reproduce our results and perform their own experiments."}, {"title": "Learning Translations via Matrix Completion", "id": "1161", "venue": "ACL", "authors": "Derry Tanti Wijaya, Brendan Callahan, John Hewitt, Jie Gao, Xiao Ling, Marianna Apidianaki and Chris Callison-Burch", "abstract": "Bilingual Lexicon Induction is the task of learning  word  translations \nwithout  bilingual parallel corpora. We model this task as a matrix completion\nproblem, and present an effective and extendable framework for completing the\nmatrix. This method harnesses diverse bilingual and monolingual signals, each\nof which may be incomplete or noisy. Our model achieves state-of-the-art\nperformance for both high and low resource languages."}, {"title": "Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback", "id": "1355", "venue": "ACL", "authors": "Khanh Nguyen, Hal Daum\u00e9 III and Jordan Boyd-Graber", "abstract": "Machine translation is a natural candidate\nproblem for reinforcement learning from\nhuman feedback: users provide quick,\ndirty ratings on candidate translations to\nguide a system to improve. Yet, current\nneural machine translation training focuses\non expensive human-generated reference\ntranslations. We describe a reinforcement\nlearning algorithm that improves\nneural machine translation systems\nfrom simulated human feedback.\nOur algorithm combines the advantage\nactor-critic algorithm (Mnih et al., 2016)\nwith the attention-based neural encoder-decoder\narchitecture (Luong et al., 2015).\nThis algorithm (a) is well-designed for\nproblems with a large action space and\ndelayed rewards, (b) effectively optimizes\ntraditional corpus-level machine translation\nmetrics, and (c) is robust to skewed,\nhigh-variance, granular feedback modeled\nafter actual human behaviors."}, {"title": "Towards Compact and Fast Neural Machine Translation Using a Combined Method", "id": "338", "venue": "ACL", "authors": "Xiaowei Zhang, Wei Chen, Feng Wang, Shuang Xu and Bo Xu", "abstract": "Neural Machine Translation (NMT) lays intensive burden on computation and\nmemory cost. It is a challenge to deploy NMT models on the devices with limited\ncomputation and memory budgets. This paper presents a four stage pipeline to\ncompress model and speed up the decoding for NMT. Our method first introduces a\ncompact architecture based on convolutional encoder and weight shared\nembeddings. Then weight pruning is applied to obtain a sparse model. Next, we\npropose a fast sequence interpolation approach which enables the greedy\ndecoding to achieve performance on par with the beam search. Hence, the\ntime-consuming beam search can be replaced by simple greedy decoding. Finally,\nvocabulary selection is used to reduce the computation of softmax layer. Our\nfinal model achieves 10 times speedup, 17 times parameters reduction, less than\n35MB storage size and comparable performance compared to the baseline model."}, {"title": "Instance Weighting for Neural Machine Translation Domain Adaptation", "id": "552", "venue": "ACL", "authors": "Rui Wang, Masao Utiyama, Lemao Liu, Kehai Chen and Eiichiro Sumita", "abstract": "Instance weighting has been widely applied to phrase-based machine translation\ndomain adaptation. However, it is challenging to be applied to Neural Machine\nTranslation (NMT) directly, because NMT is not a linear model. In this paper,\ntwo instance weighting technologies, i.e., sentence weighting and domain\nweighting with a dynamic weight learning strategy, are proposed for NMT domain\nadaptation. Empirical results on the IWSLT English-German/French tasks show\nthat the proposed methods can substantially improve NMT performance by up to\n2.7-6.7 BLEU points, outperforming the existing baselines by up to 1.6-3.6 BLEU\npoints."}, {"title": "Regularization techniques for fine-tuning in neural machine translation", "id": "704", "venue": "ACL", "authors": "Antonio Valerio Miceli Barone, Barry Haddow, Ulrich Germann and Rico Sennrich", "abstract": "We investigate techniques for supervised domain adaptation for neural machine\ntranslation where an existing model trained on a large out-of-domain dataset is\nadapted to a small in-domain dataset.  \nIn this scenario, overfitting is a major challenge. We investigate a number of\ntechniques to reduce overfitting and improve transfer learning, including\nregularization techniques such as dropout and L2-regularization towards an\nout-of-domain prior. In addition, we introduce tuneout, a novel regularization\ntechnique inspired by dropout.\nWe apply these techniques, alone and in combination, to neural machine\ntranslation, obtaining improvements on IWSLT datasets for English->German and\nEnglish$->Russian.\nWe also investigate the amounts of in-domain training data needed for domain\nadaptation in NMT, and find a logarithmic relationship between the amount of\ntraining data and gain in BLEU score."}, {"title": "Source-Side Left-to-Right or Target-Side Left-to-Right? An Empirical Comparison of Two Phrase-Based Decoding Algorithms", "id": "744", "venue": "ACL", "authors": "Yin-Wen Chang and Michael Collins", "abstract": "This paper describes an empirical study of the phrase-based decoding algorithm\nproposed by Chang and Collins (2017).  The algorithm produces a translation by\nprocessing the source-language sentence in strictly left-to-right order,\ndiffering from commonly used approaches that build the target-language sentence\nin left-to-right order. Our results show that the new algorithm is competitive\nwith Moses (Koehn et al., 2007) in terms of both speed and BLEU scores."}, {"title": "Using Target-side Monolingual Data for Neural Machine Translation through Multi-task Learning", "id": "794", "venue": "ACL", "authors": "Tobias Domhan and Felix Hieber", "abstract": "The performance of Neural Machine Translation (NMT) models relies heavily on\nthe availability of sufficient amounts of parallel data, and an efficient and\neffective way of leveraging the vastly available amounts of monolingual data\nhas yet to be found.\nWe propose to modify the decoder in a neural sequence-to-sequence model to\nenable multi-task learning for two strongly related tasks: target-side language\nmodeling and translation.\nThe decoder predicts the next target word through two channels, a target-side\nlanguage model on the lowest layer, and an attentional recurrent model which is\nconditioned on the source representation.\nThis architecture allows joint training on both large amounts of monolingual\nand moderate amounts of bilingual data to improve NMT performance.\nInitial results in the news domain for three language pairs show moderate but\nconsistent improvements over a baseline trained on bilingual data only."}], "chair_affiliation": "Simon Fraser University", "code": "4F", "room": "Copenhagen", "end_time": "12:10", "id": "33", "start_time": "10:30", "talks": [], "chair": "Anahita Mansouri Bigvand"}], [{"title": "Lunch", "start_time_iso": "2017-09-10T12:10:00", "end_time": "13:40", "end_time_iso": "2017-09-10T13:40:00", "id": "34", "start_time": "12:10", "type": "break"}], [{"title": "SIGDAT Business Meeting", "code": null, "id": "35", "start_time_iso": "2017-09-10T12:40:00", "end_time": "13:40", "end_time_iso": "2017-09-10T13:40:00", "type": "other", "room": null, "start_time": "12:40", "talks": [], "posters": []}], [{"title": "Semantics 3", "start_time_iso": "2017-09-10T13:40:00", "end_time_iso": "2017-09-10T15:20:00", "type": "paper", "posters": [], "chair_affiliation": "Sapienza University of Rome", "code": "5A", "room": "Jutland", "end_time": "15:20", "id": "36", "start_time": "13:40", "talks": [{"title": "Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling", "venue": "ACL", "end_time": "14:05", "abstract": "Semantic role labeling (SRL) is the task of identifying the predicate-argument\nstructure of a sentence. \nIt is typically regarded as an important step in the standard NLP pipeline.\nAs the semantic representations are closely related to syntactic ones, we\nexploit syntactic information in our model.  \nWe propose a version of graph convolutional networks (GCNs), a recent class of\nneural networks operating on graphs, suited to model syntactic dependency\ngraphs. \nGCNs over syntactic dependency trees are used as sentence encoders, producing\nlatent feature representations of words in a sentence.\nWe observe that GCN layers are complementary to LSTM ones: when we stack both\nGCN and LSTM layers, we obtain a substantial improvement over an already\nstate-of-the-art LSTM SRL model, resulting in the best reported scores on the\nstandard benchmark (CoNLL-2009) both for Chinese and English.", "id": "683", "start_time": "13:40", "authors": "Diego Marcheggiani and Ivan Titov"}, {"title": "Neural Semantic Parsing with Type Constraints for Semi-Structured Tables", "venue": "ACL", "end_time": "14:30", "abstract": "We present a new semantic parsing model for answering compositional questions\non semi-structured Wikipedia tables. Our parser is an encoder-decoder neural\nnetwork with two key technical innovations: (1) a grammar for the decoder that\nonly generates well-typed logical forms; and (2) an entity embedding and\nlinking module that identifies entity mentions while generalizing across\ntables. We also introduce a novel method for training our neural model with\nquestion-answer supervision. On the WikiTableQuestions data set, our parser\nachieves a state-of-the-art accuracy of 43.3% for a single model and 45.9% for\na 5-model ensemble, improving on the best prior score of 38.7% set by a\n15-model ensemble. These results suggest that type constraints and entity\nlinking are valuable components to incorporate in neural semantic parsers.", "id": "1033", "start_time": "14:05", "authors": "Jayant Krishnamurthy, Pradeep Dasigi and Matt Gardner"}, {"title": "Joint Concept Learning and Semantic Parsing from Natural Language Explanations", "venue": "ACL", "end_time": "14:55", "abstract": "Natural language constitutes a predominant medium for much of human learning\nand pedagogy. We consider the problem of concept learning from natural language\nexplanations, and a small number of labeled examples of the concept. For\nexample, in learning the concept of a phishing email, one might say `this is a\nphishing email because it asks for your bank account number'. Solving this\nproblem involves both learning to interpret open ended natural language\nstatements, and learning the concept itself. We present a joint model for (1)\nlanguage interpretation (semantic parsing) and (2) concept learning\n(classification) that does not require labeling statements with logical forms.\nInstead, the model prefers discriminative interpretations of statements in\ncontext of observable features of the data as a weak signal for parsing. On a\ndataset of email-related concepts, our approach yields across-the-board\nimprovements in classification performance, with a 30% relative improvement in\nF1 score over competitive methods in the low data regime.", "id": "1255", "start_time": "14:30", "authors": "Shashank Srivastava, Igor Labutov and Tom Mitchell"}, {"title": "Grasping the Finer Point: A Supervised Similarity Network for Metaphor Detection", "venue": "ACL", "end_time": "15:20", "abstract": "The ubiquity of metaphor in our everyday communication makes it an important\nproblem for natural language understanding. Yet, the majority of metaphor\nprocessing systems to date rely on hand-engineered features and there is still\nno consensus in the field as to which features are optimal for this task. In\nthis paper, we present the first deep learning architecture designed to capture\nmetaphorical composition. Our results demonstrate that it outperforms the\nexisting approaches in the metaphor identification task.", "id": "435", "start_time": "14:55", "authors": "Marek Rei, Luana Bulat, Douwe Kiela and Ekaterina Shutova"}], "chair": "Roberto Navigli"}, {"title": "Computational Social Science 1", "start_time_iso": "2017-09-10T13:40:00", "end_time_iso": "2017-09-10T15:20:00", "type": "paper", "posters": [], "chair_affiliation": "University of Washington", "code": "5B", "room": "Funen", "end_time": "15:20", "id": "37", "start_time": "13:40", "talks": [{"title": "Identifying civilians killed by police with distantly supervised entity-event extraction", "venue": "ACL", "end_time": "14:05", "abstract": "We propose a new, socially-impactful task for natural language processing: from\na news corpus, extract names of persons who have been killed by police. We\npresent a newly collected police fatality corpus, which we release publicly,\nand present a model to solve this problem that uses EM-based distant\nsupervision with logistic regression and convolutional neural network\nclassifiers. Our model outperforms two off-the-shelf event extractor systems,\nand it can suggest candidate victim names in some cases faster than one of the\nmajor manually-collected police fatality databases.", "id": "869", "start_time": "13:40", "authors": "Katherine Keith, Abram Handler, Michael Pinkham, Cara Magliozzi, Joshua McDuffie and Brendan O'Connor"}, {"title": "Asking too much? The rhetorical role of questions in political discourse", "venue": "ACL", "end_time": "14:30", "abstract": "Questions play a prominent role in social interactions, performing rhetorical\nfunctions that go beyond that of simple informational exchange.  The surface\nform of a question can signal the intention and background of the person asking\nit, as well as the nature of their relation with the interlocutor.  While the\ninformational nature of questions has been extensively examined in the context\nof question-answering applications, their rhetorical aspects have been largely\nunderstudied.\n\nIn this work we introduce an unsupervised methodology for extracting surface\nmotifs that recur in questions, and for grouping them according to their latent\nrhetorical role.  By applying this framework to the setting of question\nsessions in the UK parliament, we show that the resulting typology encodes key\naspects of the political discourse---such as the bifurcation in questioning\nbehavior between government and opposition parties---and reveals new insights\ninto the effects of a legislator's tenure and political career ambitions.", "id": "1165", "start_time": "14:05", "authors": "Justine Zhang, Arthur Spirling and Cristian Danescu-Niculescu-Mizil"}, {"title": "Detecting Perspectives in Political Debates", "venue": "ACL", "end_time": "14:55", "abstract": "We explore how to detect people's perspectives that occupy a certain\nproposition. We propose a Bayesian modelling approach where topics (or\npropositions) and their associated perspectives (or viewpoints) are modeled as\nlatent variables. Words associated with topics or perspectives follow different\ngenerative routes. Based on the extracted perspectives, we can extract the top\nassociated sentences from text to generate a succinct summary which allows a\nquick glimpse of the main viewpoints in a document. The model is evaluated on\ndebates from the House of Commons of the UK Parliament, revealing perspectives\nfrom the debates without the use of labelled data and obtaining better results\nthan previous related solutions under a variety of evaluations.", "id": "329", "start_time": "14:30", "authors": "David Vilares and Yulan He"}, {"title": "\"i have a feeling trump will win..................\": Forecasting Winners and Losers from User Predictions on Twitter", "venue": "ACL", "end_time": "15:20", "abstract": "Social media users often make explicit predictions about upcoming events. Such\nstatements vary in the degree of certainty the author expresses toward the\noutcome: \"Leonardo DiCaprio will win Best Actor\" vs. \"Leonardo DiCaprio may\nwin\" or \"No way Leonardo wins!\". Can popular beliefs on social media predict\nwho will win? To answer this question, we build a corpus of tweets annotated\nfor veridicality on which we train a log-linear classifier that detects\npositive veridicality with high precision. We then forecast uncertain outcomes\nusing the wisdom of crowds, by aggregating users' explicit predictions. Our\nmethod for forecasting winners is fully automated, relying only on a set of\ncontenders as input. It requires no training data of past outcomes and\noutperforms sentiment and tweet volume baselines on a broad range of contest\nprediction tasks. We further demonstrate how our approach can be used to\nmeasure the reliability of individual accounts' predictions and retrospectively\nidentify surprise outcomes.", "id": "1022", "start_time": "14:55", "authors": "Sandesh Swamy, Alan Ritter and Marie-Catherine de Marneffe"}], "chair": "Noah A. Smith"}, {"title": "Sentiment Analysis 2", "start_time_iso": "2017-09-10T13:40:00", "end_time_iso": "2017-09-10T15:20:00", "type": "paper", "posters": [], "chair_affiliation": "Hong Kong University of Science & Technology", "code": "5C", "room": "Zealand", "end_time": "15:20", "id": "38", "start_time": "13:40", "talks": [{"title": "A Question Answering Approach for Emotion Cause Extraction", "venue": "ACL", "end_time": "14:05", "abstract": "Emotion cause extraction aims to identify the reasons behind a certain emotion\nexpressed in text. It is a much more difficult task compared to emotion\nclassification. Inspired by recent advances in using deep memory networks for\nquestion answering (QA), we propose a new approach which considers emotion\ncause identification as a reading comprehension task in QA. Inspired by\nconvolutional neural networks, we propose a new mechanism to store relevant\ncontext in different memory slots to model context information. Our proposed\napproach can extract both word level sequence features and lexical features.\nPerformance evaluation shows that our method achieves the state-of-the-art\nperformance on a recently released emotion cause dataset, outperforming a\nnumber of competitive baselines by at least 3.01% in F-measure.", "id": "439", "start_time": "13:40", "authors": "Lin Gui, Jiannan Hu, Yulan He, Ruifeng Xu, Lu Qin and Jiachen Du"}, {"title": "Story Comprehension for Predicting What Happens Next", "venue": "ACL", "end_time": "14:30", "abstract": "Automatic story comprehension is a fundamental challenge in Natural Language\nUnderstanding, and can enable computers to learn about social norms, human\nbehavior and commonsense. In this paper, we present a story comprehension model\nthat explores three distinct semantic aspects: (i) the sequence of events\ndescribed in the story, (ii) its emotional trajectory, and (iii) its plot\nconsistency. We judge the model's understanding of real-world stories by\ninquiring if, like humans, it can develop an expectation of what will happen\nnext in a given story. Specifically, we use it to predict the correct ending of\na given short story from possible alternatives. The model uses a hidden\nvariable to weigh the semantic aspects in the context of the story. Our\nexperiments demonstrate the potential of our approach to characterize these\nsemantic aspects, and the strength of the hidden variable based approach. The\nmodel outperforms the state-of-the-art approaches and achieves best results on\na publicly available dataset.", "id": "1138", "start_time": "14:05", "authors": "Snigdha Chaturvedi, Haoruo Peng and Dan Roth"}, {"title": "Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm", "venue": "ACL", "end_time": "14:55", "abstract": "NLP tasks are often limited by scarcity of manually annotated data. In social\nmedia sentiment analysis and related tasks, researchers have therefore used\nbinarized emoticons and specific hashtags as forms of distant supervision. Our\npaper shows that by extending the distant supervision to a more diverse set of\nnoisy labels, the models can learn richer representations. Through emoji\nprediction on a dataset of 1246 million tweets containing one of 64 common\nemojis we obtain state-of-the-art performance on 8 benchmark datasets\nwithin emotion, sentiment and sarcasm detection using a single pretrained\nmodel. Our analyses confirm that the diversity of our emotional labels yield a\nperformance improvement over previous distant supervision approaches.", "id": "474", "start_time": "14:30", "authors": "Bjarke Felbo, Alan Mislove, Anders S\u00f8gaard, Iyad Rahwan and Sune Lehmann"}, {"title": "Opinion Recommendation Using A Neural Model", "venue": "ACL", "end_time": "15:20", "abstract": "We present opinion recommendation, a novel task of jointly generating a review\nwith a rating score that a certain user would give to a certain product which\nis unreviewed by the user, given existing reviews to the product by other\nusers, and the reviews that the user has given to other products. A\ncharacteristic of opinion recommendation is the reliance of multiple data\nsources for multi-task joint learning. We use a single neural network to model\nusers and products, generating customised product representations using a deep\nmemory network, from which customised ratings and reviews are constructed\njointly. Results show that our opinion recommendation system gives ratings that\nare closer to real user ratings on Yelp.com data compared with Yelp's own\nratings. our methods give better results compared to several pipelines\nbaselines.", "id": "1077", "start_time": "14:55", "authors": "Zhongqing Wang and Yue Zhang"}], "chair": "Pascale Fung"}, {"title": "Poster Session. Syntax 3", "start_time_iso": "2017-09-10T13:40:00", "end_time_iso": "2017-09-10T15:20:00", "type": "poster", "posters": [{"title": "CRF Autoencoder for Unsupervised Dependency Parsing", "id": "836", "venue": "ACL", "authors": "Jiong Cai, Yong Jiang and Kewei Tu", "abstract": "Unsupervised dependency parsing, which tries to discover linguistic dependency\nstructures from unannotated data, is a very challenging task. Almost all\nprevious work on this task focuses on learning generative models. In this\npaper, we develop an unsupervised dependency parsing model based on the CRF\nautoencoder. The encoder part of our model is discriminative and globally\nnormalized which allows us to use rich features as well as universal linguistic\npriors. We propose an exact algorithm for parsing as well as a tractable\nlearning algorithm. We evaluated the performance of our model on eight\nmultilingual treebanks and found that our model achieved comparable performance\nwith state-of-the-art approaches."}, {"title": "Efficient Discontinuous Phrase-Structure Parsing via the Generalized Maximum Spanning Arborescence", "id": "614", "venue": "ACL", "authors": "Caio Corro, Joseph Le Roux and Mathieu Lacroix", "abstract": "We present a new method for the joint task of tagging and non-projective\ndependency parsing. We demonstrate its usefulness with an application to\ndiscontinuous phrase-structure parsing where decoding lexicalized spines and\nsyntactic derivations is performed jointly. The main contributions of this\npaper are (1) a reduction from joint tagging and non-projective dependency\nparsing to the Generalized Maximum Spanning Arborescence problem, and (2) a\nnovel decoding algorithm for this problem through Lagrangian relaxation. We\nevaluate this model and obtain state-of-the-art results despite strong\nindependence assumptions."}, {"title": "Incremental Graph-based Neural Dependency Parsing", "id": "720", "venue": "ACL", "authors": "Xiaoqing Zheng", "abstract": "Very recently, some studies on neural dependency parsers have shown advantage\nover the traditional ones on a wide variety of languages. However, for\ngraph-based neural dependency parsing systems, they either count on the\nlong-term memory and attention mechanism to implicitly capture the high-order\nfeatures or give up the global exhaustive inference algorithms in order to\nharness the features over a rich history of parsing decisions. The former might\nmiss out the important features for specific headword predictions without the\nhelp of the explicit structural information, and the latter may suffer from the\nerror propagation as false early structural constraints are used to create\nfeatures when making future predictions. We explore the feasibility of\nexplicitly taking high-order features into account while remaining the main\nadvantage of global inference and learning for graph-based parsing. The\nproposed parser first forms an initial parse tree by head-modifier predictions\nbased on the first-order factorization. High-order features (such as\ngrandparent, sibling, and uncle) then can be defined over the initial tree, and\nused to refine the parse tree in an iterative fashion. Experimental results\nshowed that our model (called INDP) archived competitive performance to\nexisting benchmark parsers on both English and Chinese datasets."}, {"title": "Neural Discontinuous Constituency Parsing", "id": "1464", "venue": "ACL", "authors": "Milo\u0161 Stanojevi\u0107 and Raquel Garrido Alhama", "abstract": "One of the most pressing issues in discontinuous constituency transition-based\nparsing is that the relevant information for parsing decisions could be located\nin any part of the stack or the buffer. \nIn this paper, we propose a solution to this problem by replacing the\nstructured perceptron model with a recursive neural model that computes a\nglobal representation of the configuration, therefore allowing even the most\nremote parts of the configuration to influence the parsing decisions. We also\nprovide a detailed analysis of how this representation should be built out of\nsub-representations of its core elements (words, trees and stack).\n Additionally, we investigate how different types of swap oracles influence the\nresults.  Our model is the first neural discontinuous constituency parser, and\nit outperforms all the previously published models on three out of four\ndatasets while on the fourth it obtains second place by a tiny difference."}, {"title": "Stack-based Multi-layer Attention for Transition-based Dependency Parsing", "id": "638", "venue": "ACL", "authors": "Zhirui Zhang, Shujie Liu, Mu Li, Ming Zhou and Enhong Chen", "abstract": "Although sequence-to-sequence (seq2seq) network has achieved significant\nsuccess in many NLP tasks such as machine translation and text summarization,\nsimply applying this approach to transition-based dependency parsing cannot\nyield a comparable performance gain as in other state-of-the-art methods, such\nas stack-LSTM and head selection. In this paper, we propose a stack-based\nmulti-layer attention model for seq2seq learning to better leverage structural\nlinguistics information. In our method, two binary vectors are used to track\nthe decoding stack in transition-based parsing, and multi-layer attention is\nintroduced to capture multiple word dependencies in partial trees. We conduct\nexperiments on PTB and CTB datasets, and the results show that our proposed\nmodel achieves state-of-the-art accuracy and significant improvement in labeled\nprecision with respect to the baseline seq2seq model."}, {"title": "Dependency Grammar Induction with Neural Lexicalization and Big Training Data", "id": "654", "venue": "ACL", "authors": "Wenjuan Han, Yong Jiang and Kewei Tu", "abstract": "We study the impact of big models (in terms of the degree of lexicalization)\nand big data (in terms of the training corpus size) on dependency grammar\ninduction.\nWe experimented with L-DMV, a lexicalized version of Dependency Model with\nValence \\cite{Klein:2004:CIS:1218955.1219016} and L-NDMV, our lexicalized\nextension of the Neural Dependency Model with Valence\n\\cite{jiang-han-tu:2016:EMNLP2016}. \nWe find that L-DMV only benefits from very small degrees of lexicalization and\nmoderate sizes of training corpora. L-NDMV can benefit from big training data\nand lexicalization of greater degrees, especially when enhanced with good model\ninitialization, and it achieves a result that is competitive with the current\nstate-of-the-art."}, {"title": "Combining Generative and Discriminative Approaches to Unsupervised Dependency Parsing via Dual Decomposition", "id": "361", "venue": "ACL", "authors": "Yong Jiang, Wenjuan Han and Kewei Tu", "abstract": "Unsupervised dependency parsing aims to learn a dependency parser from\nunannotated sentences. Existing work focuses on either learning generative\nmodels using the expectation-maximization algorithm and its variants, or\nlearning discriminative models using the discriminative clustering algorithm.\nIn this paper, we propose a new learning strategy that learns a generative\nmodel and a discriminative model jointly based on the dual decomposition\nmethod. Our method is simple and general, yet effective to capture the\nadvantages of both models and improve their learning results. We tested our\nmethod on the UD treebank and achieved a state-of-the-art performance on thirty\nlanguages."}, {"title": "Effective Inference for Generative Neural Parsing", "id": "1298", "venue": "ACL", "authors": "Mitchell Stern, Daniel Fried and Dan Klein", "abstract": "Generative neural models have recently achieved state-of-the-art results for\nconstituency parsing. However, without a feasible search procedure, their use\nhas so far been limited to reranking the output of external parsers in which\ndecoding is more tractable. We describe an alternative to the conventional\naction-level beam search used for discriminative neural models that enables us\nto decode directly in these generative models. We then show that by improving\nour basic candidate selection strategy and using a coarse pruning function, we\ncan improve accuracy while exploring significantly less of the search space.\nApplied to the model of Choe and Charniak (2016), our inference procedure\nobtains 92.56 F1 on section 23 of the Penn Treebank, surpassing prior\nstate-of-the-art results for single-model systems."}, {"title": "Semi-supervised Structured Prediction with Neural CRF Autoencoder", "id": "493", "venue": "ACL", "authors": "Xiao Zhang, Yong Jiang, Hao Peng, Kewei Tu and Dan Goldwasser", "abstract": "In this paper we propose an end-to-end neural CRF autoencoder (NCRF-AE) model\nfor semi-supervised learning of sequential structured prediction problems. Our\nNCRF-AE consists of two parts: an encoder which is a CRF model enhanced by deep\nneural networks, and a decoder which is a generative model trying to\nreconstruct the input. Our model has a unified structure with different loss\nfunctions for labeled and unlabeled data with shared parameters. We developed a\nvariation of the EM algorithm for optimizing both the encoder and the decoder\nsimultaneously by decoupling their parameters. Our Experimental results over\nthe Part-of-Speech (POS) tagging task on eight different languages, show that\nour model can outperform competitive systems in both supervised and\nsemi-supervised scenarios."}, {"title": "TAG Parsing with Neural Networks and Vector Representations of Supertags", "id": "1377", "venue": "ACL", "authors": "Jungo Kasai, Bob Frank, Tom McCoy, Owen Rambow and Alexis Nasr", "abstract": "We present supertagging-based models for Tree Adjoining Grammar parsing that\nuse neural network architectures and dense vector representation of supertags\n(elementary trees) to achieve state-of-the-art performance in unlabeled and\nlabeled attachment scores. The shift-reduce parsing model eschews lexical\ninformation entirely, and uses only the 1-best supertags to parse a sentence,\nproviding further support for the claim that supertagging is \"almost parsing.\"\nWe demonstrate that the embedding vector representations the parser induces for\nsupertags possess linguistically interpretable structure, supporting analogies\nbetween grammatical structures like those familiar from recent work in\ndistributional semantics. This dense representation of supertags overcomes the\ndrawbacks for statistical models of TAG as compared to CCG parsing, raising the\npossibility that TAG is a viable alternative for NLP tasks that require the\nassignment of richer structural descriptions to sentences."}], "chair_affiliation": "Johns Hopkins University", "code": "5D", "room": "Aarhus", "end_time": "15:20", "id": "39", "start_time": "13:40", "talks": [], "chair": "Ryan Cotterell"}, {"title": "Poster Session. Relations", "start_time_iso": "2017-09-10T13:40:00", "end_time_iso": "2017-09-10T15:20:00", "type": "poster", "posters": [{"title": "Global Normalization of Convolutional Neural Networks for Joint Entity and Relation Classification", "id": "343", "venue": "ACL", "authors": "Heike Adel and Hinrich Sch\u00fctze", "abstract": "We introduce globally normalized convolutional neural networks for joint entity\nclassification and relation extraction. In particular, we propose a way to\nutilize a linear-chain conditional random field output layer for predicting\nentity types and relations between entities at the same time. Our experiments\nshow that global normalization outperforms a locally normalized softmax layer\non a benchmark dataset."}, {"title": "End-to-End Neural Relation Extraction with Global Optimization", "id": "308", "venue": "ACL", "authors": "Meishan Zhang, Yue Zhang and Guohong Fu", "abstract": "Neural networks have shown promising results for relation extraction.\nState-of-the-art models cast the task as an end-to-end problem, \nsolved incrementally using a local classifier.\nYet previous work using statistical models have demonstrated that global\noptimization can achieve better performances compared to local classification.\nWe build a globally optimized neural model for end-to-end relation extraction,\nproposing novel LSTM features in order to better learn context representations.\nIn addition, we present a novel method to integrate syntactic information to\nfacilitate global learning, yet requiring little background on syntactic\ngrammars thus being easy to extend. Experimental results show that our proposed\nmodel is highly effective,\nachieving the best performances on two standard benchmarks."}, {"title": "KGEval: Accuracy Estimation of Automatically Constructed Knowledge Graphs", "id": "1289", "venue": "ACL", "authors": "Prakhar Ojha and Partha Talukdar", "abstract": "Automatic construction of large knowledge graphs (KG) by mining web-scale text\ndatasets has received considerable attention recently. Estimating accuracy of\nsuch automatically constructed KGs is a challenging problem due to their size\nand diversity. This important problem has largely been ignored in prior\nresearch \u2013 we fill this gap and propose KGEval. KGEval uses coupling\nconstraints to bind facts and crowdsources those few that can infer large parts\nof the graph. We demonstrate that the objective optimized by KGEval is\nsubmodular and NP-hard, allowing guarantees for our approximation algorithm.\nThrough experiments on real-world datasets, we demonstrate that KGEval best\nestimates KG accuracy compared to other baselines, while requiring\nsignificantly lesser number of human evaluations."}, {"title": "Sparsity and Noise: Where Knowledge Graph Embeddings Fall Short", "id": "1127", "venue": "ACL", "authors": "Jay Pujara, Eriq Augustine and Lise Getoor", "abstract": "Knowledge graph (KG) embedding techniques use structured relationships between\nentities to learn low-dimensional representations of entities and relations.\nOne prominent goal of these approaches is to improve the quality of knowledge\ngraphs by removing errors and adding missing facts. Surprisingly, most\nembedding techniques have been evaluated on benchmark datasets consisting of\ndense and reliable subsets of human-curated KGs, which tend to be fairly\ncomplete and have few errors. In this paper, we consider the problem of\napplying embedding techniques to KGs extracted from text, which are often\nincomplete and contain errors. We compare the sparsity and unreliability of\ndifferent KGs and perform empirical experiments demonstrating how embedding\napproaches degrade as sparsity and unreliability increase."}, {"title": "Dual Tensor Model for Detecting Asymmetric Lexico-Semantic Relations", "id": "1056", "venue": "ACL", "authors": "Goran Glava\u0161 and Simone Paolo Ponzetto", "abstract": "Detection of lexico-semantic relations is one of the central tasks of\ncomputational semantics. Although some fundamental relations (e.g., hypernymy)\nare asymmetric, most existing models account for asymmetry only implicitly and\nuse the same concept representations to support detection of symmetric and\nasymmetric relations alike. In this work, we propose the Dual Tensor model, a\nneural architecture with which we explicitly model the asymmetry and capture\nthe translation between unspecialized and specialized word embeddings via a\npair of tensors. Although our Dual Tensor model needs only unspecialized\nembeddings as input, our experiments on hypernymy and meronymy detection\nsuggest that it can outperform more complex and resource-intensive models. We\nfurther demonstrate that the model can account for polysemy and that it\nexhibits stable performance across languages."}, {"title": "Incorporating Relation Paths in Neural Relation Extraction", "id": "1333", "venue": "ACL", "authors": "Wenyuan Zeng, Yankai Lin, Zhiyuan Liu and Maosong Sun", "abstract": "Distantly supervised relation extraction has been widely used to find novel\nrelational facts from plain text. To predict the relation between a pair of two\ntarget entities, existing methods solely rely on those direct sentences\ncontaining both entities. In fact, there are also many sentences containing\nonly one of the target entities, which also provide rich useful information but\nnot yet employed by relation extraction. To address this issue, we build\ninference chains between two target entities via intermediate entities, and\npropose a path-based neural relation extraction model to encode the relational\nsemantics from both direct sentences and inference chains. Experimental results\non real-world datasets show that, our model can make full use of those\nsentences containing only one target entity, and achieves significant and\nconsistent improvements on relation extraction as compared with strong\nbaselines. The source code of this paper can be obtained from https://\ngithub.com/thunlp/PathNRE."}, {"title": "Adversarial Training for Relation Extraction", "id": "170", "venue": "ACL", "authors": "Yi Wu, David Bamman and Stuart Russell", "abstract": "Adversarial training is a mean of regularizing classification algorithms by\ngenerating adversarial noise to the training data. We apply adversarial\ntraining in relation extraction within the multi-instance multi-label learning\nframework. We evaluate various neural network architectures on two different\ndatasets. Experimental results demonstrate that adversarial training is\ngenerally effective for both CNN and RNN models and significantly improves the\nprecision of predicted relations."}, {"title": "Context-Aware Representations for Knowledge Base Relation Extraction", "id": "306", "venue": "ACL", "authors": "Daniil Sorokin and Iryna Gurevych", "abstract": "We demonstrate that for sentence-level relation extraction it is beneficial to\nconsider other relations in the sentential context while predicting the target\nrelation. Our architecture uses an LSTM-based encoder to jointly learn\nrepresentations for all relations in a single sentence.  We combine the context\nrepresentations with an attention mechanism to make the final prediction. \n\nWe use the Wikidata knowledge base to construct a dataset of multiple relations\nper sentence and to evaluate our approach. Compared to a baseline system, our\nmethod results in an average error reduction of 24 on a held-out set of\nrelations.\n\nThe code and the dataset to replicate the experiments are made available at\nhttps://github.com/ukplab/."}, {"title": "A Soft-label Method for Noise-tolerant Distantly Supervised Relation Extraction", "id": "826", "venue": "ACL", "authors": "Tianyu Liu, Kexiang Wang, Baobao Chang and Zhifang Sui", "abstract": "Distant-supervised relation extraction inevitably suffers from wrong labeling\nproblems because it heuristically labels relational facts with knowledge bases.\nPrevious sentence level denoise models don\u2019t achieve satisfying performances\nbecause they use hard labels which are determined by distant supervision and\nimmutable during training. To this end, we introduce an entity-pair level\ndenoise method which exploits semantic information from correctly labeled\nentity pairs to correct wrong labels dynamically during training. We propose a\njoint score\nfunction which combines the relational scores based on the entity-pair\nrepresentation and the confidence of the hard label to obtain a new label,\nnamely a soft label, for certain entity pair. During training, soft labels\ninstead of hard labels serve as gold labels. Experiments on the benchmark\ndataset show that our method dramatically reduces noisy instances and\noutperforms other state-of-the-art systems."}, {"title": "A Sequential Model for Classifying Temporal Relations between Intra-Sentence Events", "id": "1006", "venue": "ACL", "authors": "Prafulla Kumar Choubey and Ruihong Huang", "abstract": "We present a sequential model for temporal relation classification between\nintra-sentence events. The key observation is that the overall syntactic\nstructure and compositional meanings of the multi-word context between events\nare important for distinguishing among fine-grained temporal relations.\nSpecifically, our approach first extracts a sequence of context words that\nindicates the temporal relation between two events, which well align with the\ndependency path between two event mentions. The context word sequence, together\nwith a parts-of-speech tag sequence and a dependency relation sequence that are\ngenerated corresponding to the word sequence, are then provided as input to\nbidirectional recurrent neural network (LSTM) models. The neural nets learn\ncompositional syntactic and semantic representations of contexts surrounding\nthe two events and predict the temporal relation between them. Evaluation of\nthe proposed approach on TimeBank corpus shows that sequential modeling is\ncapable of accurately recognizing temporal relations between events, which\noutperforms a neural net model using various discrete features as input that\nimitates previous feature based models."}, {"title": "Deep Residual Learning for Weakly-Supervised Relation Extraction", "id": "1222", "venue": "ACL", "authors": "YiYao Huang and William Yang Wang", "abstract": "Deep residual learning (ResNet) is a new method for training very deep neural\nnetworks using identity mapping for shortcut connections. ResNet has won the\nImageNet ILSVRC 2015 classification task, and achieved state-of-the-art\nperformances in many computer vision tasks. However, the effect of residual\nlearning on noisy natural language processing tasks is still not well\nunderstood. In this paper, we design a novel convolutional neural network (CNN)\nwith residual learning, and investigate its impacts on the task of distantly\nsupervised noisy relation extraction.  In contradictory to popular beliefs that\nResNet only works well for very deep networks, we found  that even with 9\nlayers of CNNs, using identity mapping could significantly improve the\nperformance for distantly-supervised relation extraction."}, {"title": "Noise-Clustered Distant Supervision for Relation Extraction: A Nonparametric Bayesian Perspective", "id": "1305", "venue": "ACL", "authors": "Qing Zhang and Houfeng Wang", "abstract": "For the task of relation extraction, distant supervision is an efficient\napproach to generate labeled data by aligning knowledge base with free texts.\nThe essence of it is a challenging incomplete multi-label classification\nproblem with sparse and noisy features. To address the challenge, this work\npresents a novel nonparametric Bayesian formulation for the task. Experiment\nresults show substantially higher top precision improvements over the\ntraditional state-of-the-art approaches."}, {"title": "Exploring Vector Spaces for Semantic Relations", "id": "785", "venue": "ACL", "authors": "Kata G\u00e1bor, Haifa Zargayouna, Isabelle Tellier, Davide Buscaldi and Thierry Charnois", "abstract": "Word embeddings are used with success for a variety of tasks involving lexical\nsemantic similarities between individual words. Using unsupervised methods and\njust cosine similarity, encouraging results were obtained for analogical\nsimilarities. In this paper, we explore the potential of pre-trained word\nembeddings to identify generic types of semantic relations in an unsupervised\nexperiment. We propose a new relational similarity measure based on the\ncombination of word2vec's CBOW input and output vectors which outperforms\nconcurrent vector representations, when used for unsupervised clustering on\nSemEval 2010 Relation Classification data."}, {"title": "Temporal dynamics of semantic relations in word embeddings: an application to predicting armed conflict participants", "id": "686", "venue": "ACL", "authors": "Andrey Kutuzov, Erik Velldal and Lilja \u00d8vrelid", "abstract": "This paper deals with using word embedding models to trace the temporal\ndynamics of semantic relations between pairs of words. The set-up is similar to\nthe well-known analogies task, but expanded with a time dimension. To this end,\nwe apply incremental updating of the models with new training texts, including\nincremental vocabulary expansion, coupled with learned transformation matrices\nthat let us map between members of the relation.\nThe proposed approach is evaluated on the task of predicting insurgent armed\ngroups based on geographical locations. The gold standard data for the time\nspan 1994--2010 is extracted from the UCDP Armed Conflicts dataset. The results\nshow that the method is feasible and outperforms the baselines, but also that\nimportant work still remains to be done."}], "chair_affiliation": "Carnegie Mellon University", "code": "5E", "room": "Odense", "end_time": "15:20", "id": "40", "start_time": "13:40", "talks": [], "chair": "Bishan Yang"}, {"title": "Poster Session. Language Models, Text Mining, and Crowd Sourcing", "start_time_iso": "2017-09-10T13:40:00", "end_time_iso": "2017-09-10T15:20:00", "type": "poster", "posters": [{"title": "Dynamic Entity Representations in Neural Language Models", "id": "1076", "venue": "ACL", "authors": "Yangfeng Ji, Chenhao Tan, Sebastian Martschat, Yejin Choi and Noah A. Smith", "abstract": "Understanding a long document requires tracking how entities are introduced and\nevolve over time. We present a new type of language model, EntityNLM, that can\nexplicitly model entities, dynamically update their representations, and\ncontextually generate their mentions. Our model is generative and flexible; it\ncan model an arbitrary number of entities in context while generating each\nentity mention at an arbitrary length. In addition, it can be used for several\ndifferent tasks such as language modeling, coreference resolution, and entity\nprediction. Experimental results with all these tasks demonstrate that our\nmodel consistently outperforms strong baselines and prior work."}, {"title": "Towards Quantum Language Models", "id": "603", "venue": "ACL", "authors": "Ivano Basile and Fabio Tamburini", "abstract": "This paper presents a new approach for building Language Models using the\nQuantum Probability Theory, a Quantum Language Model (QLM). It mainly shows\nthat relying on this probability calculus it is possible to build stochastic\nmodels able to benefit from quantum correlations due to interference and\nentanglement. We extensively tested our approach showing its superior\nperformances, both in terms of model perplexity and inserting it into an\nautomatic speech recognition evaluation setting, when compared with\nstate-of-the-art language modelling techniques."}, {"title": "Reference-Aware Language Models", "id": "938", "venue": "ACL", "authors": "Zichao Yang, Phil Blunsom, Chris Dyer and Wang Ling", "abstract": "We propose a general class of language models that treat reference as discrete\nstochastic latent variables. This decision allows for the creation of entity\nmentions by accessing external databases of referents (required by, e.g.,\ndialogue generation) or past internal state (required to explicitly model\ncoreferentiality). Beyond simple copying, our coreference model can\nadditionally refer to a referent using varied mention forms (e.g., a reference\nto \u201cJane\u201d can be realized as \u201cshe\u201d), a characteristic feature of\nreference in natural languages. Experiments on three representative\napplications show our model variants outperform models based on deterministic\nattention and standard language modeling baselines."}, {"title": "A Simple Language Model based on PMI Matrix Approximations", "id": "392", "venue": "ACL", "authors": "Oren Melamud, Ido Dagan and Jacob Goldberger", "abstract": "In this study, we introduce a new approach for learning language models by\ntraining them to estimate word-context pointwise mutual information (PMI), and\nthen deriving the desired conditional probabilities from PMI at test time.\nSpecifically, we show that with minor modifications to word2vec's algorithm, we\nget principled language models that are closely related to the well-established\nNoise Contrastive Estimation (NCE) based language models. A compelling aspect\nof our approach is that our models are trained with the same simple negative\nsampling objective function that is commonly used in word2vec to learn word\nembeddings."}, {"title": "Syllable-aware Neural Language Models: A Failure to Beat Character-aware Ones", "id": "399", "venue": "ACL", "authors": "Zhenisbek Assylbekov, Rustem Takhanov, Bagdat Myrzakhmetov and Jonathan N. Washington", "abstract": "Syllabification does not seem to improve word-level RNN language modeling\nquality when compared to character-based segmentation. However, our best\nsyllable-aware language model, achieving performance comparable to the\ncompetitive character-aware model, has 18%-33% fewer parameters and is trained\n1.2-2.2 times faster."}, {"title": "Inducing Semantic Micro-Clusters from Deep Multi-View Representations of Novels", "id": "629", "venue": "ACL", "authors": "Lea Frermann and Gy\u00f6rgy Szarvas", "abstract": "Automatically understanding the plot of novels is important both for informing\nliterary scholarship and applications such as summarization or recommendation.\nVarious models have addressed this task, but their evaluation has remained\nlargely intrinsic and qualitative. Here, we propose a principled and scalable\nframework leveraging expert-provided semantic tags (e.g., mystery, pirates) to\nevaluate plot representations in an extrinsic fashion, assessing their ability\nto produce locally coherent groupings of novels (micro-clusters) in model\nspace. We present a deep recurrent autoencoder model that learns richly\nstructured multi-view plot representations, and show that they i) yield better\nmicro-clusters than less structured representations; and ii) are interpretable,\nand thus useful for further literary analysis or labeling of the emerging\nmicro-clusters."}, {"title": "Initializing Convolutional Filters with Semantic Features for Text Classification", "id": "179", "venue": "ACL", "authors": "Shen Li, Zhe Zhao, Tao Liu, Renfen Hu and Xiaoyong Du", "abstract": "Convolutional Neural Networks (CNNs) are widely used in NLP tasks. This paper\npresents a novel weight initialization method to improve the CNNs for text\nclassification. Instead of randomly initializing the convolutional filters, we\nencode semantic features into them, which helps the model focus on learning\nuseful features at the beginning of the training. Experiments demonstrate the\neffectiveness of the initialization technique on seven text classification\ntasks, including sentiment analysis and topic classification."}, {"title": "Shortest-Path Graph Kernels for Document Similarity", "id": "330", "venue": "ACL", "authors": "Giannis Nikolentzos, Polykarpos Meladianos, Francois Rousseau, Yannis Stavrakas and Michalis Vazirgiannis", "abstract": "In this paper, we present a novel document similarity measure based on the\ndefinition of a graph kernel between pairs of documents. The proposed measure\ntakes into account both the terms contained in the documents and the\nrelationships between them. By representing each document as a graph-of-words,\nwe are able to model these relationships and then determine how similar two\ndocuments are by using a modified shortest-path graph kernel. We evaluate our\napproach on two tasks and compare it against several baseline approaches using\nvarious performance metrics such as DET curves and macro-average F1-score.\nExperimental results on a range of datasets showed that our proposed approach\noutperforms traditional techniques and is capable of measuring more accurately\nthe similarity between two documents."}, {"title": "Adapting Topic Models using Lexical Associations with Tree Priors", "id": "1075", "venue": "ACL", "authors": "Weiwei Yang, Jordan Boyd-Graber and Philip Resnik", "abstract": "Models work best when they are optimized taking into account the evaluation\ncriteria that people care about. For topic models, people often care about\ninterpretability, which can be approximated using measures of lexical\nassociation. We integrate lexical association into topic optimization using\ntree priors, which provide a flexible framework that can take advantage of both\nfirst order word associations and the higher-order associations captured by\nword embeddings. Tree priors improve topic interpretability without hurting\nextrinsic performance."}, {"title": "Finding Patterns in Noisy Crowds: Regression-based Annotation Aggregation for Crowdsourced Data", "id": "1079", "venue": "ACL", "authors": "Natalie Parde and Rodney Nielsen", "abstract": "Crowdsourcing offers a convenient means of obtaining labeled data quickly and\ninexpensively.              However, crowdsourced labels are often noisier than\nexpert-annotated data, making it difficult to aggregate them meaningfully.  We\npresent an aggregation approach that learns a regression model from\ncrowdsourced annotations to predict aggregated labels for instances that have\nno expert adjudications.  The predicted labels achieve a correlation of 0.594\nwith expert labels on our data, outperforming the best alternative aggregation\nmethod by 11.9%.  Our approach also outperforms the alternatives on third-party\ndatasets."}, {"title": "CROWD-IN-THE-LOOP: A Hybrid Approach for Annotating Semantic Roles", "id": "539", "venue": "ACL", "authors": "Chenguang Wang, Alan Akbik, laura chiticariu, Yunyao Li, Fei Xia and Anbang Xu", "abstract": "Crowdsourcing has proven to be an effective method for generating labeled data\nfor a range of NLP tasks. However, multiple recent attempts of using\ncrowdsourcing to generate gold-labeled training data for semantic role labeling\n(SRL) reported only modest results, indicating that SRL is perhaps too\ndifficult a task to be effectively crowdsourced. In this paper, we postulate\nthat while producing SRL annotation does require expert involvement in general,\na large subset of SRL labeling tasks is in fact appropriate for the crowd. We\npresent a novel workflow in which we employ a classifier to identify difficult\nannotation tasks and route each task either to experts or crowd workers\naccording to their difficulties. Our experimental evaluation shows that the\nproposed approach reduces the workload for experts by over two-thirds, and thus\nsignificantly reduces the cost of producing SRL annotation at little loss in\nquality."}, {"title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks", "id": "1011", "venue": "ACL", "authors": "Kazuma Hashimoto, caiming xiong, Yoshimasa Tsuruoka and Richard Socher", "abstract": "Transfer and multi-task learning have traditionally focused on either a single\nsource-target pair or very few, similar tasks.\nIdeally, the linguistic levels of morphology, syntax and semantics would\nbenefit each other by being trained in a single model.\nWe introduce a joint many-task model together with a strategy for successively\ngrowing its depth to solve increasingly complex tasks.\nHigher layers include shortcut connections to lower-level task predictions to\nreflect linguistic hierarchies.\nWe use a simple regularization term to allow for optimizing all model weights\nto improve one task's loss without exhibiting catastrophic interference of the\nother tasks.\nOur single end-to-end model obtains state-of-the-art or competitive results on\nfive different tasks from tagging, parsing, relatedness, and entailment tasks."}], "chair_affiliation": "Harvard University", "code": "5F", "room": "Copenhagen", "end_time": "15:20", "id": "41", "start_time": "13:40", "talks": [], "chair": "Allen Schmaltz"}], [{"title": "Coffee Break", "start_time_iso": "2017-09-10T15:20:00", "end_time": "15:50", "end_time_iso": "2017-09-10T15:50:00", "id": "42", "start_time": "15:20", "type": "break"}], [{"title": "Machine Translation 2", "start_time_iso": "2017-09-10T15:50:00", "end_time_iso": "2017-09-10T17:30:00", "type": "paper", "posters": [], "chair_affiliation": "University of Melbourne", "code": "6A", "room": "Jutland", "end_time": "17:30", "id": "43", "start_time": "15:50", "talks": [{"title": "Earth Mover's Distance Minimization for Unsupervised Bilingual Lexicon Induction", "venue": "ACL", "end_time": "16:15", "abstract": "Cross-lingual natural language processing hinges on the premise that there\nexists invariance across languages. At the word level, researchers have\nidentified such invariance in the word embedding semantic spaces of different\nlanguages. However, in order to connect the separate spaces, cross-lingual\nsupervision encoded in parallel data is typically required. In this paper, we\nattempt to establish the cross-lingual connection without relying on any\ncross-lingual supervision. By viewing word embedding spaces as distributions,\nwe propose to minimize their earth mover's distance, a measure of divergence\nbetween distributions. We demonstrate the success on the unsupervised bilingual\nlexicon induction task. In addition, we reveal an interesting finding that the\nearth mover's distance shows potential as a measure of language difference.", "id": "543", "start_time": "15:50", "authors": "Meng Zhang, Yang Liu, Huanbo Luan and Maosong Sun"}, {"title": "Unfolding and Shrinking Neural Machine Translation Ensembles", "venue": "ACL", "end_time": "16:40", "abstract": "Ensembling is a well-known technique in neural machine translation (NMT) to\nimprove system performance. Instead of a single neural net, multiple neural\nnets with the same topology are trained separately, and the decoder generates\npredictions by averaging over the individual models. Ensembling often improves\nthe quality of the generated translations drastically. However, it is not\nsuitable for production systems because it is cumbersome and slow. This work\naims to reduce the runtime to be on par with a single system without\ncompromising the translation quality. First, we show that the ensemble can be\nunfolded into a single large neural network which imitates the output of the\nensemble system. We show that unfolding can already improve the runtime in\npractice since more work can be done on the GPU. We proceed by describing a set\nof techniques to shrink the unfolded network by reducing the dimensionality of\nlayers. On Japanese-English we report that the resulting network has the size\nand decoding speed of a single NMT network but performs on the level of a\n3-ensemble system.", "id": "191", "start_time": "16:15", "authors": "Felix Stahlberg and Bill Byrne"}, {"title": "Graph Convolutional Encoders for Syntax-aware Neural Machine Translation", "venue": "ACL", "end_time": "17:05", "abstract": "We present a simple and effective approach to incorporating syntactic structure\ninto neural attention-based encoder-decoder models for machine translation. We\nrely on graph-convolutional networks (GCNs), a recent class of neural networks\ndeveloped for modeling graph-structured data. Our GCNs use predicted syntactic\ndependency trees of source sentences to produce representations of words (i.e.\nhidden states of the encoder) that are sensitive to their syntactic\nneighborhoods. GCNs take word representations as input and produce word\nrepresentations as output, so they can easily be incorporated as layers into\nstandard encoders (e.g., on top of bidirectional RNNs or convolutional neural\nnetworks). We evaluate their effectiveness with English-German and\nEnglish-Czech translation experiments for different types of encoders and\nobserve substantial improvements over their syntax-agnostic versions in all the\nconsidered setups.", "id": "971", "start_time": "16:40", "authors": "Joost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani and Khalil Simaan"}, {"title": "Trainable Greedy Decoding for Neural Machine Translation", "venue": "ACL", "end_time": "17:30", "abstract": "Recent research in neural machine translation has largely focused on two\naspects; neural network architectures and end-to-end learning algorithms. The\nproblem of decoding, however, has received relatively little attention from the\nresearch community. In this paper, we solely focus on the problem of decoding\ngiven a trained neural machine translation model. Instead of trying to build a\nnew decoding algorithm for any specific decoding objective, we propose the idea\nof trainable decoding algorithm in which we train a decoding algorithm to find\na translation that maximizes an arbitrary decoding objective. More\nspecifically, we design an actor that observes and manipulates the hidden state\nof the neural machine translation decoder and propose to train it using a\nvariant of deterministic policy gradient. We extensively evaluate the proposed\nalgorithm using four language pairs and two decoding objectives and show that\nwe can indeed train a trainable greedy decoder that generates a better\ntranslation (in terms of a target decoding objective) with minimal\ncomputational overhead.", "id": "1297", "start_time": "17:05", "authors": "Jiatao Gu, Kyunghyun Cho and Victor O.K. Li"}], "chair": "Timothy Baldwin"}, {"title": "Text Mining and NLP applications", "start_time_iso": "2017-09-10T15:50:00", "end_time_iso": "2017-09-10T17:30:00", "type": "paper", "posters": [], "chair_affiliation": "ETS", "code": "6B", "room": "Funen", "end_time": "17:30", "id": "44", "start_time": "15:50", "talks": [{"title": "Satirical News Detection and Analysis using Attention Mechanism and Linguistic Features", "venue": "ACL", "end_time": "16:15", "abstract": "Satirical news is considered to be entertainment, but it is potentially\ndeceptive and harmful. Despite the embedded genre in the article, not everyone\ncan recognize the satirical cues and therefore believe the news as true news.\nWe observe that satirical cues are often reflected in certain paragraphs rather\nthan the whole document. Existing works only consider document-level features\nto detect the satire, which could be limited. We consider paragraph-level\nlinguistic features to unveil the satire by incorporating neural network and\nattention mechanism. We investigate the difference between paragraph-level\nfeatures and document-level features, and analyze them on a large satirical\nnews dataset. The evaluation shows that the proposed model detects satirical\nnews effectively and reveals what features are important at which level.", "id": "455", "start_time": "15:50", "authors": "Fan Yang, Arjun Mukherjee and Eduard Dragut"}, {"title": "Fine Grained Citation Span for References in Wikipedia", "venue": "ACL", "end_time": "16:40", "abstract": "Verifiability is one of the core editing principles in Wikipedia, where editors\nare encouraged to provide  citations for the added content. For a Wikipedia\narticle determining what content is covered by a citation or the citation span\nis not trivial, an important aspect for automated citation finding for\nuncovered content, or fact assessments.\n\nWe address the problem of determining the citation span in Wikipedia articles.\nWe approach this problem by classifying which textual fragments in an article\nare covered or hold true given a citation. We propose a sequence classification\napproach where for a paragraph and a citation, we determine the citation span\nat a fine-grained\nlevel.\n\nWe provide a thorough experimental evaluation and compare our approach against\nbaselines adopted from the scientific domain, where we show improvement for all\nevaluation metrics.", "id": "299", "start_time": "16:15", "authors": "Besnik Fetahu, Katja Markert and Avishek Anand"}, {"title": "Joint Modeling of Topics, Citations, and Topical Authority in Academic Corpora", "venue": "TACL", "end_time": "17:05", "abstract": "Much of scientific progress stems from previously published findings, but\nsearching through the vast sea of scientific publications is difficult. We\noften rely on metrics of scholarly authority to find the prominent authors but\nthese authority indices do not differentiate authority based on research\ntopics. We present Latent Topical-Authority Indexing (LTAI) for jointly\nmodeling the topics, citations, and topical authority in a corpus of academic\npapers. Compared to previous models, LTAI differs in two main aspects. First,\nit explicitly models the generative process of the citations, rather than\ntreating the citations as given. Second, it models each author's influence on\ncitations of a paper based on the topics of the cited papers, as well as the\nciting papers. We fit LTAI to four academic corpora: CORA, Arxiv Physics, PNAS,\nand Citeseer. We compare the performance of LTAI against various baselines,\nstarting with the latent Dirichlet allocation, to the more advanced models\nincluding author-link topic model and dynamic author citation topic model. The\nresults show that LTAI achieves improved accuracy over other similar models\nwhen predicting words, citations and authors of publications.", "id": "1527", "start_time": "16:40", "authors": "Jooyeon Kim, Dongwoo Kim and Alice Oh"}, {"title": "Identifying Semantic Edit Intentions from Revisions in Wikipedia", "venue": "ACL", "end_time": "17:30", "abstract": "Most studies on human editing focus merely on syntactic revision operations,\nfailing to capture the intentions behind revision changes, which are essential\nfor facilitating the single and collaborative writing process. \nIn this work, we develop in collaboration with Wikipedia editors a 13-category\ntaxonomy of the semantic intention behind edits in Wikipedia articles. Using\nlabeled article edits, we build a computational classifier of intentions that\nachieved a micro-averaged F1 score of 0.621. We use this model to investigate\nedit intention effectiveness: how different types of edits predict the\nretention of newcomers and changes in the quality of articles, two key concerns\nfor Wikipedia today. Our analysis shows that the types of edits that users make\nin their first session predict their subsequent survival as Wikipedia editors,\nand articles in different stages need different types of edits.", "id": "472", "start_time": "17:05", "authors": "Diyi Yang, Aaron Halfaker, Robert Kraut and Eduard Hovy"}], "chair": "Jill Burstein"}, {"title": "Machine Comprehension", "start_time_iso": "2017-09-10T15:50:00", "end_time_iso": "2017-09-10T17:30:00", "type": "paper", "posters": [], "chair_affiliation": "University of California, San Diego", "code": "6C", "room": "Zealand", "end_time": "17:30", "id": "45", "start_time": "15:50", "talks": [{"title": "Accurate Supervised and Semi-Supervised Machine Reading for Long Documents", "venue": "ACL", "end_time": "16:15", "abstract": "We introduce a hierarchical architecture for machine reading capable of\nextracting precise information from long documents.\nThe model divides the document into small, overlapping windows and encodes all\nwindows in parallel with an RNN.\nIt then attends over these window encodings, reducing them to a single\nencoding, which is decoded into an answer using a sequence decoder.\nThis hierarchical approach allows the model to scale to longer documents\nwithout increasing the number of sequential steps.\nIn a supervised setting, our model achieves state of the art accuracy of 76.8\non the WikiReading dataset.\nWe also evaluate the model in a semi-supervised setting by downsampling the\nWikiReading training set to create increasingly smaller amounts of supervision,\nwhile leaving the full unlabeled document corpus to train a sequence\nautoencoder on document windows.\nWe evaluate models that can reuse autoencoder states and outputs without\nfine-tuning their weights, allowing for more efficient training and inference.", "id": "467", "start_time": "15:50", "authors": "Daniel Hewlett, Llion Jones, Alexandre Lacoste and izzeddin gur"}, {"title": "Adversarial Examples for Evaluating Reading Comprehension Systems", "venue": "ACL", "end_time": "16:40", "abstract": "Standard accuracy metrics indicate that \nreading comprehension systems are making rapid progress,\nbut the extent to which these systems truly understand language remains\nunclear.\nTo reward systems with real language understanding abilities,\nwe propose an adversarial evaluation scheme for the Stanford\nQuestion Answering Dataset (SQuAD). \nOur method tests whether systems can answer questions\nabout paragraphs that contain adversarially inserted sentences,\nwhich are automatically generated to distract computer systems\nwithout changing the correct answer or misleading humans.\nIn this adversarial setting,\nthe accuracy of sixteen published models\ndrops from an average of $75\\%$ F1 score to $36\\%$;\nwhen the adversary is allowed to add ungrammatical sequences of words,\naverage accuracy on four models decreases further to $7\\%$.\nWe hope our insights will motivate\nthe development of new models that\nunderstand language more precisely.", "id": "1288", "start_time": "16:15", "authors": "Robin Jia and Percy Liang"}, {"title": "Reasoning with Heterogeneous Knowledge for Commonsense Machine Comprehension", "venue": "ACL", "end_time": "17:05", "abstract": "Reasoning with commonsense knowledge is critical for natural language\nunderstanding. Traditional methods for commonsense machine comprehension mostly\nonly focus on one specific kind of knowledge, neglecting the fact that\ncommonsense reasoning requires simultaneously considering different kinds of\ncommonsense knowledge. In this paper, we propose a multi-knowledge reasoning\nmethod, which can exploit heterogeneous knowledge for commonsense machine\ncomprehension. Specifically, we first mine different kinds of knowledge\n(including event narrative knowledge, entity semantic knowledge and sentiment\ncoherent knowledge) and encode them as inference rules with costs. Then we\npropose a multi-knowledge reasoning model, which selects inference rules for a\nspecific reasoning context using attention mechanism, and reasons by\nsummarizing all valid inference rules. Experiments on RocStories show that our\nmethod outperforms traditional models significantly.", "id": "635", "start_time": "16:40", "authors": "Hongyu Lin, Le Sun and Xianpei Han"}, {"title": "Document-Level Multi-Aspect Sentiment Classification as Machine Comprehension", "venue": "ACL", "end_time": "17:30", "abstract": "Document-level multi-aspect sentiment classification is an important task for\ncustomer relation management. In this paper, we model the task as a machine\ncomprehension problem where pseudo question-answer pairs are constructed by a\nsmall number of aspect-related keywords and aspect ratings. A hierarchical\niterative attention model is introduced to build aspectspecific representations\nby frequent and repeated interactions between documents and aspect questions.\nWe adopt a hierarchical architecture to represent both word level and sentence\nlevel information, and use the attention operations for aspect questions and\ndocuments alternatively with the multiple hop mechanism. Experimental results\non the TripAdvisor and BeerAdvocate datasets show that our model outperforms\nclassical baselines. We will release our code and data for the method\nreplicability.", "id": "177", "start_time": "17:05", "authors": "Yichun Yin, Yangqiu Song and Ming Zhang"}], "chair": "Ndapa Nakashole"}, {"title": "Poster Session. Summarization, Generation, Dialog, and Discourse 1", "start_time_iso": "2017-09-10T15:50:00", "end_time_iso": "2017-09-10T17:30:00", "type": "poster", "posters": [{"title": "What is the Essence of a Claim? Cross-Domain Claim Identification", "id": "380", "venue": "ACL", "authors": "Johannes Daxenberger, Steffen Eger, Ivan Habernal, Christian Stab and Iryna Gurevych", "abstract": "Argument mining has become a popular research area in NLP. It typically\nincludes the identification of argumentative components, e.g. claims, as the\ncentral component of an argument. We perform a qualitative analysis across six\ndifferent datasets and show that these appear to conceptualize claims quite\ndifferently. To learn about the consequences of such different\nconceptualizations of claim for practical applications, we carried out\nextensive experiments using state-of-the-art feature-rich and deep learning\nsystems, to identify claims in a cross-domain fashion. While the divergent\nconceptualization of claims in different datasets is indeed harmful to\ncross-domain classification, we show that there are shared properties on the\nlexical level as well as system configurations that can help to overcome these\ngaps."}, {"title": "Identifying Where to Focus in Reading Comprehension for Neural Question Generation", "id": "206", "venue": "ACL", "authors": "Xinya Du and Claire Cardie", "abstract": "A first step in the task of automatically generating questions for testing\nreading comprehension is to identify \\emph{question-worthy} sentences, i.e.\nsentences in a text passage that humans find it worthwhile to ask questions\nabout. We propose a hierarchical neural sentence-level sequence tagging model\nfor this task, which existing approaches to question generation have ignored.\nThe approach is fully data-driven \u2014 with no sophisticated NLP pipelines or\nany hand-crafted rules/features \u2014 and compares favorably to a number of\nbaselines when evaluated on the SQuAD data set. When incorporated into an\nexisting neural question generation system, the resulting end-to-end system\nachieves state-of-the-art performance for paragraph-level question generation\nfor reading comprehension."}, {"title": "Break it Down for Me: A Study in Automated Lyric Annotation", "id": "730", "venue": "ACL", "authors": "Lucas Sterckx, Jason Naradowsky, Bill Byrne, Thomas Demeester and Chris Develder", "abstract": "Comprehending lyrics, as found in songs and poems, can pose a challenge to\nhuman and machine readers alike.  This motivates the need for systems that can\nunderstand the ambiguity and jargon found in such creative texts, and provide\ncommentary to aid readers in reaching the correct interpretation.\nWe introduce the task of automated lyric annotation (ALA).  Like text\nsimplification, a goal of ALA is to rephrase the original text in a more easily\nunderstandable manner. However, in ALA the system must often include additional\ninformation to clarify niche terminology and abstract concepts. To stimulate\nresearch on this task, we release a large collection of crowdsourced\nannotations for song lyrics. We analyze the performance of translation and\nretrieval models on this task, measuring performance with both automated and\nhuman evaluation. We find that each model captures a unique type of information\nimportant to the task."}, {"title": "Cascaded Attention based Unsupervised Information Distillation for Compressive Summarization", "id": "246", "venue": "ACL", "authors": "Piji Li, Wai Lam, Lidong Bing, Weiwei Guo and Hang Li", "abstract": "When people recall and digest what they have read for writing summaries, the\nimportant content is more likely to attract their attention.\n  Inspired by this observation, we propose a cascaded attention based\nunsupervised\n  model to estimate the salience information from the text for compressive\nmulti-document summarization.\n  The attention weights are learned automatically by an unsupervised data\nreconstruction framework which can capture the sentence salience.\n  By adding sparsity constraints on the number of output vectors, we can\ngenerate condensed information which can be treated as word salience.\n  Fine-grained and coarse-grained sentence compression strategies are\nincorporated to produce compressive summaries. \n  Experiments on some benchmark data sets show that our framework achieves\nbetter results than the state-of-the-art methods."}, {"title": "Deep Recurrent Generative Decoder for Abstractive Text Summarization", "id": "550", "venue": "ACL", "authors": "Piji Li, Wai Lam, Lidong Bing and Zihao Wang", "abstract": "We propose a new framework for abstractive text summarization based on a\nsequence-to-sequence oriented encoder-decoder model equipped with a deep\nrecurrent generative decoder (DRGN).\n  Latent structure information implied in the target summaries is learned based\non a recurrent latent random model for improving the summarization quality.\n  Neural variational inference is employed to address the intractable posterior\ninference for the recurrent latent variables.\n  Abstractive summaries are generated based on both the generative latent\nvariables and the discriminative deterministic states.\n  Extensive experiments on some benchmark datasets in different languages show\nthat DRGN achieves improvements over the state-of-the-art methods."}, {"title": "Extractive Summarization Using Multi-Task Learning with Document Classification", "id": "1132", "venue": "ACL", "authors": "Masaru Isonuma, Toru Fujino, Junichiro Mori, Yutaka Matsuo and Ichiro Sakata", "abstract": "The need for automatic document summarization that can be used for practical\napplications is increasing rapidly. In this paper, we propose a general\nframework for summarization that extracts sentences from a document using\nexternally related information. Our work is aimed at single document\nsummarization using small amounts of reference summaries. In particular, we\naddress document summarization in the framework of multi-task learning using\ncurriculum learning for sentence extraction and document classification. The\nproposed framework enables us to obtain better feature representations to\nextract sentences from documents. We evaluate our proposed summarization method\non two datasets: financial report and news corpus. Experimental results\ndemonstrate that our summarizers achieve performance that is comparable to\nstate-of-the-art systems."}, {"title": "Towards Automatic Construction of News Overview Articles by News Synthesis", "id": "147", "venue": "ACL", "authors": "Jianmin Zhang and Xiaojun Wan", "abstract": "In this paper we investigate a new task of automatically constructing an\noverview article from a given set of news articles about a news event.\nWe propose a news synthesis approach to address this task based on passage\nsegmentation, ranking, selection and merging.\nOur proposed approach is compared with several typical multi-document\nsummarization methods on the Wikinews dataset, and achieves the best\nperformance on both automatic evaluation and manual evaluation."}, {"title": "Joint Syntacto-Discourse Parsing and the Syntacto-Discourse Treebank", "id": "1495", "venue": "ACL", "authors": "Kai Zhao and Liang Huang", "abstract": "Discourse parsing has long been treated as a stand-alone problem independent\nfrom constituency or dependency parsing. Most attempts at this problem rely on\nannotated text segmentations (Elementary Discourse Units, EDUs) and\nsophisticated sparse or continuous features to extract syntactic information.\nIn this paper we propose the first end-to-end discourse parser that jointly\nparses in both syntax and discourse levels, as well as the first\nsyntacto-discourse treebank by integrating the Penn Treebank and the RST\nTreebank. Built upon our recent span-based constituency parser, this joint\nsyntacto-discourse parser requires no preprocessing efforts such as\nsegmentation or feature extraction, making discourse parsing more convenient.\nEmpirically, our parser achieves the state-of-the-art end-to-end discourse\nparsing accuracy."}, {"title": "Event Coreference Resolution by Iteratively Unfolding Inter-dependencies among Events", "id": "970", "venue": "ACL", "authors": "Prafulla Kumar Choubey and Ruihong Huang", "abstract": "We introduce a novel iterative approach for event coreference resolution that\ngradually builds event clusters by exploiting inter-dependencies among event\nmentions within the same chain as well as across event chains. Among event\nmentions in the same chain, we distinguish within- and cross-document event\ncoreference links by using two distinct pairwise classifiers, trained\nseparately to capture differences in feature distributions of within- and\ncross-document event clusters. Our event coreference approach alternates\nbetween WD and CD clustering and combines arguments from both event clusters\nafter every merge, continuing till no more merge can be made. And then it\nperforms further merging between event chains that are both closely related to\na set of other chains of events. Experiments on the ECB+ corpus show that our\nmodel outperforms state-of-the-art methods in joint task of WD and CD event\ncoreference resolution."}, {"title": "When to Finish? Optimal Beam Search for Neural Text Generation (modulo beam size)", "id": "1468", "venue": "ACL", "authors": "Liang Huang, Kai Zhao and Mingbo Ma", "abstract": "In neural text generation such as neural\nmachine translation, summarization, and\nimage captioning, beam search is widely\nused to improve the output text quality.\nHowever, in the neural generation set-\nting, hypotheses can finish in different\nsteps, which makes it difficult to decide\nwhen to end beam search to ensure op-\ntimality. We propose a provably optimal\nbeam search algorithm that will always re-\nturn the optimal-score complete hypothe-\nsis (modulo beam size), and finish as soon\nas the optimality is established. To counter\nneural generation\u2019s tendency for shorter\nhypotheses, we also introduce a bounded\nlength reward mechanism which allows a\nmodified version of our beam search al-\ngorithm to remain optimal. Experiments\non neural machine translation demonstrate\nthat our principled beam search algorithm\nleads to improvement in BLEU score over\npreviously proposed alternatives."}, {"title": "Steering Output Style and Topic in Neural Response Generation", "id": "1380", "venue": "ACL", "authors": "Di Wang, Nebojsa Jojic, Chris Brockett and Eric Nyberg", "abstract": "We propose simple and flexible training and decoding methods for influencing\noutput style and topic in neural encoder-decoder based language generation.\nThis capability is desirable in a variety of applications, including\nconversational systems, where successful agents need to produce language in a\nspecific style and generate responses steered by a human puppeteer or external\nknowledge. We decompose the neural generation process into empirically easier\nsub-problems: a faithfulness model and a decoding method based on\nselective-sampling. We also describe training and sampling algorithms that bias\nthe generation process with a specific language style restriction, or a topic\nrestriction. Human evaluation results show that our proposed methods are able\nto to restrict style and topic without degrading output quality in\nconversational tasks."}], "chair_affiliation": "University of Washington", "code": "6D", "room": "Aarhus", "end_time": "17:30", "id": "46", "start_time": "15:50", "talks": [], "chair": "Yangfeng Ji"}, {"title": "Poster Session. Summarization, Generation, Dialog, and Discourse 2", "start_time_iso": "2017-09-10T15:50:00", "end_time_iso": "2017-09-10T17:30:00", "type": "poster", "posters": [{"title": "Preserving Distributional Information in Dialogue Act Classification", "id": "1397", "venue": "ACL", "authors": "Quan Hung Tran, Ingrid Zukerman and Gholamreza Haffari", "abstract": "This paper introduces a novel training/decoding strategy for sequence labeling.\nInstead of greedily choosing a label at each time step, and using it for the\nnext prediction, we retain the probability distribution over the current label,\nand pass this distribution to the next prediction. This approach allows us to\navoid the effect of label bias and error propagation in sequence\nlearning/decoding. Our experiments on dialogue act classification demonstrate\nthe effectiveness of this approach. Even though our underlying neural network\nmodel is relatively simple, it outperforms more complex neural models,\nachieving state-of-the-art results on the MapTask and Switchboard corpora."}, {"title": "Adversarial Learning for Neural Dialogue Generation", "id": "11", "venue": "ACL", "authors": "Jiwei Li, Will Monroe, Tianlin Shi, S\\'ebastien Jean, Alan Ritter and Dan Jurafsky", "abstract": "We apply adversarial training to open-domain dialogue generation,\ntraining a system to produce sequences that are\n indistinguishable from human-generated dialogue utterances. \nWe cast the task as a reinforcement learning problem where we jointly train two\nsystems: a generative model to produce response sequences, and a\ndiscriminator---analagous to the human evaluator in the Turing test--- to\ndistinguish between \n the \n human-generated dialogues and the machine-generated ones. \nIn this generative adversarial network approach,\nthe outputs from the discriminator are \nused to encourage the system towards more human-like dialogue.\n\nFurther, we investigate models\nfor adversarial  evaluation that \nuses success in fooling an adversary as a dialogue evaluation metric,\nwhile avoiding a number of potential pitfalls.\n\nExperimental results on several\nmetrics, including adversarial evaluation, demonstrate\nthat the adversarially-trained system generates higher-quality responses\nthan previous baselines"}, {"title": "Using Context Information for Dialog Act Classification in DNN Framework", "id": "281", "venue": "ACL", "authors": "Yang Liu, Kun Han, Zhao Tan and Yun Lei", "abstract": "Previous work on dialog act (DA) classification has investigated different\nmethods, such as hidden Markov models, maximum entropy, conditional random\nfields, graphical models, and support vector machines.\nA few recent studies explored using deep learning neural networks for DA\nclassification, however, it is not clear yet what is the best method for using\ndialog context or DA sequential information, and how much gain it brings. This\npaper proposes several ways of using context information for DA classification,\nall in the deep learning framework. The baseline system classifies each\nutterance using the convolutional neural networks (CNN). Our proposed methods\ninclude using hierarchical models (recurrent neural networks (RNN) or CNN) for\nDA sequence tagging where the bottom layer takes the sentence CNN\nrepresentation as input, concatenating predictions from the previous utterances\nwith the CNN vector for classification, and performing sequence decoding based\non the predictions from the sentence CNN model. \nWe conduct thorough experiments and comparisons on the Switchboard corpus,\ndemonstrate that incorporating context information significantly improves DA\nclassification, and show that we achieve new state-of-the-art performance for\nthis task."}, {"title": "Modeling Dialogue Acts with Content Word Filtering and Speaker Preferences", "id": "519", "venue": "ACL", "authors": "Yohan Jo, Michael Yoder, Hyeju Jang and Carolyn Rose", "abstract": "We present an unsupervised model of dialogue act sequences in conversation. By\nmodeling topical themes as transitioning more slowly than dialogue acts in\nconversation, our model de-emphasizes content-related words in order to focus\non conversational function words that signal dialogue acts. We also incorporate\nspeaker tendencies to use some acts more than others as an additional predictor\nof dialogue act prevalence beyond temporal dependencies. According to the\nevaluation presented on two dissimilar corpora, the CNET forum and NPS Chat\ncorpus, the effectiveness of each modeling assumption is found to vary\ndepending on characteristics of the data. De-emphasizing content-related words\nyields improvement on the CNET corpus, while utilizing speaker tendencies is\nadvantageous on the NPS corpus. The components of our model complement one\nanother to achieve robust performance on both corpora and outperform\nstate-of-the-art baseline models."}, {"title": "Towards Implicit Content-Introducing for Generative Short-Text Conversation Systems", "id": "689", "venue": "ACL", "authors": "Lili Yao, Yaoyuan Zhang, Yansong Feng, Dongyan Zhao and Rui Yan", "abstract": "The study on human-computer conversation systems is a hot research topic\nnowadays. One of the prevailing methods to build the system is using the\ngenerative Sequence-to-Sequence (Seq2Seq) model through neural networks.\nHowever, the standard Seq2Seq model is prone to generate trivial responses. In\nthis paper, we aim to generate a more meaningful and informative reply when\nanswering a given question. We propose an implicit content-introducing method\nwhich incorporates additional information into the Seq2Seq model in a flexible\nway. Specifically, we fuse the general decoding and the auxiliary cue word\ninformation through our proposed hierarchical gated fusion unit. Experiments on\nreal-life data demonstrate that our model consistently outperforms a set of\ncompetitive baselines in terms of BLEU scores and human evaluation."}, {"title": "Affordable On-line Dialogue Policy Learning", "id": "697", "venue": "ACL", "authors": "Cheng Chang, Runzhe Yang, Lu Chen, Xiang Zhou and Kai Yu", "abstract": "The key to building an evolvable dialogue system in real-world scenarios is to\nensure an affordable on-line dialogue policy learning, which requires the\non-line learning process to be safe, efficient and economical. But in reality,\ndue to the scarcity of real interaction data, the dialogue system usually grows\nslowly. Besides, the poor initial dialogue policy easily leads to bad user\nexperience and incurs a failure of attracting users to contribute training\ndata, so that the learning process is unsustainable. To accurately depict this,\n two quantitative metrics are proposed to assess safety and efficiency issues.\nFor solving the unsustainable learning problem, we proposed a complete\ncompanion teaching framework incorporating the guidance from the human teacher.\nSince the human teaching is expensive, we compared various teaching schemes\nanswering the question how and when to teach, to economically utilize teaching\nbudget, so that make the online learning process affordable."}, {"title": "Generating High-Quality and Informative Conversation Responses with Sequence-to-Sequence Models", "id": "844", "venue": "ACL", "authors": "Yuanlong Shao, Stephan Gouws, Denny Britz, Anna Goldie, Brian Strope and Ray Kurzweil", "abstract": "Sequence-to-sequence models have been applied to the conversation response\ngeneration problem where the source sequence is the conversation history and\nthe target sequence is the response. Unlike translation, conversation\nresponding is inherently creative. The generation of long, informative,\ncoherent, and diverse responses remains a hard task.\nIn this work, we focus on the single turn setting. We add self-attention to the\ndecoder to maintain coherence in longer responses, and we propose a practical\napproach, called the glimpse-model, for scaling to large datasets. We introduce\na stochastic beam-search algorithm with segment-by-segment reranking which lets\nus inject diversity earlier in the generation process. We trained on a combined\ndata set of over 2.3B conversation messages mined from the web. In human\nevaluation studies, our method produces longer responses overall, with a higher\nproportion rated as acceptable and excellent as length increases, compared to\nbaseline sequence-to-sequence models with explicit length-promotion. A back-off\nstrategy produces better responses overall, in the full spectrum of lengths."}, {"title": "Bootstrapping incremental dialogue systems from minimal data: the generalisation power of dialogue grammars", "id": "956", "venue": "ACL", "authors": "Arash Eshghi, Igor Shalyminov and Oliver Lemon", "abstract": "We investigate an end-to-end method for automatically inducing task-based\ndialogue systems from small amounts  of unannotated dialogue data. It combines\nan incremental semantic grammar  - Dynamic Syntax and Type Theory with Records\n(DS-TTR) - with Reinforcement Learning (RL), where language generation and\ndialogue management are a joint  decision problem. The systems thus produced\nare incremental: dialogues are processed word-by-word, shown previously to be\nessential in supporting natural, spontaneous dialogue. We hypothesised that the\nrich linguistic knowledge within the grammar should enable a combinatorially\nlarge number of dialogue variations to be processed, even when trained on very\nfew dialogues. Our experiments show that our model can process 74% of the\nFacebook AI bAbI dataset even when trained on only 0.13% of the data (5\ndialogues).  It can in addition process 65% of bAbI+, a corpus we created by\nsystematically adding incremental dialogue phenomena such as restarts and\nself-corrections to bAbI. We compare our model with a state-of-the-art\nretrieval model, MEMN2N. We find that, in terms of semantic accuracy, the\nMEMN2N model shows very poor robustness to the bAbI+ transformations even when\ntrained on the full bAbI dataset."}, {"title": "Composite Task-Completion Dialogue Policy Learning via Hierarchical Deep Reinforcement Learning", "id": "1176", "venue": "ACL", "authors": "Baolin Peng, Xiujun Li, Lihong Li, Jianfeng Gao, Asli Celikyilmaz, Sungjin Lee and Kam-Fai Wong", "abstract": "Building a dialogue agent to fulfill complex tasks, such as travel planning, is\nchallenging because the agent has to learn to collectively complete multiple\nsubtasks. For example, the agent needs to reserve a hotel and book a flight so\nthat there leaves enough time for commute between arrival and hotel check-in.\nThis paper addresses this challenge by formulating the task in the mathematical\nframework of options over Markov Decision Processes (MDPs), and proposing a\nhierarchical deep reinforcement learning approach to learning a dialogue\nmanager that operates at different temporal scales. The dialogue manager\nconsists of: (1) a top-level dialogue policy that selects among subtasks or\noptions, (2) a low-level dialogue policy that selects primitive actions to\ncomplete the subtask given by the top-level policy, and (3) a global state\ntracker that helps ensure all cross-subtask constraints be satisfied.\nExperiments on a travel planning task with simulated and real users show that\nour approach leads to significant improvements over three baselines, two based\non handcrafted rules and the other based on flat deep reinforcement learning."}, {"title": "Why We Need New Evaluation Metrics for NLG", "id": "387", "venue": "ACL", "authors": "Jekaterina Novikova, Ond\u0159ej Du\u0161ek, Amanda Cercas Curry and Verena Rieser", "abstract": "The majority of NLG evaluation relies on automatic metrics, such as BLEU . In\nthis paper, we motivate the need for novel, system- and data-independent\nautomatic evaluation methods: We investigate a wide range of metrics, including\nstate-of-the-art word-based and novel grammar-based ones, and demonstrate that\nthey only weakly reflect human judgements of system outputs as generated by\ndata-driven, end-to-end NLG. We also show that metric performance is data- and\nsystem-specific. Nevertheless, our results also suggest that automatic metrics\nperform reliably at system-level and can support system development by finding\ncases where a system performs poorly."}, {"title": "Challenges in Data-to-Document Generation", "id": "1363", "venue": "ACL", "authors": "Sam Wiseman, Stuart Shieber and Alexander Rush", "abstract": "Recent neural models have shown significant progress on the problem of\ngenerating short descriptive texts conditioned on a small number of database\nrecords. In this work, we suggest a slightly more difficult data-to-text\ngeneration task, and investigate how effective current approaches are on this\ntask. In particular, we introduce a new, large-scale corpus of data records\npaired with descriptive documents, propose a series of extractive evaluation\nmethods for analyzing performance, and obtain baseline results using current\nneural generation methods. Experiments show that these models produce fluent\ntext, but fail to convincingly approximate human-generated documents. Moreover,\neven templated baselines exceed the performance of these neural models on some\nmetrics, though copy- and reconstruction-based extensions lead to noticeable\nimprovements."}], "chair_affiliation": "IT University of Copenhagen", "code": "6E", "room": "Odense", "end_time": "17:30", "id": "47", "start_time": "15:50", "talks": [], "chair": "Natalie Schluter"}, {"title": "Poster Session. Computational Social Science 2", "start_time_iso": "2017-09-10T15:50:00", "end_time_iso": "2017-09-10T17:30:00", "type": "poster", "posters": [{"title": "All that is English may be Hindi: Enhancing language identification through automatic ranking of the likeliness of word borrowing in social media", "id": "400", "venue": "ACL", "authors": "Jasabanta Patro, Bidisha Samanta, Saurabh Singh, Abhipsa Basu, Prithwish Mukherjee, Monojit Choudhury and Animesh Mukherjee", "abstract": "n this paper, we present a set of computational methods to identify the\nlikeliness of a word being borrowed, based on the signals from social media. In\nterms of Spearman\u2019s correlation values, our methods perform more than two\ntimes better (\u223c 0.62) in predicting the borrowing likeliness compared to the\nbest performing baseline (\u223c 0.26) reported in literature. Based on this\nlikeliness estimate we asked annotators to re-annotate the language tags of\nforeign words in predominantly native contexts. In 88% of cases the annotators\nfelt that the foreign language tag should be replaced by native language tag,\nthus indicating a huge scope for improvement of automatic language\nidentification systems."}, {"title": "Multi-View Unsupervised User Feature Embedding for Social Media-based Substance Use Prediction", "id": "714", "venue": "ACL", "authors": "Tao Ding, Warren K. Bickel and Shimei Pan", "abstract": "In this paper, we demonstrate how the state-of-the-art machine learning and\ntext mining techniques can be used to build effective social media-based\nsubstance use detection systems.  Since a substance use ground truth is\ndifficult to obtain on a large scale, to maximize system performance, we\nexplore different unsupervised feature learning methods to take advantage of a\nlarge amount of unsupervised social media data. We also demonstrate the benefit\nof using multi-view unsupervised feature learning to combine heterogeneous user\ninformation such as Facebook \"likes\" and  \"status updates\"  to enhance system\nperformance.  Based on our evaluation, our best models achieved 86% AUC for\npredicting tobacco use,  81% for alcohol use and 84% for illicit drug use, all\nof which significantly outperformed existing methods. Our investigation has\nalso uncovered interesting relations between a user's social media behavior\n(e.g., word usage) and substance use."}, {"title": "Demographic-aware word associations", "id": "792", "venue": "ACL", "authors": "Aparna Garimella, Carmen Banea and Rada Mihalcea", "abstract": "Variations of word associations across different groups of people can provide\ninsights into people\u2019s psychologies and their world views. To capture these\nvariations, we introduce the task of demographic-aware word associations. We\nbuild a new gold standard dataset consisting of word association responses for\napproximately 300 stimulus words, collected from more than 800 respondents of\ndifferent gender (male/female) and from different locations (India/United\nStates), and show that there are significant variations in the word\nassociations made by these groups. We also introduce a new demographic-aware\nword association model based on a neural net skip-gram architecture, and show\nhow computational methods for measuring word associations that specifically\naccount for writer demographics can outperform generic methods that are\nagnostic to such information."}, {"title": "A Factored Neural Network Model for Characterizing Online Discussions in Vector Space", "id": "1062", "venue": "ACL", "authors": "Hao Cheng, Hao Fang and Mari Ostendorf", "abstract": "We develop a novel factored neural model that learns comment embeddings in an\nunsupervised way leveraging the structure of distributional context in online\ndiscussion forums. The model links different context with related language\nfactors in the embedding space, providing a way to interpret the factored\nembeddings. Evaluated on a community endorsement prediction task using a large\ncollection of topic-varying Reddit discussions, the factored embeddings\nconsistently achieve improvement over other text representations. Qualitative\nanalysis shows that the model captures community style and topic, as well as\nresponse trigger patterns."}, {"title": "Dimensions of Interpersonal Relationships: Corpus and Experiments", "id": "1163", "venue": "ACL", "authors": "Farzana Rashid and Eduardo Blanco", "abstract": "This paper presents a corpus and experiments to determine dimensions of\ninterpersonal relationships. We define a set of dimensions heavily inspired by\nwork in social science. We create a corpus by retrieving pairs of people, and\nthen annotating dimensions for their relationships. A corpus analysis shows\nthat dimensions can be annotated reliably. Experimental results show that given\na pair of people, values to dimensions can be assigned automatically."}, {"title": "Argument Mining on Twitter: Arguments, Facts and Sources", "id": "934", "venue": "ACL", "authors": "Mihai Dusmanu, Elena Cabrio and Serena Villata", "abstract": "Social media collect and spread on the Web personal opinions, facts, fake news\nand all kind of information users may be interested in. Applying argument\nmining methods to such heterogeneous data sources is a challenging open\nresearch issue, in particular considering the peculiarities of the language\nused to write textual messages on social media. In addition, new issues emerge\nwhen dealing with arguments posted on such platforms, such as the need to make\na distinction between personal opinions and actual facts, and to detect the\nsource disseminating information about such facts to allow for provenance\nverification. In this paper, we apply supervised classification to identify\narguments on Twitter, and we present two new tasks for argument mining, namely\nfacts recognition and source identification. We study the feasibility of the\napproaches proposed to address these tasks on a set of tweets related to the\nGrexit and Brexit news topics."}, {"title": "Distinguishing Japanese Non-standard Usages from Standard Ones", "id": "1159", "venue": "ACL", "authors": "Tatsuya Aoki, Ryohei Sasano, Hiroya Takamura and Manabu Okumura", "abstract": "We focus on non-standard usages of common words on social media. In the context\nof social media, words sometimes have other usages that are totally different\nfrom their original. In this study, we attempt to distinguish non-standard\nusages on social media from standard ones in an unsupervised manner. Our basic\nidea is that non-standardness can be measured by the inconsistency between the\nexpected meaning of the target word and the given context. For this purpose, we\nuse context embeddings derived from word embeddings. Our experimental results\nshow that the model leveraging the context embedding outperforms other methods\nand provide us with findings, for example, on how to construct context\nembeddings and which corpus to use."}, {"title": "Connotation Frames of Power and Agency in Modern Films", "id": "1327", "venue": "ACL", "authors": "Maarten Sap, Marcella Cindy Prasettio, Ari Holtzman, Hannah Rashkin and Yejin Choi", "abstract": "The framing of an action influences how we perceive its actor. We introduce\nconnotation frames of power and agency, a pragmatic formalism organized using\nframe semantic representations, to model how different levels of power and\nagency are implicitly projected on actors through their actions. We use the new\npower and agency frames to measure the subtle, but prevalent, gender bias in\nthe portrayal of modern film characters and provide insights that deviate from\nthe well-known Bechdel test. Our contributions include an extended lexicon of\nconnotation frames along with a web interface that provides a comprehensive\nanalysis through the lens of connotation frames."}, {"title": "Controlling Human Perception of Basic User Traits", "id": "1415", "venue": "ACL", "authors": "Daniel Preo\u0163iuc-Pietro, Sharath Chandra Guntuku and Lyle Ungar", "abstract": "Much of our online communication is text-mediated and, lately, more common with\nautomated agents. Unlike interacting with humans, these agents currently do not\ntailor their language to the type of person they are communicating to. In this\npilot study, we measure the extent to which human perception of basic user\ntrait information -- gender and age -- is controllable through text. Using\nautomatic models of gender and age prediction, we estimate which tweets posted\nby a user are more likely to mis-characterize his traits. We perform multiple\ncontrolled crowdsourcing experiments in which we show that we can reduce the\nhuman prediction accuracy of gender to almost random -- an over 20\\% drop in\naccuracy. Our experiments show that it is practically feasible for multiple\napplications such as text generation, text summarization or machine translation\nto be tailored to specific traits and perceived as such."}, {"title": "Topic Signatures in Political Campaign Speeches", "id": "581", "venue": "ACL", "authors": "Cl\u00e9ment Gautrais, Peggy Cellier, Ren\u00e9 Quiniou and Alexandre Termier", "abstract": "Highlighting the recurrence of topics usage in candidates speeches is a key\nfeature to identify the main ideas of each candidate during a political\ncampaign. In this paper, we present a method combining standard topic modeling\nwith signature mining for analyzing topic recurrence in speeches of Clinton and\nTrump during the 2016 American presidential campaign. The results show that the\nmethod extracts automatically the main ideas of each candidate and, in\naddition, provides information about the evolution of these topics during the\ncampaign."}, {"title": "Assessing Objective Recommendation Quality through Political Forecasting", "id": "501", "venue": "ACL", "authors": "H. Andrew Schwartz, Masoud Rouhizadeh, Michael Bishop, Philip Tetlock, Barbara Mellers and Lyle Ungar", "abstract": "Recommendations are often rated for their subjective quality, but few\nresearchers have studied comment quality in terms of objective utility. We\nexplore\nrecommendation quality assessment with respect to both subjective (i.e.\nusers\u2019 ratings) and\nobjective (i.e., did it influence? did it improve decisions?) metrics in a\nmassive online geopolitical forecasting system, ultimately comparing linguistic\ncharacteristics of each quality metric. Using a variety of features, we predict\nall types of quality with better accuracy than the simple yet strong baseline\nof comment length. Looking at the most predictive content illustrates rater\nbiases; for example, forecasters are subjectively biased in favor of comments\nmentioning business transactions or dealings as well as material things, even\nthough such comments do not indeed prove any more useful objectively.\nAdditionally, more complex sentence constructions, as evidenced by subordinate\nconjunctions, are characteristic of comments leading to objective improvements\nin forecasting."}, {"title": "Never Abandon Minorities: Exhaustive Extraction of Bursty Phrases on Microblogs Using Set Cover Problem", "id": "536", "venue": "ACL", "authors": "Masumi Shirakawa, Takahiro Hara and Takuya Maekawa", "abstract": "We propose a language-independent data-driven method to exhaustively extract\nbursty phrases of arbitrary forms (e.g., phrases other than simple noun\nphrases) from microblogs. The burst (i.e., the rapid increase of the\noccurrence) of a phrase causes the burst of overlapping N-grams including\nincomplete ones. In other words, bursty incomplete N-grams inevitably overlap\nbursty phrases. Thus, the proposed method performs the extraction of bursty\nphrases as the set cover problem in which all bursty N-grams are covered by a\nminimum set of bursty phrases. Experimental results using Japanese Twitter data\nshowed that the proposed method outperformed word-based, noun phrase-based, and\nsegmentation-based methods both in terms of accuracy and coverage."}], "chair_affiliation": "University of Melbourne", "code": "6F", "room": "Copenhagen", "end_time": "17:30", "id": "48", "start_time": "15:50", "talks": [], "chair": "Afshin Rahimi"}], [{"title": "Social Event", "code": null, "id": "49", "start_time_iso": "2017-09-10T18:00:00", "end_time": "22:00", "end_time_iso": "2017-09-10T22:00:00", "type": "other", "room": "\u00d8ksnehallen Courtyard", "start_time": "18:00", "talks": [], "posters": []}]], "weekday": "Sunday"}, {"day": "Monday, September 11, 2017", "program": [[{"title": "Registration Day 3", "code": null, "id": "50", "start_time_iso": "2017-09-11T07:30:00", "end_time": "17:30", "end_time_iso": "2017-09-11T17:30:00", "type": "other", "room": null, "start_time": "07:30", "talks": [], "posters": []}], [{"title": "Morning Coffee", "start_time_iso": "2017-09-11T08:00:00", "end_time": "09:00", "end_time_iso": "2017-09-11T09:00:00", "id": "51", "start_time": "08:00", "type": "break"}], [{"title": "Plenary Session. Invited Talk by Dan Jurafsky", "code": null, "id": "52", "start_time_iso": "2017-09-11T09:00:00", "end_time": "10:00", "end_time_iso": "2017-09-11T10:00:00", "type": "invited_talk", "room": "Jutland", "start_time": "09:00", "talks": [], "posters": []}], [{"title": "Coffee Break", "start_time_iso": "2017-09-11T10:00:00", "end_time": "10:30", "end_time_iso": "2017-09-11T10:30:00", "id": "53", "start_time": "10:00", "type": "break"}], [{"title": "Machine Learning 3", "start_time_iso": "2017-09-11T10:30:00", "end_time_iso": "2017-09-11T12:10:00", "type": "paper", "posters": [], "chair_affiliation": "University of Groningen", "code": "7A", "room": "Jutland", "end_time": "12:10", "id": "54", "start_time": "10:30", "talks": [{"title": "Maximum Margin Reward Networks for Learning from Explicit and Implicit Supervision", "venue": "ACL", "end_time": "10:55", "abstract": "Neural networks have achieved state-of-the-art performance on several\nstructured-output prediction tasks, trained in a fully supervised\nfashion.  However, annotated examples in structured domains are often\ncostly to obtain, which thus limits the applications of neural\nnetworks.  In this work, we propose Maximum Margin Reward Networks, a\nneural network-based framework that aims to learn from both explicit\n(full structures) and implicit supervision signals (delayed feedback\non the correctness of the predicted structure).  On named entity\nrecognition and semantic parsing, our model outperforms previous\nsystems on the benchmark datasets, CoNLL-2003 and WebQuestionsSP.", "id": "1310", "start_time": "10:30", "authors": "Haoruo Peng, Ming-Wei Chang and Wen-tau Yih"}, {"title": "The Impact of Modeling Overall Argumentation with Tree Kernels", "venue": "ACL", "end_time": "11:20", "abstract": "Several approaches have been proposed to model either the explicit sequential\nstructure of an argumentative text or its implicit hierarchical structure. So\nfar, the adequacy of these models of overall argumentation remains unclear.\nThis paper asks what type of structure is actually important to tackle\ndownstream tasks in computational argumentation. We analyze patterns in the\noverall argumentation of texts from three corpora. Then, we adapt the idea of\npositional tree kernels in order to capture sequential and hierarchical\nargumentative structure together for the first time. In systematic experiments\nfor three text classification tasks, we find strong evidence for the impact of\nboth types of structure. Our results suggest that either of them is necessary\nwhile their combination may be beneficial.", "id": "349", "start_time": "10:55", "authors": "Henning Wachsmuth, Giovanni Da San Martino, Dora Kiesel and Benno Stein"}, {"title": "Learning Generic Sentence Representations Using Convolutional Neural Networks", "venue": "ACL", "end_time": "11:45", "abstract": "We propose a new encoder-decoder approach to learn distributed sentence\nrepresentations that are applicable to multiple purposes. The model is learned\nby using a convolutional neural network as an encoder to map an input sentence\ninto a continuous vector, and using a long short-term memory recurrent neural\nnetwork as a decoder. Several tasks are considered, including sentence\nreconstruction and future sentence prediction. Further, a hierarchical\nencoder-decoder model is proposed to encode a sentence to predict multiple\nfuture sentences. By training our models on a large collection of novels, we\nobtain a highly generic convolutional sentence encoder that performs well in\npractice. Experimental results on several benchmark datasets, and across a\nbroad range of applications, demonstrate the superiority of the proposed model\nover competing methods.", "id": "977", "start_time": "11:20", "authors": "Zhe Gan, Yunchen Pu, Ricardo Henao, Chunyuan Li, Xiaodong He and Lawrence Carin"}, {"title": "Repeat before Forgetting: Spaced Repetition for Efficient and Effective Training of Neural Networks", "venue": "ACL", "end_time": "12:10", "abstract": "We present a novel approach for training artificial neural networks. Our\napproach is inspired by broad evidence in psychology that shows human learners\ncan learn efficiently and effectively by increasing intervals of time between\nsubsequent reviews of previously learned materials (spaced repetition). We\ninvestigate the analogy between training neural models and findings in\npsychology about human memory model and develop an efficient and effective\nalgorithm to train neural models. The core part of our algorithm is a\ncognitively-motivated scheduler according to which training instances and their\n\"reviews\" are spaced over time. Our algorithm uses only 34-50% of data per\nepoch, is 2.9-4.8 times faster than standard training, and outperforms\ncompeting state-of-the-art baselines. Our code is available at\nscholar.harvard.edu/hadi/RbF/.", "id": "916", "start_time": "11:45", "authors": "Hadi Amiri, Timothy Miller and Guergana Savova"}], "chair": "Barbara Plank"}, {"title": "Syntax 4", "start_time_iso": "2017-09-11T10:30:00", "end_time_iso": "2017-09-11T12:10:00", "type": "paper", "posters": [], "chair_affiliation": "IT University of Copenhagen", "code": "7B", "room": "Funen", "end_time": "12:10", "id": "55", "start_time": "10:30", "talks": [{"title": "Part-of-Speech Tagging for Twitter with Adversarial Neural Networks", "venue": "ACL", "end_time": "10:55", "abstract": "In this work, we study the problem of part-of-speech tagging for Tweets. In\ncontrast to newswire articles, Tweets are usually informal and contain numerous\nout-of-vocabulary words. Moreover, there is a lack of large scale labeled\ndatasets for this domain. To tackle these challenges, we propose a novel neural\nnetwork to make use of out-of-domain labeled data, unlabeled in-domain data,\nand labeled in-domain data.  Inspired by adversarial neural networks, the\nproposed method tries to learn common features through adversarial\ndiscriminator. In addition, we hypothesize that domain-specific features of\ntarget domain should be preserved in some degree. Hence, the proposed method\nadopts a sequence-to-sequence autoencoder to perform this task.  Experimental\nresults on three different datasets  show that our method achieves better\nperformance than state-of-the-art methods.", "id": "264", "start_time": "10:30", "authors": "Tao Gui, Qi Zhang, Haoran Huang, Minlong Peng and Xuanjing Huang"}, {"title": "Investigating Different Syntactic Context Types and Context Representations for Learning Word Embeddings", "venue": "ACL", "end_time": "11:20", "abstract": "The number of word embedding models is growing every year. Most of them are\nbased on the co-occurrence information of words and their contexts. However, it\nis still an open question what is the best definition of context. We provide a\nsystematical investigation of 4 different syntactic context types and context\nrepresentations for learning word embeddings. Comprehensive experiments are\nconducted to evaluate their effectiveness on 6 extrinsic and intrinsic tasks.\nWe hope that this paper, along with the published code, would be helpful for\nchoosing the best context type and representation for a given task.", "id": "559", "start_time": "10:55", "authors": "Bofang Li, Tao Liu, Zhe Zhao, Buzhou Tang, Aleksandr Drozd, Anna Rogers and Xiaoyong Du"}, {"title": "Does syntax help discourse segmentation? Not so much", "venue": "ACL", "end_time": "11:45", "abstract": "Discourse segmentation is the first step in building discourse parsers. Most\nwork on discourse segmentation does not scale to real-world discourse parsing\nacross languages, for two reasons: (i) models rely on constituent trees, and\n(ii) experiments have relied on gold standard identification of sentence and\ntoken boundaries. We therefore investigate to what extent constituents can be\nreplaced with universal dependencies, or left out completely, as well as how\nstate-of-the-art segmenters fare in the absence of sentence boundaries. Our\nresults show that dependency information is less useful than expected, but we\nprovide a fully scalable, robust model that only relies on part-of-speech\ninformation, and show that it performs well across languages in the absence of\nany gold-standard annotation.", "id": "1113", "start_time": "11:20", "authors": "Chlo\u00e9 Braud, Oph\u00e9lie Lacroix and Anders S\u00f8gaard"}, {"title": "Nonparametric Bayesian Semi-supervised Word Segmentation", "venue": "TACL", "end_time": "12:10", "abstract": "This paper presents a novel hybrid generative/discriminative model of word\nsegmentation based on nonparametric Bayesian methods. Unlike ordinary\ndiscriminative word segmentation which relies only on labeled data, our\nsemi-supervised model also leverages a huge amount of unlabeled text to\nautomatically learn new \"words'', and further constrains them by using a\nlabeled data to segment non-standard texts such as those found in social\nnetworking services. Specifically, our hybrid model combines a discriminative\nclassifier (CRF; Lafferty et al. (2001)) and unsupervised word segmentation\n(NPYLM; Mochihashi et al. (2009)) with a transparent exchange of information\nbetween these two model structures within the semi-supervised framework\n(JESS-CM; Suzuki et al. (2008)). We confirmed that it can appropriately segment\nnon-standard texts like those in Twitter and Weibo and has nearly\nstate-of-the-art accuracy on standard datasets in Japanese, Chinese, and Thai.", "id": "1525", "start_time": "11:45", "authors": "Ryo Fujii, Ryo Domoto and Daichi Mochihashi"}], "chair": "Zeljko Agic"}, {"title": "Dialogue", "start_time_iso": "2017-09-11T10:30:00", "end_time_iso": "2017-09-11T12:10:00", "type": "paper", "posters": [], "chair_affiliation": "Bloomberg", "code": "7C", "room": "Zealand", "end_time": "12:10", "id": "56", "start_time": "10:30", "talks": [{"title": "Deal or No Deal? End-to-End Learning of Negotiation Dialogues", "venue": "ACL", "end_time": "10:55", "abstract": "Much of human dialogue occurs in semi-cooperative settings, where agents with\ndifferent goals attempt to agree on common decisions. Negotiations require\ncomplex communication and reasoning skills, but success is easy to measure,\nmaking this an interesting task for AI. We gather a large dataset of\nhuman-human negotiations on a multi-issue bargaining task, where agents who\ncannot observe each other\u2019s reward functions must reach an agreement (or a\ndeal) via natural language dialogue. For the first time, we show it is possible\nto train end-to-end models for negotiation, which must learn both linguistic\nand reasoning skills with no annotated dialogue states. We also introduce\ndialogue rollouts, in which the model plans ahead by simulating possible\ncomplete continuations of the conversation, and find that this technique\ndramatically improves performance. Our code and dataset are publicly available.", "id": "1270", "start_time": "10:30", "authors": "Mike Lewis, Denis Yarats, Yann Dauphin, Devi Parikh and Dhruv Batra"}, {"title": "Agent-Aware Dropout DQN for Safe and Efficient On-line Dialogue Policy Learning", "venue": "ACL", "end_time": "11:20", "abstract": "Hand-crafted rules and reinforcement learning (RL) are two popular choices to\nobtain dialogue policy.  The rule-based policy is often reliable within\npredefined scope but not self-adaptable, whereas RL is evolvable with data but\noften suffers from a bad initial performance. We employ a {\\em companion\nlearning} framework to integrate the two approaches for {\\em on-line} dialogue\npolicy learning, in which a pre-defined rule-based policy acts as a\n\u201cteacher\u201d and guides a data-driven RL system by giving example actions as\nwell as additional rewards. A novel {\\em agent-aware dropout} Deep Q-Network\n(AAD-DQN) is proposed to address the problem of when to consult the teacher and\nhow to learn from the teacher's experiences. AAD-DQN, as a data-driven student\npolicy, provides (1) two separate experience memories for student and teacher,\n(2) an uncertainty estimated by dropout to control the timing of consultation\nand learning. Simulation experiments showed that the proposed approach can\nsignificantly improve both {\\em safety} and {\\em efficiency} of on-line policy\noptimization compared to other companion learning approaches as well as\nsupervised pre-training using static dialogue corpus.", "id": "1403", "start_time": "10:55", "authors": "Lu Chen, Xiang Zhou, Cheng Chang, Runzhe Yang and Kai Yu"}, {"title": "Towards Debate Automation: a Recurrent Model for Predicting Debate Winners", "venue": "ACL", "end_time": "11:45", "abstract": "In this paper we introduce a practical first step towards the creation of an\nautomated debate agent: a state-of-the-art recurrent predictive model for\npredicting debate winners. By having an accurate predictive model, we are able\nto objectively rate the quality of a statement made at a specific turn in a\ndebate. The model is based on a recurrent neural network architecture with\nattention, which allows the model to effectively account for the entire debate\nwhen making its prediction. Our model achieves state-of-the-art accuracy on a\ndataset of debate transcripts annotated with audience favorability of the\ndebate teams. Finally, we discuss how future work can leverage our proposed\nmodel for the creation of an automated debate agent. We accomplish this by\ndetermining the model input that will maximize audience favorability toward a\ngiven side of a debate at an arbitrary turn.", "id": "1372", "start_time": "11:20", "authors": "Peter Potash and Anna Rumshisky"}, {"title": "Conversation Modeling on Reddit Using a Graph-Structured LSTM", "venue": "TACL", "end_time": "12:10", "abstract": "This paper presents a novel approach for modeling threaded discussions on\nsocial media using a graph-structured bidirectional LSTM which represents both\nhierarchical and temporal conversation structure. In experiments with a task of\npredicting popularity of comments in Reddit discussions, the proposed model\noutperforms a node-independent architecture for different sets of input\nfeatures. Analyses show a benefit to the model over the full course of the\ndiscussion, improving detection in both early and late stages. Further, the use\nof language cues with the bidirectional tree state updates helps with\nidentifying controversial comments.", "id": "1530", "start_time": "11:45", "authors": "Victoria Zayats and Mari Ostendorf"}], "chair": "Amanda Stent"}, {"title": "Poster Session. Machine Translation and Multilingual NLP 2", "start_time_iso": "2017-09-11T10:30:00", "end_time_iso": "2017-09-11T12:10:00", "type": "poster", "posters": [{"title": "Joint Prediction of Word Alignment with Alignment Types", "id": "1526", "venue": "ACL", "authors": "Anahita Mansouri Bigvand, Te Bu and Anoop Sarkar", "abstract": "Current word alignment models do not distinguish between different types of\nalignment links. In this paper, we provide a new probabilistic model for word\nalignment where word alignments are associated with linguistically motivated\nalignment types. We propose a novel task of joint prediction of word alignment\nand alignment types and propose novel semi-supervised learning algorithms for\nthis task. We also solve a sub-task of predicting the alignment type given an\naligned word pair. In our experimental results, the generative models we\nintroduce to model alignment types significantly outperform the models without\nalignment types."}, {"title": "Further Investigation into Reference Bias in Monolingual Evaluation of Machine Translation", "id": "194", "venue": "ACL", "authors": "Qingsong Ma, Yvette Graham, Timothy Baldwin and Qun Liu", "abstract": "Monolingual evaluation of Machine Translation (MT) aims to simplify human\nassessment by requiring assessors to compare the meaning of the MT output with\na reference\ntranslation, opening up the task to a much larger pool of genuinely qualified\nevaluators. Monolingual evaluation runs the risk, however, of bias in favour of\nMT systems that happen to produce translations superficially similar to the\nreference and, consistent with this intuition, previous investigations have\nconcluded monolingual assessment to be strongly biased in this respect. On\nre-examination of past analyses, we identify a series of potential analytical\nerrors that force some important questions to be raised about the reliability\nof past conclusions, however. We subsequently carry out further investigation\ninto reference bias via direct human assessment of MT adequacy via quality\ncontrolled crowd-sourcing. Contrary to both intuition and past conclusions,\nresults for show no significant evidence of reference bias\nin monolingual evaluation of MT."}, {"title": "A Challenge Set Approach to Evaluating Machine Translation", "id": "477", "venue": "ACL", "authors": "Pierre Isabelle, Colin Cherry and George Foster", "abstract": "Neural machine translation represents an exciting leap forward in translation\nquality. But what longstanding weaknesses does it resolve, and which remain?\nWe address these questions with a challenge set approach to translation\nevaluation and error analysis. A challenge set consists of a small set of\nsentences, each hand-designed to probe a system's capacity to bridge a\nparticular structural divergence between languages.  To exemplify this\napproach, we present an English-French challenge set, and use it to analyze\nphrase-based and neural systems. The resulting analysis provides not only a\nmore fine-grained picture of the strengths of neural systems, but also insight\ninto which linguistic phenomena remain out of reach."}, {"title": "Knowledge Distillation for Bilingual Dictionary Induction", "id": "1274", "venue": "ACL", "authors": "Ndapandula Nakashole and Raphael Flauger", "abstract": "Leveraging zero-shot learning to learn\nmapping functions between vector spaces\nof different languages is a promising approach\nto bilingual dictionary induction.\nHowever, methods using this approach\nhave not yet achieved high accuracy on the\ntask. In this paper, we propose a bridging\napproach, where our main contribution\nis a knowledge distillation training objective.\nAs teachers, rich resource translation\npaths are exploited in this role. And\nas learners, translation paths involving low\nresource languages learn from the teachers.\nOur training objective allows seamless\naddition of teacher translation paths\nfor any given low resource pair. Since our\napproach relies on the quality of monolingual\nword embeddings, we also propose to\nenhance vector representations of both the\nsource and target language with linguistic\ninformation. Our experiments on various\nlanguages show large performance gains\nfrom our distillation training objective, obtaining\nas high as 17% accuracy improvements."}, {"title": "Machine Translation, it's a question of style, innit? The case of English tag questions", "id": "1073", "venue": "ACL", "authors": "Rachel Bawden", "abstract": "In this paper, we address the problem of generating English tag questions (TQs)\n(e.g. it is, isn\u2019t it?) in Machine Translation (MT). We propose a\npost-edition solution, formulating the problem as a multi-class classification\ntask. We present (i) the automatic annotation of English TQs in a parallel\ncorpus of subtitles and (ii) an approach using a series of classifiers to\npredict TQ forms, which we use to post-edit state-of-the-art MT outputs. Our\nmethod provides significant improvements in English TQ translation when\ntranslating from Czech, French and German, in turn improving the fluidity,\nnaturalness, grammatical correctness and pragmatic coherence of MT output."}, {"title": "Deciphering Related Languages", "id": "1236", "venue": "ACL", "authors": "Nima Pourdamghani and Kevin Knight", "abstract": "We present a method for translating texts between close language pairs.  The\nmethod does not require parallel data, and it does not require the languages to\nbe written in the same script.              We show results for six language pairs:\nAfrikaans/Dutch, Bosnian/Serbian, Danish/Swedish, Macedonian/Bulgarian,\nMalaysian/Indonesian, and Polish/Belorussian.  We report BLEU scores showing\nour method to outperform others that do not use parallel data."}, {"title": "Identifying Cognate Sets Across Dictionaries of Related Languages", "id": "1131", "venue": "ACL", "authors": "Adam St Arnaud, David Beck and Grzegorz Kondrak", "abstract": "We present a system for identifying cognate sets across dictionaries of related\nlanguages. The likelihood of a cognate relationship is calculated on the basis\nof a rich set of features that capture both phonetic and semantic similarity,\nas well as the presence of regular sound correspondences. The similarity scores\nare used to cluster words from different languages that may originate from a\ncommon proto-word. When tested on the Algonquian language family, our system\ndetects 63% of cognate sets while maintaining cluster purity of 70%."}, {"title": "Learning Language Representations for Typology Prediction", "id": "1424", "venue": "ACL", "authors": "Chaitanya Malaviya, Graham Neubig and Patrick Littell", "abstract": "One central mystery of neural NLP is what neural models \"know\" about their\nsubject matter. When a neural machine translation system learns to translate\nfrom one language to another, does it learn the syntax or semantics of the\nlanguages? Can this knowledge be extracted from the system to fill holes in\nhuman scientific knowledge? Existing typological databases contain relatively\nfull feature specifications for only a few hundred languages. Exploiting the\nexistence of parallel texts in more than a thousand languages, we build a\nmassive many-to-one NMT system from 1017 languages into English, and use this\nto predict information missing from typological databases. Experiments show\nthat the proposed method is able to infer not only syntactic, but also\nphonological and phonetic inventory features, and improves over a baseline that\nhas access to information about the languages geographic and phylogenetic\nneighbors."}, {"title": "Cheap Translation for Cross-Lingual Named Entity Recognition", "id": "1023", "venue": "ACL", "authors": "Stephen Mayhew, Chen-Tse Tsai and Dan Roth", "abstract": "Recent work in NLP has attempted to deal with low-resource languages but still\nassumed a resource level that is not present for most languages, e.g., the\navailability of Wikipedia in the target language. We propose a simple method\nfor cross-lingual named entity recognition (NER) that works well in settings\nwith {\\em very} minimal resources. Our approach makes use of a lexicon to\n``translate\" annotated data available in one or several high resource\nlanguage(s) into the target language, and learns a standard monolingual NER\nmodel there. Further, when Wikipedia is available in the target language, our\nmethod can enhance Wikipedia based methods to yield state-of-the-art NER\nresults; we evaluate on 7 diverse languages, improving the state-of-the-art by\nan average of 5.5\\% F1 points. With the minimal resources required, this is an\nextremely portable cross-lingual NER approach, as illustrated using a truly\nlow-resource language, Uyghur."}, {"title": "Cross-Lingual Induction and Transfer of Verb Classes Based on Word Vector Space Specialisation", "id": "817", "venue": "ACL", "authors": "Ivan Vuli\u0107, Nikola Mrk\u0161i\u0107 and Anna Korhonen", "abstract": "Existing approaches to automatic VerbNet-style verb classification are heavily\ndependent on feature engineering and therefore limited to languages with mature\nNLP pipelines. In this work, we propose a novel cross-lingual transfer method\nfor inducing VerbNets for multiple languages. To the best of our knowledge,\nthis is the first study which demonstrates how the architectures for learning\nword embeddings can be applied to this challenging syntactic-semantic task. Our\nmethod uses cross-lingual translation pairs to tie each of the six target\nlanguages into a bilingual vector space with English, jointly specialising the\nrepresentations to encode the relational information from English VerbNet. A\nstandard clustering algorithm is then run on top of the VerbNet-specialised\nrepresentations, using vector dimensions as features for learning verb classes.\nOur results show that the proposed cross-lingual transfer approach sets new\nstate-of-the-art verb classification performance across all six target\nlanguages explored in this work."}, {"title": "Classification of telicity using cross-linguistic annotation projection", "id": "77", "venue": "ACL", "authors": "Annemarie Friedrich and Damyana Gateva", "abstract": "This paper addresses the automatic recognition of telicity, an aspectual\nnotion. A telic event includes a natural endpoint (\"she walked home\"), while an\natelic event does not (\"she walked around\"). Recognizing this difference is a\nprerequisite for temporal natural language understanding. In English, this\nclassification task is difficult, as telicity is a covert linguistic category.\nIn contrast, in Slavic languages, aspect is part of a verb's meaning and even\navailable in machine-readable dictionaries. Our contributions are as follows.\nWe successfully leverage additional silver standard training data in the form\nof projected annotations from parallel English-Czech data as well as context\ninformation, improving automatic telicity classification for English\nsignificantly compared to previous work. We also create a new data set of\nEnglish texts manually annotated with telicity."}, {"title": "Semantic Specialisation of Distributional Word Vector Spaces using Monolingual and Cross-Lingual Constraints", "id": "1533", "venue": "ACL", "authors": "Nikola Mrk\u0161i\u0107, Ivan Vuli\u0107, Diarmuid \u00d3 S\u00e9aghdha, Ira Leviant, Roi Reichart, Milica Ga\u0161i\u0107, Anna Korhonen and Steve Young", "abstract": "We present Attract-Repel, an algorithm for improving the semantic quality of\nword vectors by injecting constraints extracted from lexical resources.\nAttract-Repel facilitates the use of constraints from mono- and cross-lingual\nresources, yielding semantically specialised cross-lingual vector spaces. Our\nevaluation shows that the method can make use of existing cross-lingual\nlexicons to construct high-quality vector spaces for a plethora of different\nlanguages, facilitating semantic transfer from high- to lower-resource ones.\nThe effectiveness of our approach is demonstrated with state-of-the-art results\non semantic similarity datasets in six languages. We next show that\nAttract-Repel-specialised vectors boost performance in the downstream task of\ndialogue state tracking (DST) across multiple languages. Finally, we show that\ncross-lingual vector spaces produced by our algorithm facilitate the training\nof multilingual DST models, which brings further performance improvements."}, {"title": "Counterfactual Learning from Bandit Feedback under Deterministic Logging : A Case Study in Statistical Machine Translation", "id": "637", "venue": "ACL", "authors": "Carolin Lawrence, Artem Sokolov and Stefan Riezler", "abstract": "The goal of counterfactual learning for statistical machine translation (SMT)\nis to optimize a target SMT system from logged data that consist of user\nfeedback to translations that were predicted by another, historic SMT system. A\nchallenge arises by the fact that risk-averse commercial SMT systems\ndeterministically log the most probable translation. The lack of sufficient\nexploration of the SMT output space seemingly contradicts the theoretical\nrequirements for counterfactual learning. We show that counterfactual learning\nfrom deterministic bandit logs is possible nevertheless by smoothing out\ndeterministic components in learning. This can be achieved by additive and\nmultiplicative control variates that avoid degenerate behavior in empirical\nrisk minimization. Our simulation experiments show improvements of up to 2 BLEU\npoints by counterfactual learning from deterministic bandit feedback."}], "chair_affiliation": "LIMSI, CNRS and University of Pennsylvania", "code": "7D", "room": "Aarhus", "end_time": "12:10", "id": "57", "start_time": "10:30", "talks": [], "chair": "Marianna Apidianaki"}, {"title": "Poster Session. Information Extraction 2", "start_time_iso": "2017-09-11T10:30:00", "end_time_iso": "2017-09-11T12:10:00", "type": "poster", "posters": [{"title": "Learning Fine-grained Relations from Chinese User Generated Categories", "id": "119", "venue": "ACL", "authors": "Chengyu Wang, Yan Fan, Xiaofeng He and Aoying Zhou", "abstract": "User generated categories (UGCs) are short texts that reflect how people\ndescribe and organize entities, expressing rich semantic relations implicitly.\nWhile most methods on UGC relation extraction are based on pattern matching in\nEnglish circumstances, learning relations from Chinese UGCs poses different\nchallenges due to the flexibility of expressions. In this paper, we present a\nweakly supervised learning framework to harvest relations from Chinese UGCs. We\nidentify is-a relations via word embedding based projection and inference,\nextract non-taxonomic relations and their category patterns by graph mining. We\nconduct experiments on Chinese Wikipedia and achieve high accuracy,\noutperforming state-of-the-art methods."}, {"title": "Improving Slot Filling Performance with Attentive Neural Networks on Dependency Structures", "id": "142", "venue": "ACL", "authors": "Lifu Huang, Avirup Sil, Heng Ji and Radu Florian", "abstract": "Slot Filling (SF) aims to extract the values of certain types of attributes (or\nslots, such as person:cities\\_of\\_residence) for a given entity from a large\ncollection of source documents. \nIn this paper we propose an effective DNN architecture for SF with the\nfollowing new strategies: (1). Take a regularized dependency graph instead of a\nraw sentence as input to DNN, to compress the wide contexts between query and\ncandidate filler; (2). Incorporate two attention mechanisms: local attention\nlearned from query and candidate filler, and global attention learned from\nexternal knowledge bases, to guide the model to better select indicative\ncontexts to determine slot type. Experiments show that this framework\noutperforms state-of-the-art on both relation extraction (16% absolute F-score\ngain) and slot filling validation for each individual system (up to 8.5%\nabsolute F-score gain)."}, {"title": "Identifying Products in Online Cybercrime Marketplaces: A Dataset for Fine-grained Domain Adaptation", "id": "529", "venue": "ACL", "authors": "Greg Durrett, Jonathan K. Kummerfeld, Taylor Berg-Kirkpatrick, Rebecca Portnoff, Sadia Afroz, Damon McCoy, Kirill Levchenko and Vern Paxson", "abstract": "One weakness of machine-learned NLP models is that they typically perform\npoorly on out-of-domain data. In this work, we study the task of identifying\nproducts being bought and sold in online cybercrime forums, which exhibits\nparticularly challenging cross-domain effects. We formulate a task that\nrepresents a hybrid of slot-filling information extraction and named entity\nrecognition and annotate data from four different forums. Each of these forums\nconstitutes its own \"fine-grained domain\" in that the forums cover different\nmarket sectors with different properties, even though all forums are in the\nbroad domain of cybercrime. We characterize these domain differences in the\ncontext of a learning-based system: supervised models see decreased accuracy\nwhen applied to new forums, and standard techniques for semi-supervised\nlearning and domain adaptation have limited effectiveness on this data, which\nsuggests the need to improve these techniques. We release a dataset of 1,938\nannotated posts from across the four forums."}, {"title": "Labeling Gaps Between Words: Recognizing Overlapping Mentions with Mention Separators", "id": "625", "venue": "ACL", "authors": "Aldrian Obaja Muis and Wei Lu", "abstract": "In this paper, we propose a new model that is capable of recognizing\noverlapping mentions. We introduce a novel notion of mention separators that\ncan be effectively used to capture how mentions overlap with one another. On\ntop of a novel multigraph representation that we introduce, we show that\nefficient and exact inference can still be performed. We present some\ntheoretical analysis on the differences between our model and a recently\nproposed model for recognizing overlapping mentions, and discuss the possible\nimplications of the differences. Through extensive empirical analysis on\nstandard datasets, we demonstrate the effectiveness of our approach."}, {"title": "Deep Joint Entity Disambiguation with Local Neural Attention", "id": "1044", "venue": "ACL", "authors": "Octavian-Eugen Ganea and Thomas Hofmann", "abstract": "We propose a novel deep learning model for joint document-level entity\ndisambiguation, which leverages learned neural representations. Key components\nare entity embeddings, a neural attention mechanism over local context windows,\nand a differentiable joint inference stage for disambiguation. Our approach\nthereby combines benefits of deep learning with more traditional approaches\nsuch as graphical models and probabilistic mention-entity maps. Extensive\nexperiments show that we are able to obtain competitive or state-of-the-art\naccuracy at moderate computational costs."}, {"title": "MinIE: Minimizing Facts in Open Information Extraction", "id": "1094", "venue": "ACL", "authors": "Kiril Gashteovski, Rainer Gemulla and Luciano Del Corro", "abstract": "The goal of Open Information Extraction (OIE) is to extract surface relations\nand their arguments from natural-language text in an unsupervised,\ndomain-independent manner. In this paper, we propose MinIE, an OIE system that\naims to provide useful, compact extractions with high precision and\nrecall. MinIE approaches these goals by (1) representing information about\npolarity, modality, attribution, and quantities with semantic annotations\ninstead of in the actual extraction, and (2) identifying and removing parts\nthat are considered overly specific. We conducted an experimental study with\nseveral real-world datasets and found that MinIE achieves competitive or\nhigher precision and recall than most prior systems, while at the same time\nproducing shorter, semantically enriched extractions."}, {"title": "Scientific Information Extraction with Semi-supervised Neural Tagging", "id": "1301", "venue": "ACL", "authors": "Yi Luan, Mari Ostendorf and Hannaneh Hajishirzi", "abstract": "This paper addresses the problem of extracting                                       \n     \n\nkeyphrases\nfrom\nscientific\narticles and categorizing them as corresponding to a task, process, or\nmaterial. We cast the problem as sequence tagging and introduce \nsemi-supervised methods to a neural tagging model, which builds on recent\nadvances in named entity recognition. Since annotated training data is scarce\nin this domain, we introduce a graph-based semi-supervised algorithm together \nwith a data selection scheme to leverage unannotated articles. Both inductive\nand transductive semi-supervised learning strategies outperform\nstate-of-the-art information extraction performance on the 2017 SemEval Task 10\nScienceIE task."}, {"title": "NITE: A Neural Inductive Teaching Framework for Domain Specific NER", "id": "634", "venue": "ACL", "authors": "Siliang Tang, Ning Zhang, Jinjiang Zhang, Fei Wu and Yueting Zhuang", "abstract": "In domain-specific NER, due to insufficient labeled training data, deep models\nusually fail to behave normally. In this paper, we proposed a novel Neural\nInductive TEaching framework (NITE) to transfer knowledge from existing\ndomain-specific NER models into an arbitrary deep neural network in a\nteacher-student training manner. NITE is a general framework that builds upon\ntransfer learning and multiple instance learning, which collaboratively not\nonly transfers knowledge to a deep student network but also reduces the noise\nfrom teachers. NITE can help deep learning methods to effectively utilize\nexisting resources (i.e., models, labeled and unlabeled data) in a small\ndomain. The experiment resulted on Disease NER proved that without using any\nlabeled data, NITE can significantly boost the performance of a\nCNN-bidirectional LSTM-CRF NER neural network nearly over 30% in terms of\nF1-score."}, {"title": "Speeding up Reinforcement Learning-based Information Extraction Training using Asynchronous Methods", "id": "1324", "venue": "ACL", "authors": "Aditya Sharma, Zarana Parekh and Partha Talukdar", "abstract": "RLIE-DQN is a recently proposed Reinforcement Learning-based Information\nExtraction (IE) technique which is able to incorporate external evidence during\nthe extraction process. RLIE-DQN trains a single agent sequentially, training\non one instance at a time. This results in significant training slowdown which\nis undesirable. We leverage recent advances in parallel RL training using\nasynchronous methods and propose RLIE-A3C. RLIE-A3C trains multiple agents in\nparallel and is able to achieve upto 6x training speedup over RLIE-DQN, while\nsuffering no loss in average accuracy."}, {"title": "Leveraging Linguistic Structures for Named Entity Recognition with Bidirectional Recursive Neural Networks", "id": "1408", "venue": "ACL", "authors": "Peng-Hsuan Li, Ruo-Ping Dong, Yu-Siang Wang, Ju-Chieh Chou and Wei-Yun Ma", "abstract": "In this paper, we utilize the linguistic structures of texts to improve named\nentity recognition by BRNN-CNN, a special bidirectional recursive network\nattached with a convolutional network. Motivated by the observation that named\nentities are highly related to linguistic constituents, we propose a\nconstituent-based BRNN-CNN for named entity recognition. In contrast to\nclassical sequential labeling methods, the system first identifies which text\nchunks are possible named entities by whether they are linguistic constituents.\nThen it classifies these chunks with a constituency tree structure by\nrecursively propagating syntactic and semantic information to each constituent\nnode. This method surpasses current state-of-the-art on OntoNotes 5.0 with\nautomatically generated parses."}, {"title": "Fast and Accurate Entity Recognition with Iterated Dilated Convolutions", "id": "1005", "venue": "ACL", "authors": "Emma Strubell, Patrick Verga, David Belanger and Andrew McCallum", "abstract": "Today when many practitioners run basic NLP on the entire web and large-volume\ntraffic, faster methods are paramount to saving time and energy costs.\nRecent advances in GPU hardware have led to the emergence of bi-directional\nLSTMs as a standard method for obtaining per-token vector representations\nserving as input to labeling tasks such as NER (often followed by prediction in\na linear-chain CRF). \nThough expressive and accurate, these models fail to fully exploit GPU\nparallelism, limiting their computational efficiency.\nThis paper proposes a faster alternative to Bi-LSTMs for NER: Iterated Dilated\nConvolutional Neural Networks (ID-CNNs), which have better capacity than\ntraditional CNNs for large context and structured prediction.  \nUnlike LSTMs whose sequential processing on sentences of length N requires O(N)\ntime even in the face of parallelism, ID-CNNs permit fixed-depth convolutions\nto run in parallel across entire documents.\nWe describe a distinct combination of network structure, parameter sharing and\ntraining procedures that enable dramatic 14-20x test-time speedups while\nretaining accuracy comparable to the Bi-LSTM-CRF. Moreover, ID-CNNs trained to\naggregate context from the entire document are more accurate than Bi-LSTM-CRFs\nwhile attaining 8x faster test time speeds."}, {"title": "Entity Linking via Joint Encoding of Types, Descriptions, and Context", "id": "1356", "venue": "ACL", "authors": "Nitish Gupta, Sameer Singh and Dan Roth", "abstract": "For accurate entity linking, we need to capture various information aspects of\nan entity, such as its description in a KB, contexts in which it is mentioned,\nand structured knowledge. Additionally, a linking system should work on texts\nfrom different domains without requiring domain-specific training data or\nhand-engineered features.\n\nIn this work we present a neural, modular entity linking system that learns a\nunified dense representation for each entity using multiple sources of\ninformation, such as its description, contexts around its mentions, and its\nfine-grained types. We show that the resulting entity linking system is\neffective at combining these sources, and performs competitively, sometimes\nout-performing current state-of-the-art systems across datasets, without\nrequiring any domain-specific training data or hand-engineered features. We\nalso show that our model can effectively \u201cembed\u201d entities that are new to\nthe KB, and is able to link its mentions accurately."}, {"title": "An Insight Extraction System on BioMedical Literature with Deep Neural Networks", "id": "1295", "venue": "ACL", "authors": "Hua He, Kris Ganjam, Navendu Jain, Jessica Lundin, Ryen White and Jimmy Lin", "abstract": "Mining biomedical text offers an opportunity to automatically discover\nimportant facts and infer associations among them. As new scientific findings\nappear across a large collection of biomedical publications, our aim is to tap\ninto this literature to automate biomedical knowledge extraction and identify\nimportant insights from them. Towards that goal, we develop a system with novel\ndeep neural networks to extract insights on biomedical literature. Evaluation\nshows our system is able to provide insights with competitive accuracy of human\nacceptance and its relation extraction component outperforms previous work."}], "chair_affiliation": "University of Copenhagen", "code": "7E", "room": "Odense", "end_time": "12:10", "id": "58", "start_time": "10:30", "talks": [], "chair": "Isabelle Augenstein"}, {"title": "Poster Session. NLP Applications", "start_time_iso": "2017-09-11T10:30:00", "end_time_iso": "2017-09-11T12:10:00", "type": "poster", "posters": [{"title": "Word Etymology as Native Language Interference", "id": "642", "venue": "ACL", "authors": "Vivi Nastase and Carlo Strapparava", "abstract": "We present experiments that show the influence of native language on lexical\nchoice when producing text in another language -- in this particular case\nEnglish. We start from the premise that non-native English speakers will choose\nlexical items that are close to words in their native language. This leads us\nto an etymology-based representation of documents written by people whose\nmother tongue is an Indo-European language. Based on this representation we\ngrow a language family tree, that matches closely the Indo-European language\ntree."}, {"title": "A Simpler and More Generalizable Story Detector using Verb and Character Features", "id": "49", "venue": "ACL", "authors": "Joshua Eisenberg and Mark Finlayson", "abstract": "Story detection is the task of determining whether or not a unit of text\ncontains a story. Prior approaches achieved a maximum performance of 0.66 F1,\nand did not generalize well across different corpora. We present a new\nstate-of-the-art detector that achieves a maximum performance of 0.75 F1 (a 14%\nimprovement), with significantly greater generalizability than previous work.\nIn particular, our detector achieves performance above 0.70 F1 across a variety\nof combinations of lexically different corpora for training and testing, as\nwell as dramatic improvements (up to 4,000%) in performance when trained on a\nsmall, disfluent data set. The new detector uses two basic types of\nfeatures\u2013ones related to events, and ones related to characters\u2013totaling\n283 specific features overall; previous detectors used tens of thousands of\nfeatures, and so this detector represents a significant simplification along\nwith increased performance."}, {"title": "Multi-modular domain-tailored OCR post-correction", "id": "236", "venue": "ACL", "authors": "Sarah Schulz and Jonas Kuhn", "abstract": "One of the main obstacles for many Digital Humanities projects is the low data\navailability. Texts have to be digitized in an expensive and time consuming\nprocess whereas Optical Character Recognition (OCR) post-correction is one of\nthe time-critical factors. At the example of OCR post-correction, we show the\nadaptation of a generic system to solve a specific problem with little data.\nThe system accounts for a diversity of errors encountered in OCRed texts coming\nfrom different time periods in the domain of literature. We show that the\ncombination of different approaches, such as e.g. Statistical Machine\nTranslation and spell checking, with the help of a ranking mechanism\ntremendously improves over single-handed approaches. Since we consider the\naccessibility of the resulting tool as\na crucial part of Digital Humanities collaborations, we describe the workflow\nwe suggest for efficient text recognition and subsequent automatic and manual\npost-correction"}, {"title": "Learning to Predict Charges for Criminal Cases with Legal Basis", "id": "282", "venue": "ACL", "authors": "Bingfeng Luo, Yansong Feng, Jianbo Xu, Xiang Zhang and Dongyan Zhao", "abstract": "The charge prediction task is to determine appropriate charges for a given\ncase, which is helpful for legal assistant systems where the user input is fact\ndescription. We argue that relevant law articles play an important role in this\ntask, and therefore propose an attention-based neural network method to jointly\nmodel the charge prediction task and the relevant article extraction task in a\nunified framework. The experimental results show that, besides providing legal\nbasis, the relevant articles can also clearly improve the charge prediction\nresults, and our full model can effectively predict appropriate charges for\ncases with different expression styles."}, {"title": "Quantifying the Effects of Text Duplication on Semantic Models", "id": "488", "venue": "ACL", "authors": "Alexandra Schofield, Laure Thompson and David Mimno", "abstract": "Duplicate documents are a pervasive problem in text datasets and can have a\nstrong effect on unsupervised models. Methods to remove duplicate texts are\ntypically heuristic or very expensive, so it is vital to know when and why they\nare needed. We measure the sensitivity of two latent semantic methods to the\npresence of different levels of document repetition. By artificially creating\ndifferent forms of duplicate text we confirm several hypotheses about how\nrepeated text impacts models. While a small amount of duplication is tolerable,\nsubstantial over-representation of subsets of the text may overwhelm meaningful\ntopical patterns."}, {"title": "Identifying Semantically Deviating Outlier Documents", "id": "674", "venue": "ACL", "authors": "Honglei Zhuang, Chi Wang, Fangbo Tao, Lance Kaplan and Jiawei Han", "abstract": "A document outlier is a document that substantially deviates in semantics from\nthe majority ones in a corpus.              Automatic identification of document\noutliers\ncan be valuable in many applications, such as screening health records for\nmedical mistakes.  In this paper, we study the problem of mining semantically\ndeviating document outliers in a given corpus.              We develop a generative\nmodel\nto identify frequent and characteristic semantic regions in the word embedding\nspace to represent the given corpus, and a robust outlierness measure which is\nresistant to noisy content in documents.  Experiments conducted on two\nreal-world textual data sets show that our method can achieve an up to 135%\nimprovement over baselines in terms of recall at top-1% of the outlier ranking."}, {"title": "Detecting and Explaining Causes From Text For a Time Series Event", "id": "1184", "venue": "ACL", "authors": "Dongyeop Kang, Varun Gangal, Ang Lu, Zheng Chen and Eduard Hovy", "abstract": "Explaining underlying causes or effects about events is a challenging but\nvaluable task.\nWe define a novel problem of generating explanations of a time series event by\n(1) searching cause and effect relationships of the time series with textual\ndata and (2) constructing a connecting chain between them to generate an\nexplanation.\nTo detect causal features from text, we propose a novel method based on the\nGranger causality of time series between features extracted from text such as\nN-grams, topics, sentiments, and their composition.\nThe generation of the sequence of causal entities requires a commonsense\ncausative knowledge base with efficient reasoning. \nTo ensure good interpretability and appropriate lexical usage we combine\nsymbolic and neural representations, using a neural reasoning algorithm trained\non commonsense causal tuples to predict the next cause step.\nOur quantitative and human analysis show empirical evidence that our method\nsuccessfully extracts meaningful causality relationships between time series\nwith textual features and generates appropriate explanation between them."}, {"title": "A Novel Cascade Model for Learning Latent Similarity from Heterogeneous Sequential Data of MOOC", "id": "247", "venue": "ACL", "authors": "Zhuoxuan Jiang, Shanshan Feng, Gao Cong, Chunyan Miao and Xiaoming Li", "abstract": "Recent years have witnessed the proliferation of Massive Open Online Courses\n(MOOCs). With massive learners being offered MOOCs, there is a demand that the\nforum contents within MOOCs need to be classified in order to facilitate both\nlearners and instructors. Therefore we investigate a significant application,\nwhich is to associate forum threads to subtitles of video clips. This task can\nbe regarded as a document ranking problem, and the key is how to learn a\ndistinguishable text representation from word sequences and learners' behavior\nsequences. In this paper, we propose a novel cascade model, which can capture\nboth the latent semantics and latent similarity by modeling MOOC data.\nExperimental results on two real-world datasets demonstrate that our textual\nrepresentation outperforms state-of-the-art unsupervised counterparts for the\napplication."}, {"title": "Identifying the Provision of Choices in Privacy Policy Text", "id": "747", "venue": "ACL", "authors": "Kanthashree Mysore Sathyendra, Shomir Wilson, Florian Schaub, Sebastian Zimmeck and Norman Sadeh", "abstract": "Websites' and mobile apps' privacy policies, written in natural language, tend\nto be long and difficult to understand. Information privacy revolves around the\nfundamental principle of Notice and choice, namely the idea that users should\nbe able to make informed decisions about what information about them can be\ncollected and how it can be used. Internet users want control over their\nprivacy, but their choices are often hidden in long and convoluted privacy\npolicy texts. Moreover, little (if any) prior work has been done to detect the\nprovision of choices in text. We address this challenge of enabling user choice\nby automatically identifying and extracting pertinent choice language in\nprivacy policies. In particular, we present a two-stage architecture of\nclassification models to identify opt-out choices in privacy policy text,\nlabelling common varieties of choices with a mean F1 score of 0.735. Our\ntechniques enable the creation of systems to help Internet users to learn about\ntheir choices, thereby effectuating notice and choice and improving Internet\nprivacy."}, {"title": "An Empirical Analysis of Edit Importance between Document Versions", "id": "986", "venue": "ACL", "authors": "Tanya Goyal, Sachin Kelkar, Manas Agarwal and Jeenu Grover", "abstract": "In this paper, we present a novel approach to infer significance of various\ntextual  edits to documents. An author may make several edits to a document;\neach edit varies in its impact to the content of the document. While some edits\nare surface changes and introduce negligible change, other edits may change the\ncontent/tone of the document significantly. In this paper, we perform an\nanalysis on the human perceptions of edit importance while reviewing documents\nfrom one version to the next. We identify linguistic features that influence\nedit importance and model it in a regression based setting. We show that the\npredicted importance by our approach is highly correlated with the human\nperceived importance, established by a Mechanical Turk study."}, {"title": "Transition-Based Disfluency Detection using LSTMs", "id": "1013", "venue": "ACL", "authors": "Shaolei Wang, Wanxiang Che, Yue Zhang, Meishan Zhang and Ting Liu", "abstract": "In this paper, we model the problem of disfluency detection using a\ntransition-based framework, which incrementally constructs and labels the\ndisfluency chunk of input sentences using a new transition system without\nsyntax information. Compared with sequence labeling methods, it can capture\nnon-local chunk-level features; compared with joint parsing and disfluency\ndetection methods, it is free for noise in syntax. Experiments show that our\nmodel achieves state-of-the-art f-score of 87.5\\% on the commonly used English\nSwitchboard test set, and a set of  in-house annotated Chinese data."}, {"title": "Neural Sequence-Labelling Models for Grammatical Error Correction", "id": "457", "venue": "ACL", "authors": "Helen Yannakoudakis, Marek Rei, \u00d8istein E. Andersen and Zheng Yuan", "abstract": "We propose an approach to N -best list re-\nranking using neural sequence-labelling\nmodels. We train a compositional model\nfor error detection that calculates the prob-\nability of each token in a sentence being\ncorrect or incorrect, utilising the full sen-\ntence as context. Using the error detec-\ntion model, we then re-rank the N best\nhypotheses generated by statistical ma-\nchine translation systems. Our approach\nachieves state-of-the-art results on error\ncorrection for three different datasets, and\nit has the additional advantage of only us-\ning a small set of easily computed features\nthat require no linguistic input."}, {"title": "Adapting Sequence Models for Sentence Correction", "id": "1254", "venue": "ACL", "authors": "Allen Schmaltz, Yoon Kim, Alexander Rush and Stuart Shieber", "abstract": "In a controlled experiment of sequence-to-sequence approaches for the task of\nsentence correction, we find that character-based models are generally more\neffective than word-based models and models that encode subword information via\nconvolutions, and that modeling the output data as a series of diffs improves\neffectiveness over standard approaches. Our strongest sequence-to-sequence\nmodel improves over our strongest phrase-based statistical machine translation\nmodel, with access to the same data, by 6 M2 (0.5 GLEU) points. Additionally,\nin the data environment of the standard CoNLL-2014 setup, we demonstrate that\nmodeling (and tuning against) diffs yields similar or better M2 scores with\nsimpler models and/or significantly less data than previous\nsequence-to-sequence approaches."}], "chair_affiliation": "Johns Hopkins University", "code": "7F", "room": "Copenhagen", "end_time": "12:10", "id": "59", "start_time": "10:30", "talks": [], "chair": "Courtney Napoles"}], [{"title": "Lunch", "start_time_iso": "2017-09-11T12:10:00", "end_time": "13:40", "end_time_iso": "2017-09-11T13:40:00", "id": "60", "start_time": "12:10", "type": "break"}], [{"title": "Machine Translation and Multilingual/Multimodal NLP (Short)", "start_time_iso": "2017-09-11T13:40:00", "end_time_iso": "2017-09-11T15:25:00", "type": "paper", "posters": [], "chair_affiliation": "Carnegie Mellon University", "code": "8A", "room": "Jutland", "end_time": "15:25", "id": "61", "start_time": "13:40", "talks": [{"title": "A Study of Style in Machine Translation: Controlling the Formality of Machine Translation Output", "venue": "ACL", "end_time": "13:55", "abstract": "Stylistic variations of language, such as formality, carry speakers' intention\nbeyond literal meaning and should be conveyed adequately in translation. We\npropose to use lexical formality models to control the formality level of\nmachine translation output. We demonstrate the effectiveness of our approach in\nempirical evaluations, as measured by automatic metrics and human assessments.", "id": "1411", "start_time": "13:40", "authors": "Xing Niu, Marianna Martindale and Marine Carpuat"}, {"title": "Sharp Models on Dull Hardware: Fast and Accurate Neural Machine Translation Decoding on the CPU", "venue": "ACL", "end_time": "14:10", "abstract": "Attentional sequence-to-sequence models have become the new standard for\nmachine translation, but one challenge of such models is a significant increase\nin training and decoding cost compared to phrase-based systems. In this work we\nfocus on efficient decoding, with a goal of achieving accuracy close the\nstate-of-the-art in neural machine translation (NMT), while achieving CPU\ndecoding speed/throughput close to that of a phrasal decoder.\n\nWe approach this problem from two angles: First, we describe several techniques\nfor speeding up an NMT beam search decoder, which obtain a 4.4x speedup over a\nvery efficient baseline decoder without changing the decoder output. Second, we\npropose a simple but powerful network architecture which uses an RNN (GRU/LSTM)\nlayer at bottom, followed by a series of stacked fully-connected layers applied\nat every timestep. This architecture achieves similar accuracy to a deep\nrecurrent model, at a small fraction of the training and decoding cost. By\ncombining these techniques, our best system achieves a very competitive\naccuracy of 38.3 BLEU on WMT English-French NewsTest2014, while decoding at 100\nwords/sec on single-threaded CPU. We believe this is the best published\naccuracy/speed trade-off of an NMT system.", "id": "469", "start_time": "13:55", "authors": "Jacob Devlin"}, {"title": "Exploiting Cross-Sentence Context for Neural Machine Translation", "venue": "ACL", "end_time": "14:25", "abstract": "In translation, considering the document as a whole can help to resolve\nambiguities and inconsistencies. In this paper, we propose a cross-sentence\ncontext-aware approach and investigate the influence of historical contextual\ninformation on the performance of neural machine translation (NMT). First, this\nhistory is summarized in a hierarchical way. We then integrate the historical\nrepresentation into NMT in two strategies: 1) a warm-start of encoder and\ndecoder states, and 2) an auxiliary context source for updating decoder states.\nExperimental results on a large Chinese-English translation task show that our\napproach significantly improves upon a strong attention-based NMT system by up\nto +2.1 BLEU points.", "id": "716", "start_time": "14:10", "authors": "Longyue Wang, Zhaopeng Tu, Andy Way and Qun Liu"}, {"title": "Cross-Lingual Transfer Learning for POS Tagging without Cross-Lingual Resources", "venue": "ACL", "end_time": "14:40", "abstract": "Training a POS tagging model with crosslingual transfer learning usually\nrequires linguistic knowledge and resources about the relation between the\nsource language and the target language. In this paper, we introduce a\ncross-lingual transfer learning model for POS tagging without ancillary\nresources such as parallel corpora. The proposed cross-lingual model utilizes a\ncommon BLSTM that enables knowledge transfer from other languages, and private\nBLSTMs for language-specific representations. The cross-lingual model is\ntrained with language-adversarial training and bidirectional language modeling\nas auxiliary objectives to better represent language-general information while\nnot losing the information about a specific target language. Evaluating on POS\ndatasets from 14 languages in the Universal Dependencies corpus, we show that\nthe proposed transfer learning model improves\nthe POS tagging performance of the target languages without exploiting any\nlinguistic knowledge between the source language and the target language.", "id": "1511", "start_time": "14:25", "authors": "Joo-Kyung Kim, Young-Bum Kim, Ruhi Sarikaya and Eric Fosler-Lussier"}, {"title": "Image Pivoting for Learning Multilingual Multimodal Representations", "venue": "ACL", "end_time": "14:55", "abstract": "In this paper we propose a model to\nlearn multimodal multilingual representations\nfor matching images and sentences\nin different languages, with the aim of\nadvancing multilingual versions of image\nsearch and image understanding. Our\nmodel learns a common representation for\nimages and their descriptions in two different\nlanguages (which need not be parallel)\nby considering the image as a pivot between\ntwo languages. We introduce a new\npairwise ranking loss function which can\nhandle both symmetric and asymmetric\nsimilarity between the two modalities. We\nevaluate our models on image-description\nranking for German and English, and on\nsemantic textual similarity of image descriptions\nin English. In both cases we\nachieve state-of-the-art performance.", "id": "644", "start_time": "14:40", "authors": "Spandana Gella, Rico Sennrich, Frank Keller and Mirella Lapata"}, {"title": "Neural Machine Translation with Source Dependency Representation", "venue": "ACL", "end_time": "15:10", "abstract": "Source dependency information has been successfully introduced into statistical\nmachine translation. However, there are only a few preliminary attempts for\nNeural Machine Translation (NMT), such as concatenating representations of\nsource word and its dependency label together. In this paper, we propose a\nnovel NMT with source dependency representation to improve translation\nperformance of NMT, especially long sentences. Empirical results on NIST\nChinese-to-English translation task show that our method achieves 1.6 BLEU\nimprovements on average over a strong NMT system.", "id": "538", "start_time": "14:55", "authors": "Kehai Chen, Rui Wang, Masao Utiyama, Lemao Liu, Akihiro Tamura, Eiichiro Sumita and Tiejun Zhao"}, {"title": "Visual Denotations for Recognizing Textual Entailment", "venue": "ACL", "end_time": "15:25", "abstract": "In the logic approach to Recognizing Textual Entailment, identifying\nphrase-to-phrase semantic relations is still an unsolved problem. Resources\nsuch as the Paraphrase Database offer limited coverage despite their large size\nwhereas unsupervised distributional models of meaning often fail to recognize\nphrasal entailments. We propose to map phrases to their visual denotations and\ncompare their meaning in terms of their images. We show that our approach is\neffective in the task of Recognizing Textual Entailment when combined with\nspecific linguistic and logic features.", "id": "1388", "start_time": "15:10", "authors": "Dan Han, Pascual Mart\u00ednez-G\u00f3mez and Koji Mineshima"}], "chair": "Yulia Tsvetkov"}, {"title": "Machine Learning (Short)", "start_time_iso": "2017-09-11T13:40:00", "end_time_iso": "2017-09-11T15:25:00", "type": "paper", "posters": [], "chair_affiliation": "NYU", "code": "8B", "room": "Funen", "end_time": "15:25", "id": "62", "start_time": "13:40", "talks": [{"title": "Sequence Effects in Crowdsourced Annotations", "venue": "ACL", "end_time": "13:55", "abstract": "Manual data annotation is a vital component of NLP research. When designing\nannotation tasks, properties of the annotation interface can unintentionally\nlead to artefacts in the resulting dataset, biasing the evaluation. In this\npaper, we explore sequence effects where annotations of an item are affected by\nthe preceding items. Having assigned one label to an instance, the annotator\nmay be less (or more) likely to assign the same label to the next. During\nrating tasks, seeing a low quality item may affect the score given to the next\nitem either positively or negatively. We see clear evidence of both types of\neffects using auto-correlation studies over three different crowdsourced\ndatasets. We then recommend a simple way to minimise sequence effects.", "id": "1430", "start_time": "13:40", "authors": "Nitika Mathur, Timothy Baldwin and Trevor Cohn"}, {"title": "No Need to Pay Attention: Simple Recurrent Neural Networks Work!", "venue": "ACL", "end_time": "14:10", "abstract": "First-order factoid question answering assumes that the question can be\nanswered by a single fact in a knowledge base (KB). While this does not seem\nlike a challenging task, many recent attempts that apply either complex\nlinguistic reasoning or deep neural networks achieve 65\\%--76\\% accuracy on\nbenchmark\nsets. Our approach formulates the task as two machine learning problems:\\\ndetecting the entities in the question, and classifying the question as one of\nthe relation types in the KB. We train a recurrent neural network to solve each\nproblem. On the SimpleQuestions dataset, our approach yields substantial\nimprovements over previously published results --- even neural networks based\non much more complex architectures. The simplicity of our approach also has\npractical advantages, such as efficiency and modularity, that are valuable\nespecially in an industry setting. In fact, we present a preliminary analysis\nof the performance of our model on real queries from Comcast's X1 entertainment\nplatform with millions of users every day.", "id": "1017", "start_time": "13:55", "authors": "Ferhan Ture and Oliver Jojic"}, {"title": "The strange geometry of skip-gram with negative sampling", "venue": "ACL", "end_time": "14:25", "abstract": "Despite their ubiquity, word embeddings trained with skip-gram negative\nsampling (SGNS) remain poorly understood. We find that vector positions are not\nsimply determined by semantic similarity, but rather occupy a narrow cone,\ndiametrically opposed to the context vectors. We show that this geometric\nconcentration depends on the ratio of positive to negative examples, and that\nit is neither theoretically nor empirically inherent in related embedding\nalgorithms.", "id": "888", "start_time": "14:10", "authors": "David Mimno and Laure Thompson"}, {"title": "Natural Language Processing with Small Feed-Forward Networks", "venue": "ACL", "end_time": "14:40", "abstract": "We show that small and shallow feed-forward neural networks can achieve near\nstate-of-the-art results on a range of unstructured and structured language\nprocessing tasks while being considerably cheaper in memory and computational\nrequirements than deep recurrent models.\n\nMotivated by resource-constrained environments like mobile phones, we showcase\nsimple techniques for obtaining such small neural network models, and\ninvestigate different tradeoffs when deciding how to allocate a small memory\nbudget.", "id": "1281", "start_time": "14:25", "authors": "Jan A. Botha, Emily Pitler, Ji Ma, Anton Bakalov, Alex Salcianu, David Weiss, Ryan McDonald and Slav Petrov"}, {"title": "Deep Multi-Task Learning for Aspect Term Extraction with Memory Interaction", "venue": "ACL", "end_time": "14:55", "abstract": "We propose a novel LSTM-based deep multi-task learning framework for aspect\nterm extraction from user review sentences. Two LSTMs equipped with extended\nmemories and neural memory operations are designed for jointly handling the\nextraction tasks of aspects and opinions via memory interactions. Sentimental\nsentence constraint is also added for more accurate prediction via another\nLSTM. Experiment results over two benchmark datasets demonstrate the\neffectiveness of our framework.", "id": "1318", "start_time": "14:40", "authors": "Xin Li and Wai Lam"}, {"title": "Analogs of Linguistic Structure in Deep Representations", "venue": "ACL", "end_time": "15:10", "abstract": "We investigate the compositional structure of message vectors computed by a\ndeep\nnetwork trained on a communication game. By comparing truth-conditional\nrepresentations of encoder-produced message vectors to human-produced referring\nexpressions, we are able to identify aligned (vector, utterance) pairs with the\nsame meaning. We then search for structured relationships among these aligned\npairs to discover simple vector space transformations corresponding to\nnegation,\nconjunction, and disjunction. Our results suggest that neural representations\nare capable of spontaneously developing a ``syntax'' with functional analogues\nto qualitative properties of natural language.", "id": "300", "start_time": "14:55", "authors": "Jacob Andreas and Dan Klein"}, {"title": "A Simple Regularization-based Algorithm for Learning Cross-Domain Word Embeddings", "venue": "ACL", "end_time": "15:25", "abstract": "Learning word embeddings has received a significant amount of attention\nrecently. Often, word embeddings are learned in an unsupervised manner from a\nlarge collection of text. The genre of the text typically plays an important\nrole in the effectiveness of the resulting embeddings. How to effectively train\nword embedding models using data from different domains remains a problem that\nis less explored. In this paper, we present a simple yet effective method for\nlearning word embeddings based on text from different domains. We demonstrate\nthe effectiveness of our approach through extensive experiments on various\ndown-stream NLP tasks.", "id": "1249", "start_time": "15:10", "authors": "Wei Yang, Wei Lu and Vincent Zheng"}], "chair": "Samuel R. Bowman"}, {"title": "NLP Applications (Short)", "start_time_iso": "2017-09-11T13:40:00", "end_time_iso": "2017-09-11T15:25:00", "type": "paper", "posters": [], "chair_affiliation": "Grammarly", "code": "8C", "room": "Zealand", "end_time": "15:25", "id": "63", "start_time": "13:40", "talks": [{"title": "Learning what to read: Focused machine reading", "venue": "ACL", "end_time": "13:55", "abstract": "Recent efforts in bioinformatics have achieved tremendous progress in the ma-\nchine reading of biomedical literature, and the assembly of the extracted\nbiochem- ical interactions into large-scale models such as protein signaling\npathways. How- ever, batch machine reading of literature at today\u2019s scale\n(PubMed alone indexes over 1 million papers per year) is unfea- sible due to\nboth cost and processing over- head. In this work, we introduce a focused\nreading approach to guide the machine reading of biomedical literature towards\nwhat literature should be read to answer a biomedical query as efficiently as\npos- sible. We introduce a family of algorithms for focused reading, including\nan intuitive, strong baseline, and a second approach which uses a reinforcement\nlearning (RL) framework that learns when to explore (widen the search) or\nexploit (narrow it). We demonstrate that the RL approach is capable of\nanswering more queries than the baseline, while being more efficient, i.e.,\nreading fewer documents.", "id": "1188", "start_time": "13:40", "authors": "Enrique Noriega-Atala, Marco A. Valenzuela-Esc\u00e1rcega, Clayton Morrison and Mihai Surdeanu"}, {"title": "DOC: Deep Open Classification of Text Documents", "venue": "ACL", "end_time": "14:10", "abstract": "Traditional supervised learning makes the closed-world assumption that the\nclasses appeared in the test data must have appeared in training. This also\napplies to text learning or text classification. As learning is used\nincreasingly in dynamic open environments where some new/test documents may not\nbelong to any of the training classes, identifying these novel documents during\nclassification presents an important problem. This problem is called open-world\nclassification or open classification. This paper proposes a novel deep\nlearning based approach. It outperforms existing state-of-the-art techniques\ndramatically.", "id": "1420", "start_time": "13:55", "authors": "Lei Shu, Hu Xu and Bing Liu"}, {"title": "Charmanteau: Character Embedding Models For Portmanteau Creation", "venue": "ACL", "end_time": "14:25", "abstract": "Portmanteaus are a word formation phenomenon where two words combine into a new\nword. We propose character-level neural sequence-to-sequence (S2S) methods for\nthe task of portmanteau generation that are end-to-end-trainable, language\nindependent, and do not explicitly use additional phonetic information. We\npropose a noisy-channel-style model, which allows for the incorporation of\nunsupervised word lists, improving performance over a standard source-to-target\nmodel. This model is made possible by an exhaustive candidate generation\nstrategy specifically enabled by the features of the portmanteau task.\nExperiments find our approach superior to a state-of-the-art FST-based baseline\nwith respect to ground truth accuracy and human evaluation.", "id": "852", "start_time": "14:10", "authors": "Varun Gangal, Harsh Jhamtani, Graham Neubig, Eduard Hovy and Eric Nyberg"}, {"title": "Using Automated Metaphor Identification to Aid in Detection and Prediction of First-Episode Schizophrenia", "venue": "ACL", "end_time": "14:40", "abstract": "The diagnosis of serious mental health conditions such as schizophrenia is\nbased on the judgment of clinicians whose training takes several years, and\ncannot be easily formalized into objective measures. However, previous research\nsuggests there are disturbances in aspects of the language use of patients with\nschizophrenia.                                      \n\nUsing metaphor-identification and sentiment-analysis\nalgorithms to automatically generate features, we create a classifier,               \n     \n\nthat,\nwith high\naccuracy, can predict which patients will develop (or currently suffer from)\nschizophrenia.                                      \n\nTo our knowledge, this study is the first to demonstrate\nthe utility of automated metaphor identification algorithms for detection or\nprediction of disease.", "id": "709", "start_time": "14:25", "authors": "E. Dario Gutierrez, Guillermo Cecchi, Cheryl Corcoran and Philip Corlett"}, {"title": "Truth of Varying Shades: Analyzing Language in Fake News and Political Fact-Checking", "venue": "ACL", "end_time": "14:55", "abstract": "We present an analytic study on the language of news media in the context of\npolitical fact-checking and fake news detection. We compare the language of\nreal news with that of satire, hoaxes, and propaganda to find linguistic\ncharacteristics of untrustworthy text. To probe the feasibility of automatic\npolitical fact-checking, we also present a case study based on PolitiFact.com\nusing their factuality judgments on a 6-point scale. Experiments show that\nwhile media fact-checking remains to be an open research question, stylistic\ncues can help determine the truthfulness of text.", "id": "1326", "start_time": "14:40", "authors": "Hannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana Volkova and Yejin Choi"}, {"title": "Topic-Based Agreement and Disagreement in US Electoral Manifestos", "venue": "ACL", "end_time": "15:10", "abstract": "We present a topic-based analysis of agreement and disagreement in political\nmanifestos, which relies on a new method for topic detection based on key\nconcept  clustering. Our approach outperforms both standard techniques like LDA\nand a state-of-the-art graph-based method, and provides promising initial\nresults for this new task in computational social science.", "id": "196", "start_time": "14:55", "authors": "Stefano Menini, Federico Nanni, Simone Paolo Ponzetto and Sara Tonelli"}, {"title": "Zipporah: a Fast and Scalable Data Cleaning System for Noisy Web-Crawled Parallel Corpora", "venue": "ACL", "end_time": "15:25", "abstract": "We introduce Zipporah, a fast and scalable data cleaning system. We propose a\nnovel type of bag-of-words translation feature, and train logistic regression\nmodels to classify good data and synthetic noisy data in the proposed feature\nspace. The trained model is used to score parallel sentences in the data pool\nfor selection. As shown in experiments, Zipporah selects a high-quality\nparallel corpus from a large, mixed quality data pool. In particular, for one\nnoisy dataset, Zipporah achieves a 2.1 BLEU score improvement with using 1/5 of\nthe data over using the entire corpus.", "id": "476", "start_time": "15:10", "authors": "Hainan Xu and Philipp Koehn"}], "chair": "Joel Tetreault"}], [{"title": "Coffee Break", "start_time_iso": "2017-09-11T15:25:00", "end_time": "15:50", "end_time_iso": "2017-09-11T15:50:00", "id": "64", "start_time": "15:25", "type": "break"}], [{"title": "Plenary Session. Best Paper", "start_time_iso": "2017-09-11T15:50:00", "end_time_iso": "2017-09-11T17:25:00", "type": "paper", "posters": [], "chair_affiliation": "University College London", "code": null, "room": "Jutland", "end_time": "17:25", "id": "65", "start_time": "15:50", "talks": [{"title": "Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints", "venue": "ACL", "end_time": "16:20", "abstract": "Language is increasingly being used to de-fine rich visual recognition problems\nwith supporting image collections sourced from the web. Structured prediction\nmodels are used  in  these  tasks  to  take  advantage              of correlations \nbetween  co-occurring  labels and visual input but risk inadvertently encoding\nsocial biases found in web corpora. In this work, we study data and models\nassociated with multilabel object classification and visual semantic role\nlabeling. We find that (a) datasets for these tasks contain significant gender\nbias and (b) models  trained  on  these  datasets  further  amplify existing\nbias.             For example,  the activity cooking is over 33% more likely to \ninvolve \nfemales  than  males  in  a  training set, and a trained model further\namplifies the disparity to 68% at test time.  We propose to inject corpus-level\nconstraints for calibrating existing structured prediction models and design an\nalgorithm based on Lagrangian relaxation for collective inference.  Our method\nresults in almost no performance loss for the underlying recognition task but\ndecreases the magnitude of bias amplification by 47.5% and 40.5% for multilabel\nclassification and visual semantic role labeling, respectively\u3002", "id": "1470", "start_time": "15:55", "authors": "Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez and Kai-Wei Chang"}, {"title": "Natural Language Does Not Emerge \u2018Naturally\u2019 in Multi-Agent Dialog", "venue": "ACL", "end_time": "16:35", "abstract": "A number of recent works have proposed techniques for end-to-end learning of\ncommunication protocols among cooperative multi-agent populations, and have\nsimultaneously found the emergence of grounded human-interpretable language in\nthe protocols developed by the agents, learned without any human supervision!\n\nIn this paper, using a Task & Talk reference game between two agents as a\ntestbed,  we present a sequence of `negative' results culminating in a\n`positive' one -- showing that while most agent-invented languages are\neffective (i.e. achieve near-perfect task rewards), they are decidedly not\ninterpretable or compositional. In essence, we find that natural language does\nnot emerge `naturally',despite the semblance of ease of\nnatural-language-emergence that one may gather from recent literature. We\ndiscuss how it is possible to coax the invented languages to become more and\nmore human-like and compositional by increasing restrictions on how two agents\nmay communicate.", "id": "505", "start_time": "16:20", "authors": "Satwik Kottur, Jos\u00e9 Moura, Stefan Lee and Dhruv Batra"}, {"title": "Depression and Self-Harm Risk Assessment in Online Forums", "venue": "ACL", "end_time": "17:00", "abstract": "Users suffering from mental health conditions often turn to online resources\nfor support, including specialized online support communities or general\ncommunities such as Twitter and Reddit. In this work, we present a framework\nfor supporting and studying users in both types of communities. We propose\nmethods for identifying posts in support communities that may indicate a risk\nof self-harm, and demonstrate that our approach outperforms strong previously\nproposed methods for identifying such posts. Self-harm is closely related to\ndepression, which makes identifying depressed users on general forums a crucial\nrelated task. We introduce a large-scale general forum dataset consisting of\nusers with self-reported depression diagnoses matched with control users. We\nshow how our method can be applied to effectively identify depressed users from\ntheir use of language alone. We demonstrate that our method outperforms strong\nbaselines on this general forum dataset.", "id": "633", "start_time": "16:35", "authors": "Andrew Yates, Arman Cohan and Nazli Goharian"}, {"title": "Bringing Structure into Summaries: Crowdsourcing a Benchmark Corpus of Concept Maps", "venue": "ACL", "end_time": "17:25", "abstract": "Concept maps can be used to concisely represent important information and bring\nstructure into large document collections. Therefore, we study a variant of\nmulti-document summarization that produces summaries in the form of concept\nmaps. However, suitable evaluation datasets for this task are currently\nmissing. To close this gap, we present a newly created corpus of concept maps\nthat summarize heterogeneous collections of web documents on educational\ntopics. It was created using a novel crowdsourcing approach that allows us to\nefficiently determine important elements in large document collections. We\nrelease the corpus along with a baseline system and proposed evaluation\nprotocol to enable further research on this variant of summarization.", "id": "341", "start_time": "17:00", "authors": "Tobias Falke and Iryna Gurevych"}], "chair": "Sebastian Riedel"}], [{"title": "Plenary Session. Closing Remarks", "code": null, "id": "66", "start_time_iso": "2017-09-11T17:25:00", "end_time": "17:45", "end_time_iso": "2017-09-11T17:45:00", "type": "other", "room": "Jutland", "start_time": "17:25", "talks": [], "posters": []}]], "weekday": "Monday"}]